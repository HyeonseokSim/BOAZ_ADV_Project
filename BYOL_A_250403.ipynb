{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WrQQXU9Fff9"
      },
      "source": [
        "**To-do List**  \n",
        "\n",
        "- 우선 BYOL-A 저자들이 제공하는 Augmentation 모듈로 실험중.   \n",
        "  모든 과정이 정상적으로 실행되면 patch-mix_contrastive_learning/util\n",
        "/augmentation.py 적용 -> 일단 취소\n",
        "- ICBHI 전체 데이터에 대해 적용 (진행중, PM 3:00~)  \n",
        "- t-SNE 시각화  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "A4Fh_qklCYp1",
        "outputId": "76b8184c-c25e-4ff9-ed80-2085199dadf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (6.0.2)\n",
            "Requirement already satisfied: pytorch_lightning in /usr/local/lib/python3.11/dist-packages (2.5.1)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: easydict in /usr/local/lib/python3.11/dist-packages (1.13)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning) (1.7.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning) (24.2)\n",
            "Requirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning) (0.14.2)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.14.1)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.0)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.24.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (3.11.14)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.18.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchaudio torchvision pyyaml pytorch_lightning librosa easydict tqdm wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "t4w8WkMWwwdp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import random\n",
        "import re\n",
        "import logging\n",
        "import yaml\n",
        "import datetime\n",
        "import pickle\n",
        "import librosa\n",
        "from pathlib import Path\n",
        "from easydict import EasyDict\n",
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchaudio\n",
        "import pytorch_lightning as pl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import multiprocessing\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn.functional as F\n",
        "import torchaudio.transforms as T\n",
        "import torch.optim as optim\n",
        "\n",
        "import wandb\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from pytorch_lightning.callbacks import ModelSummary, LearningRateMonitor\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6KgSeKT4Ml7",
        "outputId": "733a38b1-1bc8-4a16-ad34-6791fcd55cb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ICBHI_TRAIN_PATH = \"/content/drive/MyDrive/ADV 프로젝트/data/sample_test\"\n",
        "ICBHI_TRAIN_PATH = \"/content/drive/MyDrive/ADV 프로젝트/data/ICBHI/train\"\n",
        "ICBHI_FINETUNING_TRAIN_PATH = \"/content/drive/MyDrive/ADV 프로젝트/data/ICBHI/ft_train\"\n",
        "ICBHI_FINETUNING_VALID_PATH = \"/content/drive/MyDrive/ADV 프로젝트/data/ICBHI/ft_valid\"\n",
        "ICBHI_TEST_PATH = \"/content/drive/MyDrive/ADV 프로젝트/data/ICBHI/test\"\n",
        "PRETRAINED_MODEL_PATH = \"/content/drive/MyDrive/ADV 프로젝트/checkpoints\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KVJJMHdBDyA",
        "outputId": "d33c1265-0018-4f8a-a1bc-77235bd87e3a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvanillahub12\u001b[0m (\u001b[33mboaz_woony-boaz\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfVeQFU7z9go"
      },
      "source": [
        "## 1. 사전훈련 데이터 EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocVAfAl30SEa"
      },
      "source": [
        "#### 1-1. 데이터셋 구성 및 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rZVY34Knz-2m"
      },
      "outputs": [],
      "source": [
        "def Extract_Annotation_Data(file_name, root):\n",
        "    tokens = file_name.split('_')\n",
        "    recording_info = pd.DataFrame(data = [tokens], columns = ['Patient number', 'Recording index', 'Chest location','Acquisition mode','Recording equipment'])\n",
        "    recording_annotations = pd.read_csv(os.path.join(root, file_name + '.txt'), names = ['Start', 'End', 'Crackles', 'Wheezes'], delimiter= '\\t')\n",
        "    return (recording_info, recording_annotations)\n",
        "\n",
        "root = ICBHI_TRAIN_PATH\n",
        "filenames = [s.split('.')[0] for s in os.listdir(path = root) if '.txt' in s]\n",
        "\n",
        "i_list = []\n",
        "train_rec_annotations = []\n",
        "train_rec_annotations_dict = {}\n",
        "for s in filenames:\n",
        "    (i,a) = Extract_Annotation_Data(s, root)\n",
        "    i_list.append(i)\n",
        "    train_rec_annotations.append(a)\n",
        "    train_rec_annotations_dict[s] = a\n",
        "\n",
        "duration_list = []\n",
        "for i in range(len(train_rec_annotations)):\n",
        "    current = train_rec_annotations[i]\n",
        "    duration = current['End'] - current['Start']\n",
        "    duration_list.extend(duration)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "GA0BqruL-3xB",
        "outputId": "01b32d62-b03a-4001-e614-441e9565e9c3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Start     End  Crackles  Wheezes\n",
              "0    0.036   0.579         0        0\n",
              "1    0.579   2.450         0        0\n",
              "2    2.450   3.893         0        0\n",
              "3    3.893   5.793         0        0\n",
              "4    5.793   7.521         0        0\n",
              "5    7.521   9.279         0        0\n",
              "6    9.279  11.150         0        0\n",
              "7   11.150  13.036         0        0\n",
              "8   13.036  14.721         0        0\n",
              "9   14.721  16.707         0        0\n",
              "10  16.707  18.507         0        0\n",
              "11  18.507  19.964         0        0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bf674c22-576d-426f-a647-3a3457110749\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Start</th>\n",
              "      <th>End</th>\n",
              "      <th>Crackles</th>\n",
              "      <th>Wheezes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.036</td>\n",
              "      <td>0.579</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.579</td>\n",
              "      <td>2.450</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.450</td>\n",
              "      <td>3.893</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.893</td>\n",
              "      <td>5.793</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.793</td>\n",
              "      <td>7.521</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>7.521</td>\n",
              "      <td>9.279</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>9.279</td>\n",
              "      <td>11.150</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>11.150</td>\n",
              "      <td>13.036</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>13.036</td>\n",
              "      <td>14.721</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>14.721</td>\n",
              "      <td>16.707</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>16.707</td>\n",
              "      <td>18.507</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>18.507</td>\n",
              "      <td>19.964</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bf674c22-576d-426f-a647-3a3457110749')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bf674c22-576d-426f-a647-3a3457110749 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bf674c22-576d-426f-a647-3a3457110749');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5a04e699-57ee-4e66-937d-3c25de39e97a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5a04e699-57ee-4e66-937d-3c25de39e97a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5a04e699-57ee-4e66-937d-3c25de39e97a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"train_rec_annotations_dict['101_1b1_Al_sc_Meditron']\",\n  \"rows\": 12,\n  \"fields\": [\n    {\n      \"column\": \"Start\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6.286075424493747,\n        \"min\": 0.036,\n        \"max\": 18.507,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          16.707,\n          14.721,\n          0.036\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"End\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6.4371023407628485,\n        \"min\": 0.579,\n        \"max\": 19.964,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          18.507,\n          16.707,\n          0.579\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Crackles\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Wheezes\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "train_rec_annotations_dict['101_1b1_Al_sc_Meditron']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "wkBep-ll0JKP",
        "outputId": "a2706590-bd89-4114-9c82-4fe09a77765e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  Patient number Recording index Chest location Acquisition mode  \\\n",
              "0            134             2b1             Ar               mc   \n",
              "0            134             2b2             Al               mc   \n",
              "0            134             2b3             Ar               mc   \n",
              "0            134             2b2             Ar               mc   \n",
              "0            135             2b1             Al               mc   \n",
              "\n",
              "  Recording equipment  \n",
              "0            LittC2SE  \n",
              "0            LittC2SE  \n",
              "0            LittC2SE  \n",
              "0            LittC2SE  \n",
              "0            LittC2SE  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-06527bea-4415-4b72-99f4-7ae9b99e8c7d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Patient number</th>\n",
              "      <th>Recording index</th>\n",
              "      <th>Chest location</th>\n",
              "      <th>Acquisition mode</th>\n",
              "      <th>Recording equipment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>134</td>\n",
              "      <td>2b1</td>\n",
              "      <td>Ar</td>\n",
              "      <td>mc</td>\n",
              "      <td>LittC2SE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>134</td>\n",
              "      <td>2b2</td>\n",
              "      <td>Al</td>\n",
              "      <td>mc</td>\n",
              "      <td>LittC2SE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>134</td>\n",
              "      <td>2b3</td>\n",
              "      <td>Ar</td>\n",
              "      <td>mc</td>\n",
              "      <td>LittC2SE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>134</td>\n",
              "      <td>2b2</td>\n",
              "      <td>Ar</td>\n",
              "      <td>mc</td>\n",
              "      <td>LittC2SE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>135</td>\n",
              "      <td>2b1</td>\n",
              "      <td>Al</td>\n",
              "      <td>mc</td>\n",
              "      <td>LittC2SE</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-06527bea-4415-4b72-99f4-7ae9b99e8c7d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-06527bea-4415-4b72-99f4-7ae9b99e8c7d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-06527bea-4415-4b72-99f4-7ae9b99e8c7d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-dea52d92-fca8-4196-b74e-4ee193c9a5e8\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dea52d92-fca8-4196-b74e-4ee193c9a5e8')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-dea52d92-fca8-4196-b74e-4ee193c9a5e8 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "recording_info",
              "summary": "{\n  \"name\": \"recording_info\",\n  \"rows\": 600,\n  \"fields\": [\n    {\n      \"column\": \"Patient number\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 77,\n        \"samples\": [\n          \"138\",\n          \"169\",\n          \"144\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Recording index\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 28,\n        \"samples\": [\n          \"1b3\",\n          \"2b5\",\n          \"1b2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Chest location\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"Ar\",\n          \"Al\",\n          \"Ll\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Acquisition mode\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"sc\",\n          \"mc\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Recording equipment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Meditron\",\n          \"Litt3200\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "recording_info = pd.concat(i_list, axis = 0)\n",
        "recording_info.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qM6jvRXD0ZnJ"
      },
      "source": [
        "#### 1-2. 호흡 주기 분포 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "id": "7vtaTZYB0CVp",
        "outputId": "1f72817d-77c5-4bd0-8e92-6922fab169e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "longest cycle:16.163\n",
            "shortest cycle:0.228\n",
            "Fraction of samples less than 5 seconds:0.9711667411712115\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "duration_list = np.array(duration_list)\n",
        "plt.hist(duration_list, bins = 50)\n",
        "print('longest cycle:{}'.format(max(duration_list)))\n",
        "print('shortest cycle:{}'.format(min(duration_list)))\n",
        "threshold = 5\n",
        "print('Fraction of samples less than {} seconds:{}'.format(threshold, np.sum(duration_list < threshold)/len(duration_list)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9tFH6xc0Ec_",
        "outputId": "65ccea4d-47fe-42d5-ed41-735f51f44140"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4474"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "len(duration_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLEk_jop0cCV"
      },
      "source": [
        "#### 1-3. Label 분포 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qIThs81z0eKr"
      },
      "outputs": [],
      "source": [
        "no_label_list = []    # Normal\n",
        "crack_list = []       # Crackle Only\n",
        "wheeze_list = []      # Wheeze Only\n",
        "both_sym_list = []    # Both\n",
        "filename_list = []    # 데이터 이름\n",
        "\n",
        "for f in filenames:\n",
        "    d = train_rec_annotations_dict[f]\n",
        "    no_labels = len(d[(d['Crackles'] == 0) & (d['Wheezes'] == 0)].index)\n",
        "    n_crackles = len(d[(d['Crackles'] == 1) & (d['Wheezes'] == 0)].index)\n",
        "    n_wheezes = len(d[(d['Crackles'] == 0) & (d['Wheezes'] == 1)].index)\n",
        "    both_sym = len(d[(d['Crackles'] == 1) & (d['Wheezes'] == 1)].index)\n",
        "    no_label_list.append(no_labels)\n",
        "    crack_list.append(n_crackles)\n",
        "    wheeze_list.append(n_wheezes)\n",
        "    both_sym_list.append(both_sym)\n",
        "    filename_list.append(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "kUQs1Qvo0gGf",
        "outputId": "cfc3d03f-e8f8-4b81-d3a7-d622519a084c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 filename  no label  crackles only  wheezes only  \\\n",
              "0  134_2b1_Ar_mc_LittC2SE         5              0             0   \n",
              "1  134_2b2_Al_mc_LittC2SE         7              0             0   \n",
              "2  134_2b3_Ar_mc_LittC2SE         5              0             1   \n",
              "3  134_2b2_Ar_mc_LittC2SE         7              0             0   \n",
              "4  135_2b1_Al_mc_LittC2SE         9              0             0   \n",
              "\n",
              "   crackles and wheezees  \n",
              "0                      0  \n",
              "1                      0  \n",
              "2                      0  \n",
              "3                      0  \n",
              "4                      0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-86ff86c2-baf1-42dd-9aa9-598365b71c2e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>no label</th>\n",
              "      <th>crackles only</th>\n",
              "      <th>wheezes only</th>\n",
              "      <th>crackles and wheezees</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>134_2b1_Ar_mc_LittC2SE</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>134_2b2_Al_mc_LittC2SE</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>134_2b3_Ar_mc_LittC2SE</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>134_2b2_Ar_mc_LittC2SE</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>135_2b1_Al_mc_LittC2SE</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-86ff86c2-baf1-42dd-9aa9-598365b71c2e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-86ff86c2-baf1-42dd-9aa9-598365b71c2e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-86ff86c2-baf1-42dd-9aa9-598365b71c2e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-832ee07d-9a54-4740-aa78-8e02af9ec315\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-832ee07d-9a54-4740-aa78-8e02af9ec315')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-832ee07d-9a54-4740-aa78-8e02af9ec315 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "file_label_df",
              "summary": "{\n  \"name\": \"file_label_df\",\n  \"rows\": 600,\n  \"fields\": [\n    {\n      \"column\": \"filename\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 600,\n        \"samples\": [\n          \"151_2p2_Ll_mc_AKGC417L\",\n          \"107_2b3_Tc_mc_AKGC417L\",\n          \"130_3p2_Ar_mc_AKGC417L\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"no label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4,\n        \"min\": 0,\n        \"max\": 33,\n        \"num_unique_values\": 24,\n        \"samples\": [\n          1,\n          25,\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"crackles only\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 16,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          11,\n          8,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"wheezes only\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 12,\n        \"num_unique_values\": 11,\n        \"samples\": [\n          2,\n          0,\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"crackles and wheezees\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 9,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          8,\n          1,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "file_label_df = pd.DataFrame(data = {'filename':filename_list, 'no label':no_label_list, 'crackles only':crack_list, 'wheezes only':wheeze_list, 'crackles and wheezees':both_sym_list})\n",
        "file_label_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbTj_j4SJeon"
      },
      "source": [
        "## 2. 데이터 전처리  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGTGA4qvwKB2"
      },
      "source": [
        "#### 2-1. 호흡 주기 분할"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "OUKiqUQ7I3gd"
      },
      "outputs": [],
      "source": [
        "def _slice_data_torchaudio(start, end, data, sample_rate):\n",
        "    \"\"\"\n",
        "    SCL paper..\n",
        "    sample_rate denotes how many sample points for one second\n",
        "    \"\"\"\n",
        "    max_ind = len(data)\n",
        "    start_ind = min(int(start * sample_rate), max_ind)\n",
        "    end_ind = min(int(end * sample_rate), max_ind)\n",
        "\n",
        "    return data[start_ind: end_ind]\n",
        "\n",
        "\n",
        "def _get_lungsound_label(crackle, wheeze):\n",
        "    if crackle == 0 and wheeze == 0:\n",
        "        return 0\n",
        "    elif crackle == 1 and wheeze == 0:\n",
        "        return 1\n",
        "    elif crackle == 0 and wheeze == 1:\n",
        "        return 2\n",
        "    elif crackle == 1 and wheeze == 1:\n",
        "        return 3\n",
        "\n",
        "\n",
        "def get_individual_cycles_torchaudio(recording_annotations, wav, sample_rate):\n",
        "    \"\"\"\n",
        "    SCL paper..\n",
        "    used to split each individual sound file into separate sound clips containing one respiratory cycle each\n",
        "    output: [(audio_chunk:np.array, label:int), (...)]\n",
        "    \"\"\"\n",
        "    sample_data = []\n",
        "    # fpath = os.path.join(data_folder, filename)\n",
        "\n",
        "    # sr = librosa.get_samplerate(fpath)\n",
        "    # data, _ = torchaudio.load(fpath)\n",
        "\n",
        "    for idx in recording_annotations.index:\n",
        "        row = recording_annotations.loc[idx]\n",
        "\n",
        "        start = row['Start'] # time (second)\n",
        "        end = row['End'] # time (second)\n",
        "        audio_chunk = _slice_data_torchaudio(start, end, wav, sample_rate)\n",
        "\n",
        "        crackles = row['Crackles']\n",
        "        wheezes = row['Wheezes']\n",
        "        sample_data.append((audio_chunk, _get_lungsound_label(crackles, wheezes)))\n",
        "\n",
        "    return sample_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XavJi3_eDWqW"
      },
      "source": [
        "#### 2-2. Log mel spectrogram 변환  \n",
        "- WaveInLMSOutDataset 클래스 정의  \n",
        "- zero-padding, random crop을 통한 전처리 방법 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "aS2KEL97DNsF"
      },
      "outputs": [],
      "source": [
        "class MelSpectrogramLibrosa:\n",
        "    \"\"\"Mel spectrogram using librosa.\"\"\"\n",
        "    def __init__(self, fs=16000, n_fft=1024, shift=160, n_mels=64, fmin=60, fmax=7800):\n",
        "        self.fs, self.n_fft, self.shift, self.n_mels, self.fmin, self.fmax = fs, n_fft, shift, n_mels, fmin, fmax\n",
        "        self.mfb = librosa.filters.mel(sr=fs, n_fft=n_fft, n_mels=n_mels, fmin=fmin, fmax=fmax)\n",
        "\n",
        "    def __call__(self, audio):\n",
        "        X = librosa.stft(np.array(audio), n_fft=self.n_fft, hop_length=self.shift)\n",
        "        return torch.tensor(np.matmul(self.mfb, np.abs(X)**2 + np.finfo(float).eps))\n",
        "\n",
        "\n",
        "class WaveInLMSOutDataset(Dataset):\n",
        "    \"\"\"Wave in, log-mel spectrogram out, dataset class.\n",
        "\n",
        "    Choosing librosa or torchaudio:\n",
        "        librosa: Stable but slower.\n",
        "        torchaudio: Faster but cannot reproduce the exact performance of pretrained weight,\n",
        "            which might be caused by the difference with librosa. Librosa was used in the pretraining.\n",
        "\n",
        "    Args:\n",
        "        cfg: Configuration settings.\n",
        "        audio_files: List of audio file pathnames.\n",
        "        labels: List of labels corresponding to the audio files.\n",
        "        tfms: Transforms (augmentations), callable.\n",
        "        use_librosa: True if using librosa for converting audio to log-mel spectrogram (LMS).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cfg, mode, audio_files, labels, tfms, use_librosa=False):\n",
        "        # argment check\n",
        "        # assert (labels is None) or (len(audio_files) == len(labels)), 'The number of audio files and labels has to be the same.'\n",
        "        super().__init__()\n",
        "\n",
        "        # initializations\n",
        "        self.cfg = cfg\n",
        "        self.mode = mode\n",
        "        self.files = audio_files\n",
        "        self.labels = labels\n",
        "        self.tfms = tfms\n",
        "        self.unit_length = int(cfg.unit_sec * cfg.sample_rate)\n",
        "        self.to_melspecgram = MelSpectrogramLibrosa(\n",
        "            fs=cfg.sample_rate,\n",
        "            n_fft=cfg.n_fft,\n",
        "            shift=cfg.hop_length,\n",
        "            n_mels=cfg.n_mels,\n",
        "            fmin=cfg.f_min,\n",
        "            fmax=cfg.f_max,\n",
        "        ) if use_librosa else T.MelSpectrogram(\n",
        "            sample_rate=cfg.sample_rate,\n",
        "            n_fft=cfg.n_fft,\n",
        "            win_length=cfg.win_length,\n",
        "            hop_length=cfg.hop_length,\n",
        "            n_mels=cfg.n_mels,\n",
        "            f_min=cfg.f_min,\n",
        "            f_max=cfg.f_max,\n",
        "            power=2,\n",
        "        )\n",
        "\n",
        "        # 모든 호흡 주기를 저장할 리스트\n",
        "        self.audio_cycles = []\n",
        "        self.label_list = []\n",
        "\n",
        "        # 데이터 로드\n",
        "        if self.mode == 'train':\n",
        "            rec_annotations_dict = train_rec_annotations_dict\n",
        "        elif self.mode == 'finetuning_train':\n",
        "            rec_annotations_dict = finetuning_train_rec_annotations_dict\n",
        "        elif self.mode == 'finetuning_valid':\n",
        "            rec_annotations_dict = finetuning_valid_rec_annotations_dict\n",
        "        elif self.mode == 'test':\n",
        "            rec_annotations_dict = test_rec_annotations_dict\n",
        "\n",
        "        # 각 wav 파일을 처리하여 호흡 주기 단위로 저장\n",
        "        for i, file in enumerate(self.files):\n",
        "            wav, sr = torchaudio.load(file)\n",
        "\n",
        "            # Resample if needed\n",
        "            if sr != self.cfg.sample_rate:\n",
        "                resampler = T.Resample(orig_freq=sr, new_freq=self.cfg.sample_rate)\n",
        "                wav = resampler(wav)\n",
        "\n",
        "            # Ensure single channel\n",
        "            assert wav.shape[0] == 1, f'Convert .wav files to single channel audio, {file} has {wav.shape[0]} channels.'\n",
        "            wav = wav[0]  # (1, length) -> (length,)\n",
        "\n",
        "            # 호흡 주기 분할\n",
        "            annotation = rec_annotations_dict[file.stem]\n",
        "            cycles = get_individual_cycles_torchaudio(annotation, wav, self.cfg.sample_rate)\n",
        "\n",
        "            for cycle, label in cycles:\n",
        "                self.audio_cycles.append(cycle)\n",
        "                if self.labels:\n",
        "                    self.label_list.append(label)  # 각 cycle에 해당하는 원본 wav의 label 사용\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_cycles)  # 전체 호흡 주기 개수\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        cycle = self.audio_cycles[idx]\n",
        "\n",
        "        # zero padding: unit_length보다 짧은 경우, 양쪽에 0-padding 추가\n",
        "        length_adj = self.unit_length - len(cycle)\n",
        "        if length_adj > 0:\n",
        "            half_unit = self.unit_length // 2\n",
        "\n",
        "            if length_adj < half_unit:\n",
        "                # 길이 차이가 unit_length의 절반보다 작다면 zero padding 적용\n",
        "                half_adj = length_adj // 2\n",
        "                cycle = F.pad(cycle, (half_adj, length_adj - half_adj))\n",
        "            else:\n",
        "                # 길이 차이가 unit_length의 절반보다 크다면 충분히 cycle 반복하여 채우고 zero padding 적용\n",
        "                repeat_factor = (self.unit_length // len(cycle))        # 반복 횟수 결정\n",
        "                cycle = cycle.repeat(repeat_factor)[:self.unit_length]  # 필요한 길이만큼 자름\n",
        "                remaining_length = self.unit_length - len(cycle)        # 남은 길이 계산\n",
        "\n",
        "                # 남은 길이를 반으로 나눠서 앞뒤에 zero padding 적용\n",
        "                half_pad = remaining_length // 2\n",
        "                cycle = F.pad(cycle, (half_pad, remaining_length - half_pad))\n",
        "\n",
        "        # random crop: unit_length보다 긴 경우, 랜덤한 위치에서 unit_length만큼 자르기\n",
        "        length_adj = len(cycle) - self.unit_length\n",
        "        if length_adj > 0:\n",
        "            start = random.randint(0, length_adj // 4)  # 시작점을 cycle 길이의 앞쪽에서 랜덤 선택\n",
        "            end = start + self.unit_length  # 시작점으로부터 unit_length 길이만큼 크롭\n",
        "            cycle = cycle[start:end]\n",
        "\n",
        "        # Log mel spectrogram 변환 -> (1, n_mels, time)\n",
        "        lms = (self.to_melspecgram(cycle) + torch.finfo().eps).log().unsqueeze(0)  # (1, n_mels, time)\n",
        "\n",
        "        # transform (augmentation)\n",
        "        if self.tfms:\n",
        "            lms = self.tfms(lms)\n",
        "\n",
        "        # label이 있다면 반환\n",
        "        if self.labels:\n",
        "            return lms, self.label_list[idx]\n",
        "        return lms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GKbLHbIznaR"
      },
      "source": [
        "## 3. 모델 설계"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-9scNDMDdRx"
      },
      "source": [
        "#### 3-1. 모델 구조 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "nlAwhi3zDeE-"
      },
      "outputs": [],
      "source": [
        "class NetworkCommonMixIn():\n",
        "    \"\"\"Common mixin for network definition.\"\"\"\n",
        "\n",
        "    def load_weight(self, weight_file, device, state_dict=None, key_check=True):\n",
        "        \"\"\"Utility to load a weight file to a device.\"\"\"\n",
        "\n",
        "        state_dict = state_dict or torch.load(weight_file, map_location=device)\n",
        "        if 'state_dict' in state_dict:\n",
        "            state_dict = state_dict['state_dict']\n",
        "        # Remove unneeded prefixes from the keys of parameters.\n",
        "        if key_check:\n",
        "            weights = {}\n",
        "            for k in state_dict:\n",
        "                m = re.search(r'(^fc\\.|\\.fc\\.|^features\\.|\\.features\\.)', k)\n",
        "                if m is None: continue\n",
        "                new_k = k[m.start():]\n",
        "                new_k = new_k[1:] if new_k[0] == '.' else new_k\n",
        "                weights[new_k] = state_dict[k]\n",
        "        else:\n",
        "            weights = state_dict\n",
        "        # Load weights and set model to eval().\n",
        "        self.load_state_dict(weights)\n",
        "        self.eval()\n",
        "        logging.info(f'Using audio embbeding network pretrained weight: {Path(weight_file).name}')\n",
        "        return self\n",
        "\n",
        "    def set_trainable(self, trainable=False):\n",
        "        for p in self.parameters():\n",
        "            p.requires_grad = trainable\n",
        "\n",
        "\n",
        "class AudioNTT2020Task6(nn.Module, NetworkCommonMixIn):\n",
        "    \"\"\"DCASE2020 Task6 NTT Solution Audio Embedding Network.\"\"\"\n",
        "\n",
        "    def __init__(self, n_mels, d):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, 3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "\n",
        "            nn.Conv2d(64, 64, 3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "\n",
        "            nn.Conv2d(64, 64, 3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(64 * (n_mels // (2**3)), d),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.3),\n",
        "            nn.Linear(d, d),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.d = d\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)       # (batch, ch, mel, time)\n",
        "        x = x.permute(0, 3, 2, 1) # (batch, time, mel, ch)\n",
        "        B, T, D, C = x.shape\n",
        "        x = x.reshape((B, T, C*D)) # (batch, time, mel*ch)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class AudioNTT2020(AudioNTT2020Task6):\n",
        "    \"\"\"BYOL-A General Purpose Representation Network.\n",
        "    This is an extension of the DCASE 2020 Task 6 NTT Solution Audio Embedding Network.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_mels=64, d=512):\n",
        "        super().__init__(n_mels=n_mels, d=d)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = super().forward(x)\n",
        "        (x1, _) = torch.max(x, dim=1)\n",
        "        x2 = torch.mean(x, dim=1)\n",
        "        x = x1 + x2\n",
        "        assert x.shape[1] == self.d and x.ndim == 2\n",
        "        return x\n",
        "\n",
        "\n",
        "class AudioNTT2020Task6X(nn.Module, NetworkCommonMixIn):\n",
        "    \"\"\"A variant of DCASE2020 Task6 NTT Solution Audio Embedding Network.\n",
        "    Enabeld to return features by layers.\n",
        "    Examples:\n",
        "        model(x) -> returns sample-level features of [B, T, D].\n",
        "        model(x, layered=True) -> returns sample-level layered features of [B, T, 5*D]\n",
        "        model.by_layers(model.(x, layered=True)) -> returns sample-level features by layers as a list of [B, T, D] * 5\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_mels, d):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, 3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 64, 3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 64, 3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "        )\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Linear(64 * (n_mels // (2**3)), d),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.fc2 = nn.Sequential(\n",
        "            nn.Dropout(p=0.3),\n",
        "            nn.Linear(d, d),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.d = d\n",
        "        self.n_feature_layer = 5\n",
        "\n",
        "    def forward(self, x, layered=False):\n",
        "        def reshape_conv_feature(v):\n",
        "            B, CH, F, T = v.shape\n",
        "            v = v.permute(0, 3, 1, 2).reshape(B, T, CH*F)\n",
        "            # pad 0 at the end to make the feature dimension -> self.d\n",
        "            if v.shape[-1] < self.d:\n",
        "                v = torch.nn.functional.pad(v, (0, self.d - v.shape[-1]), 'constant', 0.0)\n",
        "            # average to the target length\n",
        "            while v.shape[1] > target_t:\n",
        "                ## when odd time frames -> average last two frames into one frame\n",
        "                if v.shape[1] % 2 == 1:\n",
        "                    v = torch.cat([v[:, :-2], v[:, -2:].mean(1, keepdim=True)], axis=1)\n",
        "                # [B, T, D] -> [B, T/2, D]\n",
        "                T = v.shape[1]\n",
        "                v = v.reshape(B, T//2, 2, v.shape[-1])\n",
        "                v = v.mean(2) # average adjoining two time frame features.\n",
        "            return v\n",
        "\n",
        "        target_t = x.shape[-1] // 8\n",
        "        features = []\n",
        "        x = self.conv1(x)         # (batch, ch, mel, time)\n",
        "        features.append(reshape_conv_feature(x))\n",
        "        x = self.conv2(x)\n",
        "        features.append(reshape_conv_feature(x))\n",
        "        x = self.conv3(x)\n",
        "        features.append(reshape_conv_feature(x))\n",
        "        x = x.permute(0, 3, 2, 1) # (batch, time, mel, ch)\n",
        "        B, T, D, C = x.shape\n",
        "        x = x.reshape((B, T, C*D)) # (batch, time, mel*ch)\n",
        "        x = self.fc1(x)\n",
        "        features.append(x)\n",
        "        x = self.fc2(x)\n",
        "        features.append(x)\n",
        "\n",
        "        if layered:\n",
        "            return torch.cat(features, dim=-1) # [B, T, 5*D]\n",
        "        return x # [B, T, D]\n",
        "\n",
        "    def by_layers(self, layered_features):\n",
        "        \"\"\"Decompose layered features into the list of features for each layer.\"\"\"\n",
        "        *B, LD = layered_features.shape\n",
        "        assert LD == self.n_feature_layer * self.d\n",
        "        layered_features = layered_features.reshape(*B, self.n_feature_layer, self.d)\n",
        "        layered_features = layered_features.permute(2, 0, 1, 3) if len(layered_features.shape) > 3 else layered_features.permute(1, 0, 2)\n",
        "        return [layered_features[l] for l in range(self.n_feature_layer)]\n",
        "\n",
        "    def load_weight(self, weight_file, device):\n",
        "        \"\"\"Whapper function for loading BYOL-A pre-trained weights.\"\"\"\n",
        "        namemap = {\n",
        "            'features.0': 'conv1.0', 'features.1': 'conv1.1',\n",
        "            'features.4': 'conv2.0', 'features.5': 'conv2.1',\n",
        "            'features.8': 'conv3.0', 'features.9': 'conv3.1',\n",
        "            'fc.0': 'fc1.0',\n",
        "            'fc.3': 'fc2.1',\n",
        "        }\n",
        "        state_dict = torch.load(weight_file, map_location=device)\n",
        "        new_dict = {}\n",
        "        # replace keys and remove 'num_batches_tracked'\n",
        "        for key in state_dict:\n",
        "            if 'num_batches_tracked' in key:\n",
        "                continue\n",
        "            new_key = key\n",
        "            for map_key in namemap:\n",
        "                if map_key in key:\n",
        "                    new_key = key.replace(map_key, namemap[map_key])\n",
        "                    break\n",
        "            new_dict[new_key] = state_dict[key]\n",
        "        return super().load_weight(weight_file, device, state_dict=new_dict, key_check=False)\n",
        "\n",
        "\n",
        "class AudioNTT2020X(AudioNTT2020Task6X):\n",
        "    \"\"\"BYOL-A General Purpose Representation Network.\n",
        "    This is an extension of the DCASE 2020 Task 6 NTT Solution Audio Embedding Network.\n",
        "    Enabeld to return features by layers.\n",
        "\n",
        "    Examples:\n",
        "        model(x) -> returns sample-level features of [B, D].\n",
        "        model(x, layered=True) -> returns sample-level layered features of [B, 5*D]\n",
        "        model(x, layered=True, by_layers=True) -> returns sample-level features by layers as a list of [B, D] * 5\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_mels=64, d=2048):\n",
        "        super().__init__(n_mels=n_mels, d=d)\n",
        "\n",
        "    def forward(self, x, layered=False, by_layers=False):\n",
        "        x = super().forward(x, layered=layered)\n",
        "        (x1, _) = torch.max(x, dim=1)\n",
        "        x2 = torch.mean(x, dim=1)\n",
        "        x = x1 + x2\n",
        "        if by_layers:\n",
        "            return self.by_layers(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oluurwU9EyLf"
      },
      "source": [
        "#### 3-2. BYOL 훈련 알고리즘 정의"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Online Network, Target Network, EMA 등 정의"
      ],
      "metadata": {
        "id": "2N7D8MTi72fs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ZIzapl36Ex49"
      },
      "outputs": [],
      "source": [
        "\"\"\"BYOL for Audio\n",
        "\n",
        "Kudos to Phil Wang, this implementation is based on https://github.com/lucidrains/byol-pytorch/\n",
        "\n",
        "This code is customized to enable:\n",
        "- Decoupling augmentations.\n",
        "- Feeding two augmented input batches independently.\n",
        "\"\"\"\n",
        "\n",
        "import copy\n",
        "from functools import wraps\n",
        "\n",
        "# helper functions\n",
        "\n",
        "def default(val, def_val):\n",
        "    return def_val if val is None else val\n",
        "\n",
        "def flatten(t):\n",
        "    return t.reshape(t.shape[0], -1)\n",
        "\n",
        "def singleton(cache_key):\n",
        "    def inner_fn(fn):\n",
        "        @wraps(fn)\n",
        "        def wrapper(self, *args, **kwargs):\n",
        "            instance = getattr(self, cache_key)\n",
        "            if instance is not None:\n",
        "                return instance\n",
        "\n",
        "            instance = fn(self, *args, **kwargs)\n",
        "            setattr(self, cache_key, instance)\n",
        "            return instance\n",
        "        return wrapper\n",
        "    return inner_fn\n",
        "\n",
        "def get_module_device(module):\n",
        "    return next(module.parameters()).device\n",
        "\n",
        "def set_requires_grad(model, val):\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad = val\n",
        "\n",
        "# loss fn\n",
        "\n",
        "def loss_fn(x, y):\n",
        "    x = F.normalize(x, dim=-1, p=2)\n",
        "    y = F.normalize(y, dim=-1, p=2)\n",
        "    return 2 - 2 * (x * y).sum(dim=-1)\n",
        "\n",
        "# exponential moving average\n",
        "\n",
        "class EMA():\n",
        "    def __init__(self, beta):\n",
        "        super().__init__()\n",
        "        self.beta = beta\n",
        "\n",
        "    def update_average(self, old, new):\n",
        "        if old is None:\n",
        "            return new\n",
        "        return old * self.beta + (1 - self.beta) * new\n",
        "\n",
        "def update_moving_average(ema_updater, ma_model, current_model):\n",
        "    for current_params, ma_params in zip(current_model.parameters(), ma_model.parameters()):\n",
        "        old_weight, up_weight = ma_params.data, current_params.data\n",
        "        ma_params.data = ema_updater.update_average(old_weight, up_weight)\n",
        "\n",
        "# MLP class for projector and predictor\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, dim, projection_size, hidden_size = 4096):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_size),\n",
        "            nn.BatchNorm1d(hidden_size),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(hidden_size, projection_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# a wrapper class for the base neural network\n",
        "# will manage the interception of the hidden layer output\n",
        "# and pipe it into the projecter and predictor nets\n",
        "\n",
        "class NetWrapper(nn.Module):\n",
        "    def __init__(self, net, projection_size, projection_hidden_size, layer = -2):\n",
        "        super().__init__()\n",
        "        self.net = net\n",
        "        self.layer = layer\n",
        "\n",
        "        self.projector = None\n",
        "        self.projection_size = projection_size\n",
        "        self.projection_hidden_size = projection_hidden_size\n",
        "\n",
        "        self.hidden = {}\n",
        "        self.hook_registered = False\n",
        "\n",
        "    def _find_layer(self):\n",
        "        if type(self.layer) == str:\n",
        "            modules = dict([*self.net.named_modules()])\n",
        "            return modules.get(self.layer, None)\n",
        "        elif type(self.layer) == int:\n",
        "            children = [*self.net.children()]\n",
        "            return children[self.layer]\n",
        "        return None\n",
        "\n",
        "    def _hook(self, _, input, output):\n",
        "        device = input[0].device\n",
        "        self.hidden[device] = flatten(output)\n",
        "\n",
        "    def _register_hook(self):\n",
        "        layer = self._find_layer()\n",
        "        assert layer is not None, f'hidden layer ({self.layer}) not found'\n",
        "        handle = layer.register_forward_hook(self._hook)\n",
        "        self.hook_registered = True\n",
        "\n",
        "    @singleton('projector')\n",
        "    def _get_projector(self, hidden):\n",
        "        _, dim = hidden.shape\n",
        "        projector = MLP(dim, self.projection_size, self.projection_hidden_size)\n",
        "        return projector.to(hidden)\n",
        "\n",
        "    def get_representation(self, x):\n",
        "        if self.layer == -1:\n",
        "            return self.net(x)\n",
        "\n",
        "        if not self.hook_registered:\n",
        "            self._register_hook()\n",
        "\n",
        "        self.hidden.clear()\n",
        "        _ = self.net(x)\n",
        "        hidden = self.hidden[x.device]\n",
        "        self.hidden.clear()\n",
        "\n",
        "        assert hidden is not None, f'hidden layer {self.layer} never emitted an output'\n",
        "        return hidden\n",
        "\n",
        "    def forward(self, x, return_projection = True):\n",
        "        representation = self.get_representation(x)\n",
        "\n",
        "        if not return_projection:\n",
        "            return representation\n",
        "\n",
        "        projector = self._get_projector(representation)\n",
        "        projection = projector(representation)\n",
        "        return projection, representation\n",
        "\n",
        "# main class\n",
        "\n",
        "\n",
        "class BYOL(nn.Module):\n",
        "    \"\"\"BYOL training module that is:\n",
        "    - Decoupled augmentations.\n",
        "    - Accepts two augmented inputs independently.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        net,\n",
        "        image_size,\n",
        "        hidden_layer=-1,\n",
        "        projection_size=256,\n",
        "        projection_hidden_size=4096,\n",
        "        moving_average_decay=0.99,\n",
        "        use_momentum=True,\n",
        "        channels=1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.net = net\n",
        "\n",
        "        self.online_encoder = NetWrapper(net, projection_size, projection_hidden_size, layer=hidden_layer)\n",
        "\n",
        "        self.use_momentum = use_momentum\n",
        "        self.target_encoder = None\n",
        "        self.target_ema_updater = EMA(moving_average_decay)\n",
        "\n",
        "        self.online_predictor = MLP(projection_size, projection_size, projection_hidden_size)\n",
        "\n",
        "        # get device of network and make wrapper same device\n",
        "        device = get_module_device(net)\n",
        "        self.to(device)\n",
        "\n",
        "        # send a mock image tensor to instantiate singleton parameters\n",
        "        with torch.no_grad():\n",
        "            self.forward(torch.randn(2, channels, image_size[0], image_size[1]),\n",
        "                         torch.randn(2, channels, image_size[0], image_size[1]))\n",
        "\n",
        "    @singleton('target_encoder')\n",
        "    def _get_target_encoder(self):\n",
        "        target_encoder = copy.deepcopy(self.online_encoder)\n",
        "        set_requires_grad(target_encoder, False)\n",
        "        return target_encoder\n",
        "\n",
        "    def reset_moving_average(self):\n",
        "        del self.target_encoder\n",
        "        self.target_encoder = None\n",
        "\n",
        "    def update_moving_average(self):\n",
        "        assert self.use_momentum, 'you do not need to update the moving average, since you have turned off momentum for the target encoder'\n",
        "        assert self.target_encoder is not None, 'target encoder has not been created yet'\n",
        "        update_moving_average(self.target_ema_updater, self.target_encoder, self.online_encoder)\n",
        "\n",
        "    def forward(self, image_one, image_two,\n",
        "        return_embedding = False,\n",
        "        return_projection = True\n",
        "    ):\n",
        "        if return_embedding:\n",
        "            return self.online_encoder(x, return_projection=return_projection)\n",
        "\n",
        "        online_proj_one, _ = self.online_encoder(image_one)\n",
        "        online_proj_two, _ = self.online_encoder(image_two)\n",
        "\n",
        "        online_pred_one = self.online_predictor(online_proj_one)\n",
        "        online_pred_two = self.online_predictor(online_proj_two)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            target_encoder = self._get_target_encoder() if self.use_momentum else self.online_encoder\n",
        "            target_proj_one, _ = target_encoder(image_one)\n",
        "            target_proj_two, _ = target_encoder(image_two)\n",
        "            target_proj_one.detach_()\n",
        "            target_proj_two.detach_()\n",
        "\n",
        "        loss_one = loss_fn(online_pred_one, target_proj_two.detach())\n",
        "        loss_two = loss_fn(online_pred_two, target_proj_one.detach())\n",
        "\n",
        "        loss = loss_one + loss_two\n",
        "        return loss.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "훈련 과정에서 계산할 평균, 표준편차 연산 코드, 배치 정규화 코드 정의\n",
        "\n"
      ],
      "metadata": {
        "id": "7J9gfZWU9A7f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RunningMean:\n",
        "    \"\"\"Running mean calculator for arbitrary axis configuration.\"\"\"\n",
        "\n",
        "    def __init__(self, axis):\n",
        "        self.n = 0\n",
        "        self.axis = axis\n",
        "\n",
        "    def put(self, x):\n",
        "        # https://math.stackexchange.com/questions/106700/incremental-averageing\n",
        "        self.n += 1\n",
        "        if self.n == 1:\n",
        "            self.mu = x.mean(self.axis, keepdims=True)\n",
        "        else:\n",
        "            self.mu += (x.mean(self.axis, keepdims=True) - self.mu) / self.n\n",
        "\n",
        "    def __call__(self):\n",
        "        return self.mu\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n\n",
        "\n",
        "\n",
        "class RunningVariance:\n",
        "    \"\"\"Calculate mean/variance of tensors online.\n",
        "    Thanks to https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, axis, mean):\n",
        "        self.update_mean(mean)\n",
        "        self.s2 = RunningMean(axis)\n",
        "\n",
        "    def update_mean(self, mean):\n",
        "        self.mean = mean\n",
        "\n",
        "    def put(self, x):\n",
        "        self.s2.put((x - self.mean) **2)\n",
        "\n",
        "    def __call__(self):\n",
        "        return self.s2()\n",
        "\n",
        "    def std(self):\n",
        "        # Calculate the std using PyTorch functions and convert to a PyTorch tensor and add small value for numerical stability\n",
        "        return torch.sqrt(self.s2() + torch.finfo(self.s2().dtype).eps)\n",
        "\n",
        "\n",
        "class RunningNorm(nn.Module):\n",
        "    \"\"\"Online Normalization using Running Mean/Std.\n",
        "\n",
        "    This module will only update the statistics up to the specified number of epochs.\n",
        "    After the `max_update_epochs`, this will normalize with the last updated statistics.\n",
        "\n",
        "    Args:\n",
        "        epoch_samples: Number of samples in one epoch\n",
        "        max_update_epochs: Number of epochs to allow update of running mean/variance.\n",
        "        axis: Axis setting used to calculate mean/variance.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, epoch_samples, max_update_epochs=10, axis=[1, 2]):\n",
        "        super().__init__()\n",
        "        self.max_update = epoch_samples * max_update_epochs\n",
        "        self.ema_mean = RunningMean(axis)\n",
        "        self.ema_var = RunningVariance(axis, 0)\n",
        "\n",
        "    def forward(self, image):\n",
        "        if len(self.ema_mean) < self.max_update:\n",
        "            self.ema_mean.put(image)\n",
        "            self.ema_var.update_mean(self.ema_mean())\n",
        "            self.ema_var.put(image)\n",
        "            self.mean = self.ema_mean()\n",
        "            self.std = torch.clamp(self.ema_var.std(), torch.finfo().eps, torch.finfo().max)\n",
        "        return ((image - self.mean) / self.std)\n",
        "\n",
        "    def __repr__(self):\n",
        "        format_string = self.__class__.__name__ + f'(max_update={self.max_update},axis={self.ema_mean.axis})'\n",
        "        return format_string\n",
        "\n",
        "\n",
        "class PrecomputedNorm(nn.Module):\n",
        "    \"\"\"Normalization using Pre-computed Mean/Std.\n",
        "\n",
        "    Args:\n",
        "        stats: Precomputed (mean, std).\n",
        "        axis: Axis setting used to calculate mean/variance.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, stats, axis=[1, 2]):\n",
        "        super().__init__()\n",
        "        self.axis = axis\n",
        "        self.mean, self.std = stats\n",
        "\n",
        "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
        "        return ((X - self.mean) / self.std)\n",
        "\n",
        "    def __repr__(self):\n",
        "        format_string = self.__class__.__name__ + f'(mean={self.mean}, std={self.std}, axis={self.axis})'\n",
        "        return format_string\n",
        "\n",
        "\n",
        "class NormalizeBatch(nn.Module):\n",
        "    \"\"\"Normalization of Input Batch.\n",
        "\n",
        "    Note:\n",
        "        Unlike other blocks, use this with *batch inputs*.\n",
        "\n",
        "    Args:\n",
        "        axis: Axis setting used to calculate mean/variance.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, axis=[0, 2, 3]):\n",
        "        super().__init__()\n",
        "        self.axis = axis\n",
        "\n",
        "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
        "        _mean = X.mean(dim=self.axis, keepdims=True)\n",
        "        _std = torch.clamp(X.std(dim=self.axis, keepdims=True), torch.finfo().eps, torch.finfo().max)\n",
        "        return ((X - _mean) / _std)\n",
        "\n",
        "    def __repr__(self):\n",
        "        format_string = self.__class__.__name__ + f'(axis={self.axis})'\n",
        "        return format_string"
      ],
      "metadata": {
        "id": "_Sc9rIQw7wYF"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "훈련 로그 기록 등 보조적인 함수 정의"
      ],
      "metadata": {
        "id": "G2Lk7Nzk9KNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def get_timestamp():\n",
        "    \"\"\"ex) Outputs 202104220830\"\"\"\n",
        "    return datetime.datetime.now().strftime('%y%m%d%H%M')\n",
        "\n",
        "def load_yaml_config(path_to_config):\n",
        "    \"\"\"Loads yaml configuration settings as an EasyDict object.\"\"\"\n",
        "    path_to_config = Path(path_to_config)\n",
        "    assert path_to_config.is_file()\n",
        "    with open(path_to_config) as f:\n",
        "        yaml_contents = yaml.safe_load(f)\n",
        "    cfg = EasyDict(yaml_contents)\n",
        "    return cfg\n",
        "\n",
        "def get_logger(name):\n",
        "    logging.basicConfig(format='%(asctime)s %(levelname)s %(message)s',\n",
        "                        datefmt='%Y-%m-%d %H:%M', level=logging.DEBUG)\n",
        "    logger = logging.getLogger(name)\n",
        "    return logger"
      ],
      "metadata": {
        "id": "w33ghnk87zzh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeb3I6vKKBhf"
      },
      "source": [
        "#### 3-3. Augmentations 방법 정의  \n",
        "BYOL-A 논문 저자들이 제안하는 방법을 적용  \n",
        "1. RandomResizeCrop  \n",
        "2. Mixup  \n",
        "3. MixGaussianNoise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "FecGX_SWKD0l"
      },
      "outputs": [],
      "source": [
        "class RandomResizeCrop(nn.Module):\n",
        "    \"\"\"Random Resize Crop block.\n",
        "\n",
        "    Args:\n",
        "        virtual_crop_scale: Virtual crop area `(F ratio, T ratio)` in ratio to input size.\n",
        "        freq_scale: Random frequency range `(min, max)`.\n",
        "        time_scale: Random time frame range `(min, max)`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, virtual_crop_scale=(1.0, 1.5), freq_scale=(0.6, 1.5), time_scale=(0.6, 1.5)):\n",
        "        super().__init__()\n",
        "        self.virtual_crop_scale = virtual_crop_scale\n",
        "        self.freq_scale = freq_scale\n",
        "        self.time_scale = time_scale\n",
        "        self.interpolation = 'bicubic'\n",
        "        assert time_scale[1] >= 1.0 and freq_scale[1] >= 1.0\n",
        "\n",
        "    @staticmethod\n",
        "    def get_params(virtual_crop_size, in_size, time_scale, freq_scale):\n",
        "        canvas_h, canvas_w = virtual_crop_size\n",
        "        src_h, src_w = in_size\n",
        "        h = np.clip(int(np.random.uniform(*freq_scale) * src_h), 1, canvas_h)\n",
        "        w = np.clip(int(np.random.uniform(*time_scale) * src_w), 1, canvas_w)\n",
        "        i = random.randint(0, canvas_h - h) if canvas_h > h else 0\n",
        "        j = random.randint(0, canvas_w - w) if canvas_w > w else 0\n",
        "        return i, j, h, w\n",
        "\n",
        "    def forward(self, lms):\n",
        "        # make virtual_crop_arear empty space (virtual crop area) and copy the input log mel spectrogram to th the center\n",
        "        virtual_crop_size = [int(s * c) for s, c in zip(lms.shape[-2:], self.virtual_crop_scale)]\n",
        "        virtual_crop_area = (torch.zeros((lms.shape[0], virtual_crop_size[0], virtual_crop_size[1]))\n",
        "                             .to(torch.float).to(lms.device))\n",
        "        _, lh, lw = virtual_crop_area.shape\n",
        "        c, h, w = lms.shape\n",
        "        x, y = (lw - w) // 2, (lh - h) // 2\n",
        "        virtual_crop_area[:, y:y+h, x:x+w] = lms\n",
        "        # get random area\n",
        "        i, j, h, w = self.get_params(virtual_crop_area.shape[-2:], lms.shape[-2:], self.time_scale, self.freq_scale)\n",
        "        crop = virtual_crop_area[:, i:i+h, j:j+w]\n",
        "        # print(f'shapes {virtual_crop_area.shape} {crop.shape} -> {lms.shape}')\n",
        "        lms = F.interpolate(crop.unsqueeze(0), size=lms.shape[-2:],\n",
        "            mode=self.interpolation, align_corners=True).squeeze(0)\n",
        "        return lms.to(torch.float)\n",
        "\n",
        "    def __repr__(self):\n",
        "        format_string = self.__class__.__name__ + f'(virtual_crop_size={self.virtual_crop_scale}'\n",
        "        format_string += ', time_scale={0}'.format(tuple(round(s, 4) for s in self.time_scale))\n",
        "        format_string += ', freq_scale={0})'.format(tuple(round(r, 4) for r in self.freq_scale))\n",
        "        return format_string\n",
        "\n",
        "\n",
        "def log_mixup_exp(xa, xb, alpha):\n",
        "    xa = xa.exp()\n",
        "    xb = xb.exp()\n",
        "    x = alpha * xa + (1. - alpha) * xb\n",
        "    return torch.log(x + torch.finfo(x.dtype).eps)\n",
        "\n",
        "\n",
        "class MixupBYOLA(nn.Module):\n",
        "    \"\"\"Mixup for BYOL-A.\n",
        "\n",
        "    Args:\n",
        "        ratio: Alpha in the paper.\n",
        "        n_memory: Size of memory bank FIFO.\n",
        "        log_mixup_exp: Use log-mixup-exp to mix if this is True, or mix without notion of log-scale.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, ratio=0.4, n_memory=2048, log_mixup_exp=True):\n",
        "        super().__init__()\n",
        "        self.ratio = ratio\n",
        "        self.n = n_memory\n",
        "        self.log_mixup_exp = log_mixup_exp\n",
        "        self.memory_bank = []\n",
        "\n",
        "    def forward(self, x):\n",
        "        # mix random\n",
        "        alpha = self.ratio * np.random.random()\n",
        "        if self.memory_bank:\n",
        "            # get z as a mixing background sound\n",
        "            z = self.memory_bank[np.random.randint(len(self.memory_bank))]\n",
        "            # mix them\n",
        "            mixed = log_mixup_exp(x, z, 1. - alpha) if self.log_mixup_exp \\\n",
        "                    else alpha * z + (1. - alpha) * x\n",
        "        else:\n",
        "            mixed = x\n",
        "        # update memory bank\n",
        "        self.memory_bank = (self.memory_bank + [x])[-self.n:]\n",
        "\n",
        "        return mixed.to(torch.float)\n",
        "\n",
        "    def __repr__(self):\n",
        "        format_string = self.__class__.__name__ + f'(ratio={self.ratio},n={self.n}'\n",
        "        format_string += f',log_mixup_exp={self.log_mixup_exp})'\n",
        "        return format_string\n",
        "\n",
        "\n",
        "class MixGaussianNoise():\n",
        "    \"\"\"Gaussian Noise Mixer.\n",
        "    This interpolates with random sample, unlike Mixup.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, ratio=0.3):\n",
        "        self.ratio = ratio\n",
        "\n",
        "    def forward(self, lms):\n",
        "        x = lms.exp()\n",
        "\n",
        "        lambd = self.ratio * np.random.rand()\n",
        "        z = torch.normal(0, lambd, x.shape).exp()\n",
        "        mixed = (1 - lambd) * x + z + torch.finfo(x.dtype).eps\n",
        "\n",
        "        return mixed.log()\n",
        "\n",
        "    def __repr__(self):\n",
        "        format_string = self.__class__.__name__ + f'(ratio={self.ratio})'\n",
        "        return format_string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QZvrenoz4GU"
      },
      "source": [
        "## 4. Pretraining"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4HzJqqbFI9b"
      },
      "source": [
        "#### 4-1. 훈련 관련 파라미터 설정    "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 오디오 관련 설정\n",
        "- unit sec: 한 개의 오디오 클립(segment)의 길이 (초 단위). 3초 길이의 오디오 조각을 사용함.\n",
        "- sample_rate: 샘플링 레이트\n",
        "- n_fft:\tFFT(Fast Fourier Transform) 윈도우 크기. 스펙트로그램 변환 시 한 번에 처리할 샘플 수.  \n",
        "- win_length: STFT(SHORT-TIME Fourier Transform)에서 한 프레임의 길이. n_fft와 같음.\n",
        "- hop_length: STFT에서 프레임 간 오버랩 없이 이동하는 샘플 수. (16kHz 기준 10ms 간격)\n",
        "- n_mels:\tMel-spectrogram의 Mel filterbank 개수.\n",
        "- f_min: Mel-spectrogram에서 최소 주파수 (Hz).\n",
        "- f_max: Mel-spectrogram에서 최대 주파수 (Hz).  \n",
        "\n",
        "2. 모델 관련 설정\n",
        "- feature_d\t오디오 특징(feature) 벡터의 차원 수.\n",
        "- proj_size\tProjection 레이어의 출력 차원.\n",
        "- proj_dim BYOL에서 사용되는 MLP projection head의 중간 차원 크기.\n",
        "- ema_decay\tEMA (Exponential Moving Average) 업데이트 계수. 타겟 네트워크 업데이트에 사용됨.  \n",
        "\n",
        "3. 훈련 관련 설정  \n",
        "- seed\t42\t랜덤 시드 값. 실험 재현성을 보장하기 위해 사용됨.\n",
        "- bs 배치 크기 (batch size). 한 번의 학습 스텝에서 처리할 샘플 개수.\n",
        "- lr 학습률 (learning rate). 모델을 학습할 때 가중치를 업데이트하는 크기.\n",
        "- epochs 전체 데이터셋을 학습하는 횟수 (에포크 수).\n",
        "- num_workers\t데이터 로딩 시 병렬 처리를 위한 워커(worker) 수.  \n",
        "\n",
        "4. 체크포인트 관련 설정  \n",
        "- shape Mel-spectrogram의 최종 출력 형태 (n_mels x time frames).\n",
        "- checkpoint_folder\tcheckpoints\t모델 가중치를 저장할 폴더 이름."
      ],
      "metadata": {
        "id": "I60X4TmqeNmQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "-1d1iKIQFKUI"
      },
      "outputs": [],
      "source": [
        "config_data = {\n",
        "    # 1. 오디오 관련 설정\n",
        "    \"unit_sec\": 3,\n",
        "    \"sample_rate\": 22050,\n",
        "    \"n_fft\": 512,\n",
        "    \"win_length\": 512,\n",
        "    \"hop_length\": 256,\n",
        "    \"n_mels\": 64,\n",
        "    \"f_min\": 60,\n",
        "    \"f_max\": 2000,\n",
        "\n",
        "    # 2. 모델 관련 설정\n",
        "    \"feature_d\": 2048,\n",
        "    \"proj_size\": 256,\n",
        "    \"proj_dim\": 4096,\n",
        "    \"ema_decay\": 0.99,\n",
        "\n",
        "    # 3. 훈련 관련 설정\n",
        "    \"seed\": 42,\n",
        "    \"bs\": 64,  # GPU 메모리 고려하여 조정\n",
        "    \"lr\": 0.0003,\n",
        "    \"epochs\": 100,\n",
        "    \"num_workers\": 8,\n",
        "\n",
        "    # 4. 체크포인트 관련 설정\n",
        "    \"shape\": [64, 258],\n",
        "    \"checkpoint_folder\": PRETRAINED_MODEL_PATH\n",
        "}\n",
        "\n",
        "# YAML 파일 저장\n",
        "yaml_filename = \"icbhi_config.yaml\"\n",
        "with open(yaml_filename, \"w\") as yaml_file:\n",
        "    yaml.dump(config_data, yaml_file, default_flow_style=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5m9KnO-xEMll"
      },
      "source": [
        "#### 4-2. 모델 훈련 코드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "saC4MtAKENJd"
      },
      "outputs": [],
      "source": [
        "# 데이터 전처리 및 증강 모듈 정의\n",
        "class AugmentationModule:\n",
        "    \"\"\"BYOL-A augmentation module example, the same parameter with the paper.\"\"\"\n",
        "    def __init__(self, size, epoch_samples, log_mixup_exp=True, mixup_ratio=0.4):\n",
        "        self.train_transform = torch.nn.Sequential(\n",
        "            # 입력 데이터를 혼합하여 더 강력한 표현 학습을 가능하게 함\n",
        "            MixupBYOLA(ratio=mixup_ratio, log_mixup_exp=log_mixup_exp),\n",
        "            # 랜덤한 크기로 시간 및 주파수 영역을 크롭\n",
        "            RandomResizeCrop(virtual_crop_scale=(1.0, 1.5), freq_scale=(0.6, 1.5), time_scale=(0.6, 1.5)),\n",
        "        )\n",
        "        # 데이터 정규화\n",
        "        self.pre_norm = RunningNorm(epoch_samples=epoch_samples)\n",
        "        print('Augmentations:', self.train_transform)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        # 데이터 정규화\n",
        "        x = self.pre_norm(x)\n",
        "        # 같은 데이터를 두 번 변형하여 두 개의 증강 버전을 생성 → BYOL 모델에 필요한 두 개의 입력 생성\n",
        "        return self.train_transform(x), self.train_transform(x)\n",
        "\n",
        "\n",
        "class Pretraining_EarlyStopping(pl.Callback):\n",
        "    def __init__(self, patience, min_lr, min_delta=0.001):\n",
        "        self.patience = patience      # 개선되지 않는 epoch 허용 개수 (15)\n",
        "        self.min_lr = min_lr          # 최소 학습률\n",
        "        self.min_delta = min_delta    # 최소 개선량\n",
        "        self.counter = 0              # 개선되지 않은 epoch 횟수\n",
        "        self.best_loss = float('inf') # 최소 Train Loss 저장\n",
        "        self.min_lr_reached = False   # 최소 학습률 도달 여부 확인\n",
        "\n",
        "    def on_train_epoch_end(self, trainer, train_loss):\n",
        "        # min_lr에 도달했는지 확인\n",
        "        current_lr = trainer.optimizers[0].param_groups[0]['lr']\n",
        "        if current_lr <= self.min_lr:\n",
        "            self.min_lr_reached = True\n",
        "\n",
        "        # 최소 학습률 도달 후, train_loss 추적\n",
        "        if self.min_lr_reached:\n",
        "            train_loss = trainer.callback_metrics.get(\"train_loss\", None)\n",
        "            if train_loss is not None:\n",
        "                if train_loss.item() < self.best_loss - self.min_delta:\n",
        "                    self.best_loss = train_loss.item()\n",
        "                    self.counter = 0   # 개선되었으므로 초기화\n",
        "                else:\n",
        "                    self.counter += 1  # 개선되지 않으면 카운트 증가\n",
        "\n",
        "                if self.counter >= self.patience:\n",
        "                    print(\"Early stopping triggered\")\n",
        "                    trainer.should_stop = True  # 학습 중단\n",
        "\n",
        "\n",
        "\n",
        "class BYOLALearner(pl.LightningModule):\n",
        "    \"\"\"BYOL-A learner. Shows batch statistics for each epochs.\"\"\"\n",
        "    def __init__(self, model, lr, shape, early_stopping_callback, **kwargs):\n",
        "        super().__init__()                  # 부모 클래스의 속성 및 메소드 가져오기\n",
        "        self.learner = BYOL(model, image_size=shape, **kwargs)  # BYOL 모델 학습\n",
        "        self.lr = lr                        # 학습률\n",
        "        self.post_norm = NormalizeBatch()   # 배치 정규화\n",
        "        self.early_stopping_callback = early_stopping_callback  # EarlyStopping\n",
        "\n",
        "    def forward(self, images1, images2):\n",
        "        return self.learner(images1, images2)\n",
        "\n",
        "    def training_step(self, paired_inputs, batch_idx):\n",
        "        def to_np(A): return [a.cpu().numpy() for a in A]\n",
        "        bs = paired_inputs[0].shape[0]      # 배치 크기\n",
        "\n",
        "        # 배치 크기 B, 채널 1, 주파수 축 F, 시간 축 T\n",
        "        # [(B,1,F,T), (B,1,F,T)] -> (2*B,1,F,T)\n",
        "\n",
        "        paired_inputs = torch.cat(paired_inputs)  # 배치 차원을 기준으로 두 개의 입력 텐서를 하나로 합침 (2 * batch_size)\n",
        "        mb, sb = to_np((paired_inputs.mean(), paired_inputs.std()))    # 입력 데이터의 평균 및 표준편차 계산 (정규화 전)\n",
        "\n",
        "        paired_inputs = self.post_norm(paired_inputs)                  # 배치 정규화 수행\n",
        "        ma, sa = to_np((paired_inputs.mean(), paired_inputs.std()))    # 정규화 후 평균 및 표준편차 계산\n",
        "\n",
        "        loss = self.forward(paired_inputs[:bs], paired_inputs[bs:])    # 첫 번째 절반과 두 번째 절반을 각각 입력으로 사용하여 BYOL 손실 계산\n",
        "        current_lr = self.trainer.optimizers[0].param_groups[0]['lr']  # 현재 학습률 가져오기\n",
        "\n",
        "        # 로그 저장\n",
        "        for k, v in {'train_loss': loss, 'learning_rate': current_lr, 'mb': mb, 'sb': sb, 'ma': ma, 'sa': sa}.items():\n",
        "            self.log(k, float(v), prog_bar=True, on_step=False, on_epoch=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
        "\n",
        "        # 10 epoch 동안 train_loss 개선이 없으면 학습률 감소 (10배 줄임)\n",
        "        scheduler = {\n",
        "            'scheduler': torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "                optimizer, mode='min', factor=0.1, patience=10, verbose=True\n",
        "            ),\n",
        "            'monitor': 'train_loss',  # 학습률 감소 기준: train_loss\n",
        "            'interval': 'epoch',\n",
        "            'frequency': 1  # 매 epoch마다 체크\n",
        "        }\n",
        "\n",
        "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "        # EarlyStopping 체크\n",
        "        train_loss = self.trainer.callback_metrics.get(\"train_loss\", None)\n",
        "        if train_loss is not None:\n",
        "            self.early_stopping_callback.on_train_epoch_end(self.trainer, train_loss.item())\n",
        "\n",
        "    def on_before_zero_grad(self, _):\n",
        "        self.learner.update_moving_average()  # Moving Average로 가중치를 천천히 업데이트\n",
        "\n",
        "\n",
        "# 사전훈련 수행\n",
        "def BYOL_TRAIN(audio_dir, config_path='icbhi_config.yaml'):\n",
        "\n",
        "    ########################################  1. 초기 설정  ########################################\n",
        "\n",
        "    # config 불러오기\n",
        "    cfg = load_yaml_config(config_path)  # BYOL-A 모델 훈련을 위한 설정값이 저장된 config.yaml 로드\n",
        "\n",
        "    logger = get_logger(__name__)        # 로깅 설정 (학습 진행 상황을 출력)\n",
        "    logger.info(cfg)                     # 설정값(cfg)을 로그로 출력\n",
        "    seed_everything(cfg.seed)            # cfg.seed = 42을 사용하여 랜덤 시드 고정\n",
        "\n",
        "    wandb.init(\n",
        "        project=\"ICBHI_BYOL\",    # 프로젝트명\n",
        "        name=f\"BYOL_{cfg.epochs}epoch_{get_timestamp()}\",\n",
        "        config={                 # WandB에 기록할 설정값\n",
        "            \"epochs\": cfg.epochs,\n",
        "            \"batch_size\": cfg.bs,\n",
        "            \"lr\": cfg.lr,\n",
        "            \"ema_decay\": cfg.ema_decay,\n",
        "            \"spectrogram size\": cfg.shape\n",
        "        }\n",
        "    )\n",
        "\n",
        "    wandb_logger = WandbLogger(log_model=\"all\")  # WandB Logger 설정\n",
        "\n",
        "    ########################################  2. 데이터셋 로드  ########################################\n",
        "\n",
        "    # audio_dir 폴더에서 .wav 파일을 모두 찾아 정렬된 리스트로 저장\n",
        "    files = sorted(Path(audio_dir).glob('*.wav'))\n",
        "\n",
        "    # 데이터 증강 적용\n",
        "    spectrogram_size = tuple(cfg.shape)\n",
        "    tfms = AugmentationModule(spectrogram_size, 2*len(duration_list))  # 스펙트로그램 크기, 데이터 정규화를 위한 epoch 샘플 개수 지정 (2배로 설정)\n",
        "\n",
        "    # WaveInLMSOutDataset 클래스의 객체를 생성하여 데이터셋을 구성\n",
        "    ds = WaveInLMSOutDataset(cfg,         # cfg: 오디오 변환 관련 설정값(n_fft, n_mels 등) 반영\n",
        "                             'train',     # 사전훈련용 데이터 사용\n",
        "                             files,       # files: 오디오 파일 리스트\n",
        "                             labels=False, # BYOL은 비지도 학습이므로 label 미사용\n",
        "                             tfms=tfms)   # tfms=tfms: 앞에서 생성한 증강 모듈을 데이터셋에 적용\n",
        "\n",
        "    # ds 데이터셋을 DataLoader로 변환하여 배치 단위로 불러올 수 있도록 함\n",
        "    dl = DataLoader(ds, batch_size=cfg.bs,                    # batch_size=cfg.bs: 한 번에 bs개 샘플을 학습\n",
        "                    num_workers=multiprocessing.cpu_count(),  # num_workers=multiprocessing.cpu_count(): CPU 코어 개수만큼 데이터 로딩 속도 증가\n",
        "                    pin_memory=True, shuffle=True)            # pin_memory=True: GPU 사용 시 성능 최적화, shuffle=True: 데이터셋을 매 epoch마다 랜덤으로 섞음\n",
        "\n",
        "    # 데이터셋에 포함된 .wav 파일 개수와 audio_dir 경로를 로그로 출력\n",
        "    logger.info(f'Dataset: {len(files)} .wav files from {audio_dir}')\n",
        "\n",
        "    ########################################  3. 모델 정의  ########################################\n",
        "\n",
        "    # Feature 차원 (feature_d), Mel Spectrogram 크기, get_timestamp(): 현재 시간을 포함하여 고유 ID 부여\n",
        "    # Epoch 수 (100), 배치 크기, 학습률(소수점 아래를 제거하여 문자열 생성) (0.0003), 랜덤 시드 (42)\n",
        "    name = (f'BYOLA-Pretrained-d{cfg.feature_d}s{cfg.shape[0]}x{cfg.shape[1]}-{get_timestamp()}'\n",
        "            f'-e{cfg.epochs}-bs{cfg.bs}-lr{str(cfg.lr)[2:]}'\n",
        "            f'-rs{cfg.seed}')\n",
        "\n",
        "    # 학습 시작 로그 출력\n",
        "    logger.info(f'Training {name}...')\n",
        "\n",
        "    # n_mels=cfg.n_mels : Mel Spectrogram의 Mel 필터 개수 설정\n",
        "    # d=cfg.feature_d : 모델의 feature embedding 차원 설정\n",
        "    model = AudioNTT2020(n_mels=cfg.n_mels, d=cfg.feature_d)\n",
        "\n",
        "    # WandB에 모델 가중치 & 그래디언트 추적\n",
        "    wandb.watch(model, log=\"all\", log_freq=10)\n",
        "\n",
        "    ########################################  4. 학습 수행  ########################################\n",
        "\n",
        "    # 학습률 스케줄러를 모니터링하는 콜백 추가\n",
        "    lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
        "\n",
        "    # EarlyStopping 설정\n",
        "    early_stopping_callback = Pretraining_EarlyStopping(patience=12, min_lr=1e-6)\n",
        "\n",
        "    learner = BYOLALearner(model, cfg.lr, cfg.shape,             # cfg.shape : Mel Spectrogram 크기 설정\n",
        "                           early_stopping_callback,\n",
        "                           hidden_layer=-1,                      # BYOL에서 feature extraction을 수행할 레이어 설정, -1이면 모델의 마지막 레이어를 사용\n",
        "                           projection_size=cfg.proj_size,        # BYOL의 projection head의 출력 차원 설정\n",
        "                           projection_hidden_size=cfg.proj_dim,  # projection head 내부 hidden layer 크기\n",
        "                           moving_average_decay=cfg.ema_decay)   # BYOL의 Moving Average 업데이트 비율\n",
        "\n",
        "    # PyTorch Lightning을 사용하여 GPU 학습을 수행할 Trainer 객체 생성\n",
        "    # max_depth=2: 더 깊은 레이어까지 출력 (세부적인 모델 정보 확인)\n",
        "    trainer = pl.Trainer(\n",
        "        accelerator='gpu', devices=1, logger=wandb_logger,\n",
        "        max_epochs=cfg.epochs,\n",
        "        callbacks=[ModelSummary(max_depth=2), lr_monitor, early_stopping_callback]\n",
        "    )\n",
        "\n",
        "    # dl(DataLoader)에서 제공하는 데이터를 사용하여 BYOL 모델 학습\n",
        "    trainer.fit(learner, dl)\n",
        "\n",
        "    ########################################  5. 학습된 모델 가중치 저장  ########################################\n",
        "\n",
        "    to_file = Path(cfg.checkpoint_folder)/(name+'.pth')  # 저장 경로 설정\n",
        "    to_file.parent.mkdir(exist_ok=True, parents=True)    # checkpoints 폴더가 존재하지 않으면 생성\n",
        "    torch.save(model.state_dict(), to_file)              # model.state_dict()를 저장하여 가중치만 저장\n",
        "    logger.info(f'Saved weight as {to_file}')            # \"Saved weight as checkpoints/... .pth\" 로그 출력\n",
        "\n",
        "    wandb.finish()    # WandB 로깅 종료\n",
        "\n",
        "    return to_file"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4-3. 모델 훈련"
      ],
      "metadata": {
        "id": "WvLOoXnB-RbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# wandb.finish()"
      ],
      "metadata": {
        "id": "jQcgfVbdUM_h"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "rljlGE5z3_kt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "139c781195ee4ce88f44010f932b075e",
            "2552bf31359b4b0c9b8f835f00f88184",
            "b48c0825e8eb46598c23f5c6d9cbcc5c",
            "6bf02a6654f5466ea5448afef86a6884",
            "cf5a977434a04fa0bb13b292f27e81d4",
            "bf8d1b5b69e348b4a584b0a838bc8a6d",
            "2f99a8a6cdb84103a319c976a1646c2e",
            "7c7ac2ac58de47729449b4013990bf10",
            "f8c9110a444749718e71a117c6d45d7c",
            "6dda7c97e80344fd9e9433f2d4bcef58",
            "1f8e3d0ee3e846bca8c6e6d972e594e5"
          ]
        },
        "outputId": "6dccf4b2-f405-476b-d678-76492ee18497"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Augmentations: Sequential(\n",
            "  (0): MixupBYOLA(ratio=0.4,n=2048,log_mixup_exp=True)\n",
            "  (1): RandomResizeCrop(virtual_crop_size=(1.0, 1.5), time_scale=(0.6, 1.5), freq_scale=(0.6, 1.5))\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name                     | Type           | Params | Mode \n",
            "--------------------------------------------------------------------\n",
            "0 | learner                  | BYOL           | 31.7 M | train\n",
            "1 | learner.net              | AudioNTT2020   | 5.3 M  | train\n",
            "2 | learner.online_encoder   | NetWrapper     | 14.8 M | train\n",
            "3 | learner.online_predictor | MLP            | 2.1 M  | train\n",
            "4 | learner.target_encoder   | NetWrapper     | 14.8 M | train\n",
            "5 | post_norm                | NormalizeBatch | 0      | train\n",
            "--------------------------------------------------------------------\n",
            "16.9 M    Trainable params\n",
            "14.8 M    Non-trainable params\n",
            "31.7 M    Total params\n",
            "126.611   Total estimated model params size (MB)\n",
            "62        Modules in train mode\n",
            "0         Modules in eval mode\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "139c781195ee4ce88f44010f932b075e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇███</td></tr><tr><td>learning_rate</td><td>████████████████████████▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>lr-Adam</td><td>████████████████████████▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>ma</td><td>▄▅▄▂▅▅▂▅▄▅▁▄▄▄▆▅▆▂▅▆▇▄▅▅▆▆▅▃▇▄▆▆█▅▅▆▅▇▆▄</td></tr><tr><td>mb</td><td>▇▅▅▄▇▇▆▆█▇▇▆▆▅▆▄▅▇▆▄▅▆▅▅▇▆█▄▆▅▆▇▆▇█▅▇▁▇▇</td></tr><tr><td>sa</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sb</td><td>█▅▅▆▅▆▅▇█▄▅▄▄▃▅▇▅▆▆▄▁▃▆█▆▆▆▆▇█▆▅▅▄▅▃▆▄▄▇</td></tr><tr><td>train_loss</td><td>█▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>learning_rate</td><td>0.0</td></tr><tr><td>lr-Adam</td><td>0.0</td></tr><tr><td>ma</td><td>0.0</td></tr><tr><td>mb</td><td>0.08671</td></tr><tr><td>sa</td><td>1</td></tr><tr><td>sb</td><td>0.76832</td></tr><tr><td>train_loss</td><td>0.28369</td></tr><tr><td>trainer/global_step</td><td>6999</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">BYOL_100epoch_2504031102</strong> at: <a href='https://wandb.ai/boaz_woony-boaz/ICBHI_BYOL/runs/x5lu1o5o' target=\"_blank\">https://wandb.ai/boaz_woony-boaz/ICBHI_BYOL/runs/x5lu1o5o</a><br> View project at: <a href='https://wandb.ai/boaz_woony-boaz/ICBHI_BYOL' target=\"_blank\">https://wandb.ai/boaz_woony-boaz/ICBHI_BYOL</a><br>Synced 5 W&B file(s), 0 media file(s), 200 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250403_110227-x5lu1o5o/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "CHECKPOINT_PATH = BYOL_TRAIN(ICBHI_TRAIN_PATH, config_path='icbhi_config.yaml')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ep-OT8Ge08lQ"
      },
      "source": [
        "## 5. Fine Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5-1. 데이터셋 구성"
      ],
      "metadata": {
        "id": "s5KOyYSlWmJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Train Data\n",
        "finetuning_train_filenames = [s.split('.')[0] for s in os.listdir(path = ICBHI_FINETUNING_TRAIN_PATH) if '.txt' in s]\n",
        "finetuning_train_rec_annotations_dict = {}\n",
        "for s in finetuning_train_filenames:\n",
        "    (i,a) = Extract_Annotation_Data(s, ICBHI_FINETUNING_TRAIN_PATH)\n",
        "    finetuning_train_rec_annotations_dict[s] = a\n",
        "\n",
        "# 2. Validation Data\n",
        "finetuning_valid_filenames = [s.split('.')[0] for s in os.listdir(path = ICBHI_FINETUNING_VALID_PATH) if '.txt' in s]\n",
        "finetuning_valid_rec_annotations_dict = {}\n",
        "for s in finetuning_valid_filenames:\n",
        "    (i,a) = Extract_Annotation_Data(s, ICBHI_FINETUNING_VALID_PATH)\n",
        "    finetuning_valid_rec_annotations_dict[s] = a"
      ],
      "metadata": {
        "id": "c_w4oRCLWoU4"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5-2. Fine-tuning 코드"
      ],
      "metadata": {
        "id": "510saN3lDOfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FineTunedModel(nn.Module):\n",
        "    def __init__(self, base_model, num_classes, feature_dim):\n",
        "        super().__init__()\n",
        "        self.base_model = base_model  # 사전훈련된 BYOL-A 모델\n",
        "        self.classifier = nn.Linear(feature_dim, num_classes)  # Fine-tuning을 위한 FC Layer 추가\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.base_model(x)  # 특징 추출\n",
        "        x = self.classifier(x)  # 분류\n",
        "        return x\n",
        "\n",
        "class Finetuning_EarlyStopping:\n",
        "    def __init__(self, patience, min_delta=0.001):\n",
        "        self.patience = patience  # 개선되지 않는 epoch 허용 개수\n",
        "        self.min_delta = min_delta  # 최소 개선량 (이보다 작으면 개선으로 간주하지 않음)\n",
        "        self.counter = 0  # 개선되지 않은 epoch 횟수\n",
        "        self.best_loss = float('inf')  # 최소 Validation Loss 저장\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if val_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0  # 개선되었으므로 초기화\n",
        "        else:\n",
        "            self.counter += 1  # 개선되지 않으면 카운트 증가\n",
        "\n",
        "        if self.counter >= self.patience:\n",
        "            print(\"Early stopping triggered\")\n",
        "            return True  # 학습 중단\n",
        "        return False  # 학습 계속 진행\n",
        "\n",
        "def BYOL_FINETUNING(train_audio_dir, valid_audio_dir, CHECKPOINT_PATH, config_path='icbhi_config.yaml'):\n",
        "    # config.yaml 파일에서 설정값 불러오기\n",
        "    cfg = load_yaml_config(config_path)\n",
        "\n",
        "    logger = get_logger(__name__)\n",
        "    logger.info(cfg)\n",
        "    seed_everything(cfg.seed)            # 랜덤 시드 고정\n",
        "\n",
        "    # wandb 초기화\n",
        "    wandb.init(\n",
        "        project=\"ICBHI_BYOL\",\n",
        "        name=f\"BYOL_Finetuning_{cfg.epochs}epoch_{get_timestamp()}\",\n",
        "        config={\"batch_size\": cfg.bs, \"sample_rate\": cfg.sample_rate, \"spectrogram size\": cfg.shape}\n",
        "    )\n",
        "\n",
        "    # Train 데이터 로드\n",
        "    train_files = sorted(Path(train_audio_dir).glob('*.wav'))\n",
        "    train_ds = WaveInLMSOutDataset(cfg, 'finetuning_train', train_files, labels=True, tfms=False)\n",
        "    train_dl = DataLoader(train_ds, batch_size=cfg.bs, num_workers=multiprocessing.cpu_count(), pin_memory=True, shuffle=True)\n",
        "\n",
        "    # Valid 데이터 로드\n",
        "    valid_files = sorted(Path(valid_audio_dir).glob('*.wav'))\n",
        "    valid_ds = WaveInLMSOutDataset(cfg, 'finetuning_valid', valid_files, labels=True, tfms=False)\n",
        "    valid_dl = DataLoader(valid_ds, batch_size=cfg.bs, num_workers=multiprocessing.cpu_count(), pin_memory=True, shuffle=False)\n",
        "\n",
        "    # Fine-tuning 모델 이름 설정\n",
        "    name = (f'BYOLA-Finetuned-d{cfg.feature_d}s{cfg.shape[0]}x{cfg.shape[1]}-{get_timestamp()}'\n",
        "            f'-e{cfg.epochs}-bs{cfg.bs}-lr{str(cfg.lr)[2:]}'\n",
        "            f'-rs{cfg.seed}')\n",
        "\n",
        "    # 사전훈련된 가중치 불러오기\n",
        "    pretrained_weights = torch.load(CHECKPOINT_PATH, map_location='cuda')\n",
        "    model = AudioNTT2020(n_mels=cfg.n_mels, d=cfg.feature_d)\n",
        "    model.load_state_dict(pretrained_weights)\n",
        "\n",
        "    # Fine-tuning을 위해 학습 가능한 상태로 변경\n",
        "    model.train()\n",
        "\n",
        "    # 가중치 Freezing\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # 분류기만 학습하도록 설정\n",
        "    for param in model.fc.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    # Fine-tuning 모델 정의\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = FineTunedModel(model, num_classes=4, feature_dim=cfg.feature_d).to(device)\n",
        "\n",
        "    # Loss, 옵티마이저 정의\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=cfg.lr, weight_decay=1e-4)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True, min_lr=1e-6)\n",
        "    early_stopping = Finetuning_EarlyStopping(patience=15, min_delta=0.001)\n",
        "\n",
        "    # 학습 루프\n",
        "    num_epochs = cfg.epochs\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training Phase\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for batch_idx, (inputs, labels) in enumerate(train_dl):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            # 배치별 Confusion Matrix 계산 및 Sensitivity, Specificity 로깅\n",
        "            batch_conf_matrix = confusion_matrix(labels.cpu().numpy(), predicted.cpu().numpy(), labels=[0, 1, 2, 3])\n",
        "            sensitivity_list = []\n",
        "            specificity_list = []\n",
        "\n",
        "            for i in range(len(batch_conf_matrix)):\n",
        "                TP = batch_conf_matrix[i, i]\n",
        "                FN = sum(batch_conf_matrix[i, :]) - TP\n",
        "                FP = sum(batch_conf_matrix[:, i]) - TP\n",
        "                TN = batch_conf_matrix.sum() - (TP + FN + FP)\n",
        "\n",
        "                sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "                specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
        "\n",
        "                sensitivity_list.append(sensitivity)\n",
        "                specificity_list.append(specificity)\n",
        "\n",
        "            mean_sensitivity = np.mean(sensitivity_list)\n",
        "            mean_specificity = np.mean(specificity_list)\n",
        "            train_acc = 100 * correct / total\n",
        "\n",
        "            # wandb 실시간 로깅\n",
        "            wandb.log({\n",
        "                \"Train/Batch Progress\": batch_idx + 1,\n",
        "                \"Train/Batch Accuracy\": train_acc,\n",
        "                \"Train/Batch Sensitivity\": mean_sensitivity,\n",
        "                \"Train/Batch Specificity\": mean_specificity\n",
        "            })\n",
        "\n",
        "        # Validation Phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in valid_dl:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss /= len(valid_dl)\n",
        "        val_acc = 100 * correct / total\n",
        "\n",
        "        # 스케줄러 적용 & 학습률이 감소했는지 확인\n",
        "        prev_lr = optimizer.param_groups[0]['lr']  # 이전 학습률 저장\n",
        "        scheduler.step(val_loss)\n",
        "        new_lr = optimizer.param_groups[0]['lr']  # 새 학습률 확인\n",
        "\n",
        "        # 학습률이 감소하면 EarlyStopping patience 초기화\n",
        "        if new_lr < prev_lr:\n",
        "            print(f\"Learning rate decreased. Current lr is {new_lr}. EarlyStopping patience is reseted.\")\n",
        "            early_stopping.counter = 0  # EarlyStopping patience 초기화\n",
        "\n",
        "        # Early Stopping 적용\n",
        "        if early_stopping(val_loss):\n",
        "            print(f\"Stopped early at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Train Loss: {running_loss / len(train_dl):.4f}, Train Accuracy: {train_acc:.2f}%, \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "        # wandb 에포크별 로깅\n",
        "        wandb.log({\n",
        "            \"Train/Loss\": running_loss / len(train_dl),\n",
        "            \"Train/Accuracy\": train_acc,\n",
        "            \"Validation/Loss\": val_loss,\n",
        "            \"Validation/Accuracy\": val_acc\n",
        "        })\n",
        "\n",
        "    # 가중치 저장\n",
        "    to_file = Path(cfg.checkpoint_folder)/(name+'.pth')\n",
        "    torch.save(model.state_dict(), to_file)\n",
        "    print(\"Fine-tuned model saved successfully.\")\n",
        "\n",
        "    # wandb 종료\n",
        "    wandb.finish()\n",
        "\n",
        "    return to_file"
      ],
      "metadata": {
        "id": "nvRyRedpI1KW"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5-3. Fine Tuning 수행"
      ],
      "metadata": {
        "id": "C8BI1AvHBDhw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FILE_NAME = \"BYOLA-Pretrained-d2048s64x258-2504020547-e100-bs64-lr0003-rs42.pth\"\n",
        "# CHECKPOINT_PATH = os.path.join(PRETRAINED_MODEL_PATH, FILE_NAME)"
      ],
      "metadata": {
        "id": "AwNSqdayl_z9"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CHECKPOINT_PATH_2 = BYOL_FINETUNING(ICBHI_FINETUNING_TRAIN_PATH, ICBHI_FINETUNING_VALID_PATH, CHECKPOINT_PATH, config_path='icbhi_config.yaml')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-x5Itj4YytAE",
        "outputId": "f1d9182e-963e-4c94-eb8f-52bd0c2d1286"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250403_121346-fe8kb9f1</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/boaz_woony-boaz/ICBHI_BYOL/runs/fe8kb9f1' target=\"_blank\">BYOL_Finetuning_100epoch_2504031213</a></strong> to <a href='https://wandb.ai/boaz_woony-boaz/ICBHI_BYOL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/boaz_woony-boaz/ICBHI_BYOL' target=\"_blank\">https://wandb.ai/boaz_woony-boaz/ICBHI_BYOL</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/boaz_woony-boaz/ICBHI_BYOL/runs/fe8kb9f1' target=\"_blank\">https://wandb.ai/boaz_woony-boaz/ICBHI_BYOL/runs/fe8kb9f1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (64) may be set too high. Or, the value for `n_freqs` (257) may be set too low.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Train Loss: 1.3468, Train Accuracy: 52.85%, Val Loss: 1.8117, Val Accuracy: 25.58%\n",
            "Epoch 2, Train Loss: 0.7909, Train Accuracy: 70.29%, Val Loss: 1.8045, Val Accuracy: 16.28%\n",
            "Epoch 3, Train Loss: 0.7135, Train Accuracy: 73.63%, Val Loss: 2.0051, Val Accuracy: 14.53%\n",
            "Epoch 4, Train Loss: 0.6415, Train Accuracy: 76.75%, Val Loss: 2.3437, Val Accuracy: 13.37%\n",
            "Epoch 5, Train Loss: 0.5703, Train Accuracy: 78.69%, Val Loss: 2.3232, Val Accuracy: 19.19%\n",
            "Epoch 6, Train Loss: 0.4994, Train Accuracy: 81.59%, Val Loss: 2.7074, Val Accuracy: 15.70%\n",
            "Epoch 7, Train Loss: 0.4653, Train Accuracy: 83.21%, Val Loss: 2.9262, Val Accuracy: 19.19%\n",
            "Epoch 8, Train Loss: 0.4498, Train Accuracy: 83.32%, Val Loss: 2.5586, Val Accuracy: 25.00%\n",
            "Epoch 9, Train Loss: 0.3949, Train Accuracy: 86.44%, Val Loss: 2.8650, Val Accuracy: 19.19%\n",
            "Epoch 10, Train Loss: 0.3494, Train Accuracy: 88.70%, Val Loss: 2.9911, Val Accuracy: 26.74%\n",
            "Epoch 11, Train Loss: 0.3318, Train Accuracy: 88.27%, Val Loss: 3.5889, Val Accuracy: 19.77%\n",
            "Epoch 12, Train Loss: 0.3123, Train Accuracy: 87.73%, Val Loss: 3.0601, Val Accuracy: 13.95%\n",
            "Learning rate decreased. Current lr is 2.9999999999999997e-05. EarlyStopping patience is reseted.\n",
            "Epoch 13, Train Loss: 0.3403, Train Accuracy: 87.08%, Val Loss: 3.5551, Val Accuracy: 28.49%\n",
            "Epoch 14, Train Loss: 0.2707, Train Accuracy: 90.64%, Val Loss: 3.1611, Val Accuracy: 24.42%\n",
            "Epoch 15, Train Loss: 0.2364, Train Accuracy: 92.47%, Val Loss: 3.0601, Val Accuracy: 26.16%\n",
            "Epoch 16, Train Loss: 0.2379, Train Accuracy: 93.00%, Val Loss: 3.0814, Val Accuracy: 23.26%\n",
            "Epoch 17, Train Loss: 0.2180, Train Accuracy: 92.57%, Val Loss: 3.1285, Val Accuracy: 23.84%\n",
            "Epoch 18, Train Loss: 0.2220, Train Accuracy: 93.54%, Val Loss: 3.0047, Val Accuracy: 27.91%\n",
            "Epoch 19, Train Loss: 0.2285, Train Accuracy: 93.11%, Val Loss: 3.0213, Val Accuracy: 26.16%\n",
            "Epoch 20, Train Loss: 0.2105, Train Accuracy: 94.40%, Val Loss: 3.1392, Val Accuracy: 25.00%\n",
            "Epoch 21, Train Loss: 0.2132, Train Accuracy: 93.43%, Val Loss: 3.0839, Val Accuracy: 29.07%\n",
            "Epoch 22, Train Loss: 0.2089, Train Accuracy: 93.86%, Val Loss: 3.0495, Val Accuracy: 25.58%\n",
            "Epoch 23, Train Loss: 0.2128, Train Accuracy: 93.54%, Val Loss: 3.1650, Val Accuracy: 25.00%\n",
            "Learning rate decreased. Current lr is 3e-06. EarlyStopping patience is reseted.\n",
            "Epoch 24, Train Loss: 0.2226, Train Accuracy: 93.11%, Val Loss: 3.0842, Val Accuracy: 26.74%\n",
            "Epoch 25, Train Loss: 0.2005, Train Accuracy: 94.29%, Val Loss: 3.1082, Val Accuracy: 26.16%\n",
            "Epoch 26, Train Loss: 0.1987, Train Accuracy: 94.73%, Val Loss: 3.1693, Val Accuracy: 26.74%\n",
            "Epoch 27, Train Loss: 0.2018, Train Accuracy: 94.62%, Val Loss: 3.1509, Val Accuracy: 25.00%\n",
            "Epoch 28, Train Loss: 0.2029, Train Accuracy: 95.05%, Val Loss: 3.1180, Val Accuracy: 25.58%\n",
            "Epoch 29, Train Loss: 0.1982, Train Accuracy: 94.40%, Val Loss: 3.0556, Val Accuracy: 27.33%\n",
            "Epoch 30, Train Loss: 0.2046, Train Accuracy: 94.73%, Val Loss: 3.1097, Val Accuracy: 26.16%\n",
            "Epoch 31, Train Loss: 0.1995, Train Accuracy: 95.16%, Val Loss: 3.1755, Val Accuracy: 25.58%\n",
            "Epoch 32, Train Loss: 0.2037, Train Accuracy: 94.73%, Val Loss: 3.1463, Val Accuracy: 23.84%\n",
            "Epoch 33, Train Loss: 0.2001, Train Accuracy: 94.51%, Val Loss: 3.1463, Val Accuracy: 25.00%\n",
            "Epoch 34, Train Loss: 0.1992, Train Accuracy: 94.51%, Val Loss: 3.1277, Val Accuracy: 26.16%\n",
            "Learning rate decreased. Current lr is 1e-06. EarlyStopping patience is reseted.\n",
            "Epoch 35, Train Loss: 0.2003, Train Accuracy: 94.51%, Val Loss: 3.1300, Val Accuracy: 27.91%\n",
            "Epoch 36, Train Loss: 0.2072, Train Accuracy: 94.29%, Val Loss: 3.1496, Val Accuracy: 24.42%\n",
            "Epoch 37, Train Loss: 0.1941, Train Accuracy: 94.73%, Val Loss: 3.1047, Val Accuracy: 26.16%\n",
            "Epoch 38, Train Loss: 0.1944, Train Accuracy: 94.40%, Val Loss: 3.0659, Val Accuracy: 27.91%\n",
            "Epoch 39, Train Loss: 0.1943, Train Accuracy: 94.62%, Val Loss: 3.1195, Val Accuracy: 25.58%\n",
            "Epoch 40, Train Loss: 0.2009, Train Accuracy: 94.51%, Val Loss: 3.1364, Val Accuracy: 24.42%\n",
            "Epoch 41, Train Loss: 0.1994, Train Accuracy: 94.29%, Val Loss: 3.1084, Val Accuracy: 26.16%\n",
            "Epoch 42, Train Loss: 0.1947, Train Accuracy: 95.16%, Val Loss: 3.1430, Val Accuracy: 25.58%\n",
            "Epoch 43, Train Loss: 0.2012, Train Accuracy: 94.62%, Val Loss: 3.1197, Val Accuracy: 26.16%\n",
            "Epoch 44, Train Loss: 0.1885, Train Accuracy: 95.26%, Val Loss: 3.1484, Val Accuracy: 25.58%\n",
            "Epoch 45, Train Loss: 0.2017, Train Accuracy: 94.94%, Val Loss: 3.1294, Val Accuracy: 27.33%\n",
            "Epoch 46, Train Loss: 0.2071, Train Accuracy: 94.94%, Val Loss: 3.1486, Val Accuracy: 27.33%\n",
            "Epoch 47, Train Loss: 0.2116, Train Accuracy: 93.43%, Val Loss: 3.1483, Val Accuracy: 24.42%\n",
            "Epoch 48, Train Loss: 0.2006, Train Accuracy: 94.51%, Val Loss: 3.1332, Val Accuracy: 26.16%\n",
            "Early stopping triggered\n",
            "Stopped early at epoch 49\n",
            "Fine-tuned model saved successfully.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train/Accuracy</td><td>▁▄▄▅▅▆▆▇▇▇▇▇████████████████████████████</td></tr><tr><td>Train/Batch Accuracy</td><td>▁▄▄▅▅▆▆▆▆▆▆▇▇█▇▇█▇██████████████████████</td></tr><tr><td>Train/Batch Progress</td><td>▃█▃▆▃▄▅▃▅▅▁▅▁▃▄▁▇▅▄▅▇▂█▂▇▁▂▃▃▃▅▃▅▅▃▇▄▁▃▆</td></tr><tr><td>Train/Batch Sensitivity</td><td>▁▄▅▄▄▇▆▆▇▇▇▆█▇█▆▇▇█▇▇▇███▇██▇▆▅█▇██▇████</td></tr><tr><td>Train/Batch Specificity</td><td>▁▃▃▄▆▆▆▅▆▇█▆█▇██▇█▇▇▇▆▇▇▇▇▇██▇▇▆▇██▇▇▆██</td></tr><tr><td>Train/Loss</td><td>█▅▄▄▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation/Accuracy</td><td>▆▂▂▁▄▄▆▄▇▄█▆▇▅▆▇▆█▆▆▇▇▆▆▇▆▆▆▇▇▇▇▆▆▇▇▆▇▇▇</td></tr><tr><td>Validation/Loss</td><td>▁▁▂▃▃▅▄▅▆██▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train/Accuracy</td><td>94.51023</td></tr><tr><td>Train/Batch Accuracy</td><td>94.29494</td></tr><tr><td>Train/Batch Progress</td><td>15</td></tr><tr><td>Train/Batch Sensitivity</td><td>0.65873</td></tr><tr><td>Train/Batch Specificity</td><td>0.95312</td></tr><tr><td>Train/Loss</td><td>0.20063</td></tr><tr><td>Validation/Accuracy</td><td>26.16279</td></tr><tr><td>Validation/Loss</td><td>3.13319</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">BYOL_Finetuning_100epoch_2504031213</strong> at: <a href='https://wandb.ai/boaz_woony-boaz/ICBHI_BYOL/runs/fe8kb9f1' target=\"_blank\">https://wandb.ai/boaz_woony-boaz/ICBHI_BYOL/runs/fe8kb9f1</a><br> View project at: <a href='https://wandb.ai/boaz_woony-boaz/ICBHI_BYOL' target=\"_blank\">https://wandb.ai/boaz_woony-boaz/ICBHI_BYOL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250403_121346-fe8kb9f1/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. 성능 평가"
      ],
      "metadata": {
        "id": "tRgce8gWzJ_Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6-1. 데이터셋 구성"
      ],
      "metadata": {
        "id": "Ax1DYV9_oym7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_filenames = [s.split('.')[0] for s in os.listdir(path = ICBHI_TEST_PATH) if '.txt' in s]\n",
        "test_rec_annotations_dict = {}\n",
        "for s in test_filenames:\n",
        "    (i,a) = Extract_Annotation_Data(s, ICBHI_TEST_PATH)\n",
        "    test_rec_annotations_dict[s] = a"
      ],
      "metadata": {
        "id": "SYwl0BvLo0Ob"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6-2. 성능 평가 코드"
      ],
      "metadata": {
        "id": "1UwO1izLruph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def BYOL_EVALUATION(test_audio_dir, CHECKPOINT_PATH, config_path='icbhi_config.yaml'):\n",
        "    # config.yaml 파일에서 설정값 불러오기\n",
        "    cfg = load_yaml_config(config_path)\n",
        "\n",
        "    logger = get_logger(__name__)        # 로깅 설정 (학습 진행 상황을 출력)\n",
        "    logger.info(cfg)                     # 설정값(cfg)을 로그로 출력\n",
        "    seed_everything(cfg.seed)            # cfg.seed = 42을 사용하여 랜덤 시드 고정\n",
        "\n",
        "    # wandb 초기화\n",
        "    wandb.init(\n",
        "        project=\"ICBHI_BYOL\",\n",
        "        name=f\"BYOL_Evaluation_{get_timestamp()}\",\n",
        "        config={\"batch_size\": cfg.bs, \"sample_rate\": cfg.sample_rate, \"spectrogram size\": cfg.shape}\n",
        "    )\n",
        "\n",
        "    # Test 데이터 로드 (증강x)\n",
        "    test_files = sorted(Path(test_audio_dir).glob('*.wav'))\n",
        "    test_tfms = AugmentationModule(tuple(cfg.shape), 2*len(test_files))\n",
        "    test_ds = WaveInLMSOutDataset(cfg, 'test', test_files, labels=True, tfms=False)\n",
        "    test_dl = DataLoader(test_ds, batch_size=cfg.bs, num_workers=multiprocessing.cpu_count(), pin_memory=True, shuffle=False)\n",
        "    print(f'Test Dataset: {len(test_files)} .wav files from {test_audio_dir}')\n",
        "\n",
        "    # Fine-tuning된 가중치 불러오기\n",
        "    finetuned_weights = torch.load(CHECKPOINT_PATH, map_location='cuda')\n",
        "\n",
        "    # Model 정의\n",
        "    base_model = AudioNTT2020(n_mels=cfg.n_mels, d=cfg.feature_d)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = FineTunedModel(base_model, num_classes=4, feature_dim=cfg.feature_d).to(device)\n",
        "\n",
        "    # 가중치 로드\n",
        "    model.load_state_dict(finetuned_weights)\n",
        "\n",
        "    # 모델 평가\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, labels) in enumerate(test_dl):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())    # 정답 라벨 저장\n",
        "            all_preds.extend(predicted.cpu().numpy())  # 예측값 저장\n",
        "\n",
        "            # 배치 진행 상황 출력\n",
        "            print(f\"Processed batch {batch_idx+1}/{len(test_dl)} - Accuracy so far: {100 * correct / total:.2f}%\")\n",
        "\n",
        "    # 혼동 행렬 계산\n",
        "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "    print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "\n",
        "    # 클래스별 Sensitivity(민감도)와 Specificity(특이도) 계산\n",
        "    sensitivity_list = []\n",
        "    specificity_list = []\n",
        "\n",
        "    for i in range(len(conf_matrix)):  # 클래스 개수만큼 반복\n",
        "        TP = conf_matrix[i, i]                   # True Positive\n",
        "        FN = sum(conf_matrix[i, :]) - TP         # False Negative\n",
        "        FP = sum(conf_matrix[:, i]) - TP         # False Positive\n",
        "        TN = conf_matrix.sum() - (TP + FN + FP)  # True Negative\n",
        "\n",
        "        sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "        specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
        "\n",
        "        sensitivity_list.append(sensitivity)\n",
        "        specificity_list.append(specificity)\n",
        "\n",
        "        # 클래스별 민감도, 특이도 로그 출력\n",
        "        print(f\"Class {i}: Sensitivity = {sensitivity:.4f}, Specificity = {specificity:.4f}\")\n",
        "\n",
        "    # 평균 민감도 & 특이도 계산\n",
        "    accuracy = (correct / total) * 100\n",
        "    mean_sensitivity = np.mean(sensitivity_list)\n",
        "    mean_specificity = np.mean(specificity_list)\n",
        "\n",
        "    print(f\"\\nFinal Results:\")\n",
        "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
        "    print(f\"Mean Sensitivity: {mean_sensitivity:.4f}\")\n",
        "    print(f\"Mean Specificity: {mean_specificity:.4f}\")\n",
        "    print(f\"Average Score: {(mean_sensitivity+mean_specificity)/2:.4f}\")\n",
        "\n",
        "    def log_confusion_matrix_wandb(y_true, y_pred, class_names):\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        fig, ax = plt.subplots(figsize=(6, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                    xticklabels=class_names, yticklabels=class_names, ax=ax)\n",
        "        ax.set_xlabel('Predicted')\n",
        "        ax.set_ylabel('True')\n",
        "        ax.set_title('Confusion Matrix')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # wandb에 이미지로 로그\n",
        "        wandb.log({\"confusion_matrix\": wandb.Image(fig)})\n",
        "        plt.close(fig)\n",
        "\n",
        "    # 혼동행렬은 따로 이미지로 출력\n",
        "    log_confusion_matrix_wandb(\n",
        "        y_true=np.array(all_labels),\n",
        "        y_pred=np.array(all_preds),\n",
        "        class_names=[\"Normal\", \"Creekle\", \"Wheezle\", \"Both\"]\n",
        "    )\n",
        "\n",
        "    wandb.finish()"
      ],
      "metadata": {
        "id": "j6EzN_8LzJg_"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6-3. 성능 평가"
      ],
      "metadata": {
        "id": "5e183CQmtzle"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FILE_NAME_2 = \"BYOLA-Finetuned-d2048s64x258-2504030319-e100-bs64-lr0003-rs42.pth\"\n",
        "# CHECKPOINT_PATH_2 = os.path.join(PRETRAINED_MODEL_PATH, FILE_NAME_2)"
      ],
      "metadata": {
        "id": "UNstIvGzt7E_"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BYOL_EVALUATION(ICBHI_TEST_PATH, CHECKPOINT_PATH_2, config_path='icbhi_config.yaml')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 975
        },
        "id": "DyyV0oLIt_MK",
        "outputId": "4abcbab5-8489-4e3c-d498-7e66353d7f05"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250403_123251-lx2kjwy2</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/boaz_woony-boaz/ICBHI_BYOL/runs/lx2kjwy2' target=\"_blank\">BYOL_Evaluation_2504031232</a></strong> to <a href='https://wandb.ai/boaz_woony-boaz/ICBHI_BYOL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/boaz_woony-boaz/ICBHI_BYOL' target=\"_blank\">https://wandb.ai/boaz_woony-boaz/ICBHI_BYOL</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/boaz_woony-boaz/ICBHI_BYOL/runs/lx2kjwy2' target=\"_blank\">https://wandb.ai/boaz_woony-boaz/ICBHI_BYOL/runs/lx2kjwy2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Augmentations: Sequential(\n",
            "  (0): MixupBYOLA(ratio=0.4,n=2048,log_mixup_exp=True)\n",
            "  (1): RandomResizeCrop(virtual_crop_size=(1.0, 1.5), time_scale=(0.6, 1.5), freq_scale=(0.6, 1.5))\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (64) may be set too high. Or, the value for `n_freqs` (257) may be set too low.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Dataset: 170 .wav files from /content/drive/MyDrive/ADV 프로젝트/data/ICBHI/test\n",
            "Processed batch 1/21 - Accuracy so far: 34.38%\n",
            "Processed batch 2/21 - Accuracy so far: 39.84%\n",
            "Processed batch 3/21 - Accuracy so far: 32.29%\n",
            "Processed batch 4/21 - Accuracy so far: 27.34%\n",
            "Processed batch 5/21 - Accuracy so far: 34.38%\n",
            "Processed batch 6/21 - Accuracy so far: 35.42%\n",
            "Processed batch 7/21 - Accuracy so far: 36.61%\n",
            "Processed batch 8/21 - Accuracy so far: 36.33%\n",
            "Processed batch 9/21 - Accuracy so far: 35.24%\n",
            "Processed batch 10/21 - Accuracy so far: 39.53%\n",
            "Processed batch 11/21 - Accuracy so far: 39.20%\n",
            "Processed batch 12/21 - Accuracy so far: 41.93%\n",
            "Processed batch 13/21 - Accuracy so far: 43.27%\n",
            "Processed batch 14/21 - Accuracy so far: 43.42%\n",
            "Processed batch 15/21 - Accuracy so far: 44.58%\n",
            "Processed batch 16/21 - Accuracy so far: 43.07%\n",
            "Processed batch 17/21 - Accuracy so far: 44.30%\n",
            "Processed batch 18/21 - Accuracy so far: 44.62%\n",
            "Processed batch 19/21 - Accuracy so far: 45.15%\n",
            "Processed batch 20/21 - Accuracy so far: 45.31%\n",
            "Processed batch 21/21 - Accuracy so far: 46.11%\n",
            "Confusion Matrix:\n",
            " [[502 118 143  39]\n",
            " [129  51  83  26]\n",
            " [ 79   4  45   6]\n",
            " [ 37   7  42  12]]\n",
            "Class 0: Sensitivity = 0.6259, Specificity = 0.5298\n",
            "Class 1: Sensitivity = 0.1765, Specificity = 0.8752\n",
            "Class 2: Sensitivity = 0.3358, Specificity = 0.7746\n",
            "Class 3: Sensitivity = 0.1224, Specificity = 0.9420\n",
            "\n",
            "Final Results:\n",
            "Accuracy: 46.11%\n",
            "Mean Sensitivity: 0.3152\n",
            "Mean Specificity: 0.7804\n",
            "Average Score: 0.5478\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">BYOL_Evaluation_2504031232</strong> at: <a href='https://wandb.ai/boaz_woony-boaz/ICBHI_BYOL/runs/lx2kjwy2' target=\"_blank\">https://wandb.ai/boaz_woony-boaz/ICBHI_BYOL/runs/lx2kjwy2</a><br> View project at: <a href='https://wandb.ai/boaz_woony-boaz/ICBHI_BYOL' target=\"_blank\">https://wandb.ai/boaz_woony-boaz/ICBHI_BYOL</a><br>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250403_123251-lx2kjwy2/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "OfVeQFU7z9go",
        "qM6jvRXD0ZnJ",
        "oLEk_jop0cCV",
        "PbTj_j4SJeon",
        "SGTGA4qvwKB2",
        "XavJi3_eDWqW",
        "9GKbLHbIznaR",
        "y-9scNDMDdRx",
        "oluurwU9EyLf",
        "yeb3I6vKKBhf",
        "-4HzJqqbFI9b",
        "5m9KnO-xEMll",
        "s5KOyYSlWmJo",
        "510saN3lDOfe",
        "sfjd08qqKIzS",
        "1UwO1izLruph"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "139c781195ee4ce88f44010f932b075e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2552bf31359b4b0c9b8f835f00f88184",
              "IPY_MODEL_b48c0825e8eb46598c23f5c6d9cbcc5c",
              "IPY_MODEL_6bf02a6654f5466ea5448afef86a6884"
            ],
            "layout": "IPY_MODEL_cf5a977434a04fa0bb13b292f27e81d4"
          }
        },
        "2552bf31359b4b0c9b8f835f00f88184": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf8d1b5b69e348b4a584b0a838bc8a6d",
            "placeholder": "​",
            "style": "IPY_MODEL_2f99a8a6cdb84103a319c976a1646c2e",
            "value": "Epoch 99: 100%"
          }
        },
        "b48c0825e8eb46598c23f5c6d9cbcc5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c7ac2ac58de47729449b4013990bf10",
            "max": 70,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f8c9110a444749718e71a117c6d45d7c",
            "value": 70
          }
        },
        "6bf02a6654f5466ea5448afef86a6884": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6dda7c97e80344fd9e9433f2d4bcef58",
            "placeholder": "​",
            "style": "IPY_MODEL_1f8e3d0ee3e846bca8c6e6d972e594e5",
            "value": " 70/70 [00:34&lt;00:00,  2.01it/s, v_num=1o5o, train_loss=0.284, learning_rate=3e-6, mb=0.0867, sb=0.768, ma=1.41e-9, sa=1.000]"
          }
        },
        "cf5a977434a04fa0bb13b292f27e81d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "bf8d1b5b69e348b4a584b0a838bc8a6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f99a8a6cdb84103a319c976a1646c2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7c7ac2ac58de47729449b4013990bf10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8c9110a444749718e71a117c6d45d7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6dda7c97e80344fd9e9433f2d4bcef58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f8e3d0ee3e846bca8c6e6d972e594e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}