{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WrQQXU9Fff9"
      },
      "source": [
        "**4월 21일 변경사항**\n",
        "1. Fine-tuning, Test 내부 코드 수정\n",
        "  - 신경망 입력 전 정규화, Classifier층 입력 전 정규화\n",
        "  - Batch 단위로 입력하며 gradient를 계산하지 않고, full-batch로 연산\n",
        "2. Weight_decay Scheduler가 적용이 안 됐길래 수정\n",
        "\n",
        "**확인된 특이사항**\n",
        "1. ICBHI 공식 split에 따르면 동일 환자는 train/test 중 하나의 set에만 들어가도록 분할했다고 함.  \n",
        "  그런데 실제로 데이터를 확인해보면 156번, 218번 환자의 청진음들은 train/test set에 섞여 있음\n",
        "2. 청진음 파일 하나의 이름을 아래와 같이 수정해야 함\n",
        "  - 226_1b1_Pl_sc_LittC2SE.wav -> 226_1b1_Pl_sc_Meditron.wav\n",
        "  - 226_1b1_Pl_sc_LittC2SE.txt -> 226_1b1_Pl_sc_Meditron.txt\n",
        "\n",
        "**BYOL-A 실험 세팅**   \n",
        "\n",
        "1. 데이터 분할: ICBHI train-test 분할(6:4) 공식 기준에 맞게 분리  \n",
        " - Train Data: 539, Test Data: 381, Total: 920  \n",
        "2. Train data 내 사전훈련 / Fine-tuning 데이터를 분리할 때 동일 환자의 청진음은 하나의 set에만 들어가도록 분리  \n",
        " - 사전훈련 / Fine-tuning 8:2로 분리\n",
        " - Fine-tuning 시 Validtaion set 만들지 않는 걸로 변경\n",
        " - Test Data는 파인튜닝까지 마친 모델을 평가하는 용도로만 사용\n",
        "3. Augmentation은 BYOL-A 논문 저자들이 제안한 방법을 사용\n",
        " - 이 방법으로 고정하여 실험중이므로 SpecAugmentation은 아직 사용하지 않음\n",
        " - 논문 저자들이 제안한 방법  \n",
        "  1) Mixup for foreground acoustic event: 과거에 무작위로 선택된 입력 오디오를 소량 비율로 혼합  \n",
        "  2) Randomly Resize&Crop: 무작위로 크롭 영역 샘플링 -> bicubic interpolation으로 크기 복구   \n",
        "4. 전처리 세부사항  \n",
        " - 호흡 주기로 끊어준 음성을 5초 길이로 고정\n",
        " - 5초보다 짧으면 -> 양 옆 제로 패딩 (음성이 너무 짧으면 충분히 반복해 준 후 양 옆 제로 패딩)\n",
        " - 5초보다 길면 -> 랜덤 크롭만 수행\n",
        "5. 사전훈련 세부사항\n",
        " - 옵티마이저: AdamW\n",
        " - 활성화함수: GELU\n",
        " - Epoch: 최대 200 epoch\n",
        " - Sampling rate: 16kHz\n",
        " - Weight Decay = 0.4 → 0.04 (코사인 스케줄러)\n",
        " - lr 3e-05 → 3e-04 (선형 증가) → 1e-06 (코사인 스케줄러)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egM1OwiPbNvp"
      },
      "source": [
        "#### 환경 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 568,
          "status": "ok",
          "timestamp": 1745181025111,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "L0yOvq732Pvn",
        "outputId": "cee1922e-ec86-480b-c25f-9c6a582a0e96"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'jupyter (Python 3.11.2)' requires the ipykernel package.\n",
            "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages."
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "executionInfo": {
          "elapsed": 4210,
          "status": "ok",
          "timestamp": 1745181108795,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "A4Fh_qklCYp1",
        "outputId": "e3fba6fb-932f-4bc1-d143-2cfd2997f296"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (6.0.2)\n",
            "Collecting pytorch_lightning\n",
            "  Downloading pytorch_lightning-2.5.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.2.post1)\n",
            "Requirement already satisfied: easydict in /usr/local/lib/python3.10/dist-packages (1.13)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.19.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.1.0)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch_lightning)\n",
            "  Downloading torchmetrics-1.7.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (24.2)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch_lightning)\n",
            "  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.6.0)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.13.0)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.25.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.10.4)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.19.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (69.5.1)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (3.11.11)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.12.14)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.18.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Downloading pytorch_lightning-2.5.1-py3-none-any.whl (822 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.0/823.0 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n",
            "Downloading torchmetrics-1.7.1-py3-none-any.whl (961 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m961.5/961.5 kB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lightning-utilities, torchmetrics, pytorch_lightning\n",
            "Successfully installed lightning-utilities-0.14.3 pytorch_lightning-2.5.1 torchmetrics-1.7.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchaudio torchvision pyyaml pytorch_lightning librosa easydict tqdm wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "executionInfo": {
          "elapsed": 19281,
          "status": "ok",
          "timestamp": 1745181128066,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "t4w8WkMWwwdp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import random\n",
        "import re\n",
        "import logging\n",
        "import math\n",
        "import yaml\n",
        "import datetime\n",
        "import pickle\n",
        "import librosa\n",
        "from pathlib import Path\n",
        "from easydict import EasyDict\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "from zoneinfo import ZoneInfo\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchaudio\n",
        "import pytorch_lightning as pl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import multiprocessing\n",
        "import wandb\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn.functional as F\n",
        "import torchaudio.transforms as T\n",
        "import torch.optim as optim\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from pytorch_lightning.callbacks import ModelSummary, LearningRateMonitor, ModelCheckpoint\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "from sklearn.manifold import TSNE\n",
        "from collections import Counter, defaultdict\n",
        "from itertools import chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 3652,
          "status": "ok",
          "timestamp": 1745181131708,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "W6KgSeKT4Ml7",
        "outputId": "07f46f24-3ab0-461b-cfc7-a40cab9dc274"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "gcsfuse is already the newest version (2.11.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 179 not upgraded.\n",
            "{\"timestamp\":{\"seconds\":1745181130,\"nanos\":922025887},\"severity\":\"INFO\",\"message\":\"Start gcsfuse/2.11.1 (Go version go1.24.0) for app \\\"\\\" using mount point: /mnt/gcs\\n\"}\n",
            "{\"timestamp\":{\"seconds\":1745181130,\"nanos\":922080084},\"severity\":\"INFO\",\"message\":\"GCSFuse config\",\"config\":{\"AppName\":\"\",\"CacheDir\":\"\",\"Debug\":{\"ExitOnInvariantViolation\":false,\"Fuse\":false,\"Gcs\":false,\"LogMutex\":false},\"EnableAtomicRenameObject\":false,\"EnableHns\":true,\"FileCache\":{\"CacheFileForRangeRead\":false,\"DownloadChunkSizeMb\":50,\"EnableCrc\":false,\"EnableODirect\":false,\"EnableParallelDownloads\":false,\"ExperimentalParallelDownloadsDefaultOn\":false,\"MaxParallelDownloads\":16,\"MaxSizeMb\":-1,\"ParallelDownloadsPerFile\":16,\"WriteBufferSize\":4194304},\"FileSystem\":{\"DirMode\":\"755\",\"DisableParallelDirops\":false,\"FileMode\":\"644\",\"FuseOptions\":[],\"Gid\":-1,\"HandleSigterm\":true,\"IgnoreInterrupts\":true,\"KernelListCacheTtlSecs\":0,\"PreconditionErrors\":true,\"RenameDirLimit\":0,\"TempDir\":\"\",\"Uid\":-1},\"Foreground\":false,\"GcsAuth\":{\"AnonymousAccess\":false,\"KeyFile\":\"\",\"ReuseTokenFromUrl\":true,\"TokenUrl\":\"\"},\"GcsConnection\":{\"BillingProject\":\"\",\"ClientProtocol\":\"http1\",\"CustomEndpoint\":\"\",\"ExperimentalEnableJsonRead\":false,\"GrpcConnPoolSize\":1,\"HttpClientTimeout\":0,\"LimitBytesPerSec\":-1,\"LimitOpsPerSec\":-1,\"MaxConnsPerHost\":0,\"MaxIdleConnsPerHost\":100,\"SequentialReadSizeMb\":200},\"GcsRetries\":{\"ChunkTransferTimeoutSecs\":10,\"MaxRetryAttempts\":0,\"MaxRetrySleep\":30000000000,\"Multiplier\":2,\"ReadStall\":{\"Enable\":false,\"InitialReqTimeout\":20000000000,\"MaxReqTimeout\":1200000000000,\"MinReqTimeout\":1500000000,\"ReqIncreaseRate\":15,\"ReqTargetPercentile\":0.99}},\"ImplicitDirs\":false,\"List\":{\"EnableEmptyManagedFolders\":false},\"Logging\":{\"FilePath\":\"\",\"Format\":\"json\",\"LogRotate\":{\"BackupFileCount\":10,\"Compress\":true,\"MaxFileSizeMb\":512},\"Severity\":\"INFO\"},\"MetadataCache\":{\"DeprecatedStatCacheCapacity\":20460,\"DeprecatedStatCacheTtl\":60000000000,\"DeprecatedTypeCacheTtl\":60000000000,\"EnableNonexistentTypeCache\":false,\"ExperimentalMetadataPrefetchOnMount\":\"disabled\",\"NegativeTtlSecs\":5,\"StatCacheMaxSizeMb\":32,\"TtlSecs\":60,\"TypeCacheMaxSizeMb\":4},\"Metrics\":{\"CloudMetricsExportIntervalSecs\":0,\"EnableOtel\":true,\"PrometheusPort\":0,\"StackdriverExportInterval\":0},\"Monitoring\":{\"ExperimentalOpentelemetryCollectorAddress\":\"\",\"ExperimentalTracingMode\":\"\",\"ExperimentalTracingSamplingRatio\":0},\"OnlyDir\":\"\",\"Write\":{\"BlockSizeMb\":33554432,\"CreateEmptyFile\":false,\"EnableStreamingWrites\":false,\"GlobalMaxBlocks\":9223372036854775807,\"MaxBlocksPerFile\":1}}}\n",
            "{\"timestamp\":{\"seconds\":1745181131,\"nanos\":810540392},\"severity\":\"INFO\",\"message\":\"File system has been successfully mounted.\"}\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get install -y gcsfuse\n",
        "!mkdir -p /mnt/gcs\n",
        "!gcsfuse hyeonseok_dataset /mnt/gcs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 219,
          "status": "ok",
          "timestamp": 1745181131919,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "JOsYx9Tb2a2W",
        "outputId": "27913ebe-2a83-4d5b-a49d-9da1aed60dc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CHECKPOINT  HF_Lung_V1-master  ICBHI  pickle  SPRSound\ttrain_test_split.txt\n"
          ]
        }
      ],
      "source": [
        "!ls /mnt/gcs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 380,
          "status": "ok",
          "timestamp": 1745181132294,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "uUDAB45N2dEg",
        "outputId": "d62c3bd6-206b-4351-a795-60fa91cb254e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1840개의 파일이 확인되었습니다.\n"
          ]
        }
      ],
      "source": [
        "GCS_DATA_PATH = \"/mnt/gcs/ICBHI\"\n",
        "\n",
        "count = 0\n",
        "if os.path.exists(GCS_DATA_PATH):\n",
        "    for fname in os.listdir(GCS_DATA_PATH):\n",
        "        count += 1\n",
        "    print(f\"{count}개의 파일이 확인되었습니다.\")\n",
        "else:\n",
        "    print(\"해당 경로가 존재하지 않습니다.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "executionInfo": {
          "elapsed": 6,
          "status": "ok",
          "timestamp": 1745181132294,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "IOXsH0nr2jhy"
      },
      "outputs": [],
      "source": [
        "GCS_DATA_PATH = \"/mnt/gcs/ICBHI\"\n",
        "PRETRAINED_MODEL_PATH = \"/mnt/gcs/CHECKPOINT\"\n",
        "PICKLE_PATH = \"/mnt/gcs/pickle\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 7696,
          "status": "ok",
          "timestamp": 1745181139985,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "9KVJJMHdBDyA",
        "outputId": "36b3d42c-aae8-4039-ac0e-3b70c6d594e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ],
      "source": [
        "!wandb login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCviD6mvgfGi"
      },
      "source": [
        "## 0. 데이터셋 구성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hDbz2L0s9Sb"
      },
      "source": [
        "#### 0-1. Train-Test Split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3AVbrT09Sv5"
      },
      "source": [
        "파일이름 변경  \n",
        "- 226_1b1_Pl_sc_LittC2SE.wav -> 226_1b1_Pl_sc_Meditron.wav\n",
        "- 226_1b1_Pl_sc_LittC2SE.txt -> 226_1b1_Pl_sc_Meditron.txt  \n",
        "  \n",
        "텍스트 파일 로드 (ICBHI 공식 train-test 분리 기준)\n",
        "- info: 파일명에서 얻은 청진 정보 목록  \n",
        "- rec_annotations: 호흡 주기별 label 목록"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "executionInfo": {
          "elapsed": 14,
          "status": "ok",
          "timestamp": 1745181139986,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "qEQ0XCQYggtw"
      },
      "outputs": [],
      "source": [
        "patient_number_dict = []\n",
        "data_split_list = []\n",
        "file_name_list = []\n",
        "\n",
        "text = \"/mnt/gcs/train_test_split.txt\"\n",
        "\n",
        "with open(text, \"r\") as file:\n",
        "    for line in file:\n",
        "        name, split = line.strip().split(\"\\t\")  # txt 파일의 각 줄을 탭으로 분리\n",
        "        number = int(name.split('_')[0])        # 파일명에서 맨 앞 환자번호 분리\n",
        "\n",
        "        patient_number_dict.append(number)\n",
        "        data_split_list.append(split)\n",
        "        file_name_list.append(name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fda9T4cHjMx7"
      },
      "source": [
        "##### pickle로 외부 저장"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "executionInfo": {
          "elapsed": 11,
          "status": "ok",
          "timestamp": 1745181139986,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "C4c2V1PFEp4y"
      },
      "outputs": [],
      "source": [
        "# def Extract_Annotation_Data(file_name, root):\n",
        "#     tokens = file_name.split('_')\n",
        "#     recording_info = pd.DataFrame(data = [tokens], columns = ['Patient number', 'Recording index', 'Chest location','Acquisition mode','Recording equipment'])\n",
        "#     recording_annotations = pd.read_csv(os.path.join(root, file_name + '.txt'), names = ['Start', 'End', 'Crackles', 'Wheezes'], delimiter= '\\t')\n",
        "#     return (recording_info, recording_annotations)\n",
        "\n",
        "# train_rec_info_dict = {}\n",
        "# train_rec_annotations_dict = {}\n",
        "# test_rec_info_dict = {}\n",
        "# test_rec_annotations_dict = {}\n",
        "\n",
        "# for i, filename in enumerate(file_name_list):\n",
        "#     split = data_split_list[i]\n",
        "#     (info, annotations) = Extract_Annotation_Data(filename, ROOT)\n",
        "#     if split == 'train':\n",
        "#         train_rec_info_dict[filename] = info\n",
        "#         train_rec_annotations_dict[filename] = annotations\n",
        "#     elif split == 'test':\n",
        "#         test_rec_info_dict[filename] = info\n",
        "#         test_rec_annotations_dict[filename] = annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "executionInfo": {
          "elapsed": 9,
          "status": "ok",
          "timestamp": 1745181139986,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "JdvGm_TxhArZ"
      },
      "outputs": [],
      "source": [
        "# pickle_dict = {\n",
        "#     'train_rec_info_dict': train_rec_info_dict,\n",
        "#     'train_rec_annotations_dict': train_rec_annotations_dict,\n",
        "#     'test_rec_info_dict': test_rec_info_dict,\n",
        "#     'test_rec_annotations_dict': test_rec_annotations_dict\n",
        "# }\n",
        "\n",
        "# save_path = os.path.join(PICKLE_PATH, 'saved_data.pkl')\n",
        "# with open(save_path, 'wb') as f:\n",
        "#     pickle.dump(pickle_dict, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgQ7ZxdPjSle"
      },
      "source": [
        "##### 저장된 데이터 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "executionInfo": {
          "elapsed": 1131,
          "status": "ok",
          "timestamp": 1745181141109,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "Mk-X8PjljJ_b"
      },
      "outputs": [],
      "source": [
        "save_path = os.path.join(PICKLE_PATH, 'saved_data.pkl')\n",
        "with open(save_path, 'rb') as f:\n",
        "    pickle_dict = pickle.load(f)\n",
        "\n",
        "train_rec_info_dict = pickle_dict['train_rec_info_dict']\n",
        "train_rec_annotations_dict = pickle_dict['train_rec_annotations_dict']\n",
        "test_rec_info_dict = pickle_dict['test_rec_info_dict']\n",
        "test_rec_annotations_dict = pickle_dict['test_rec_annotations_dict']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 29,
          "status": "ok",
          "timestamp": 1745181141109,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "uIL-oNVPEvxe",
        "outputId": "1a1246ff-124f-433b-d6eb-f7c7869933e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 539, Test: 381, Total: 920\n"
          ]
        }
      ],
      "source": [
        "# 데이터 개수 확인\n",
        "print(f\"Train: {len(train_rec_annotations_dict)}, Test: {len(test_rec_annotations_dict)}, Total: {len(train_rec_annotations_dict)+len(test_rec_annotations_dict)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aD9zwuY9tH7z"
      },
      "source": [
        "#### 0-2. 사전훈련-파인튜닝 Split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMv48_Qu9VW0"
      },
      "source": [
        "목표 : Train 데이터를 사전훈련 / 파인튜닝 데이터로 재분리 (환자 단위로 분할하여 환자끼리 안 겹치도록 함)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 25,
          "status": "ok",
          "timestamp": 1745181141110,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "SM08kCG17PuW",
        "outputId": "bebfb4d0-1d63-4049-d11c-85f6eb5c5bcb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 79, Test: 49\n"
          ]
        }
      ],
      "source": [
        "ICBHI_df = pd.DataFrame(data = {'Patient number': patient_number_dict, 'Split': data_split_list}, index=file_name_list)\n",
        "train_df = ICBHI_df[ICBHI_df['Split']=='train']\n",
        "test_df = ICBHI_df[ICBHI_df['Split']=='test']\n",
        "\n",
        "train_patient_counts = train_df['Patient number'].value_counts()\n",
        "test_patient_counts = test_df['Patient number'].value_counts()\n",
        "\n",
        "# 환자 수 정보\n",
        "print(f\"Train: {len(train_patient_counts)}, Test: {len(test_patient_counts)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "executionInfo": {
          "elapsed": 23,
          "status": "ok",
          "timestamp": 1745181141110,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "dpaiBhJNWwoJ",
        "outputId": "ab670dcf-22bf-4d9b-c142-09d908a33f83"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Patient number</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>130</th>\n",
              "      <td>66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107</th>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>138</th>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>172</th>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186</th>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ],
            "text/plain": [
              "Patient number\n",
              "130    66\n",
              "107    28\n",
              "138    27\n",
              "172    27\n",
              "186    24\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_patient_counts.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pT7to2QLW0sj"
      },
      "source": [
        "80:20으로 데이터 분할  \n",
        "이때 130번 환자는 데이터 수가 너무 많으므로, 과적합을 방지하기 위해 Fine-tuning 데이터셋에 미리 할당"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 19,
          "status": "ok",
          "timestamp": 1745181141111,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "GHMLAmXK7ov7",
        "outputId": "56dc77a6-cd13-45e4-d99c-80e2edf42a47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "사전훈련 데이터 수: 428, 환자 수: 65\n",
            "파인튜닝 데이터 수: 111, 환자 수: 14\n",
            "평가 데이터 수: 381, 환자 수: 49\n"
          ]
        }
      ],
      "source": [
        "# 랜덤하게 데이터 혼합\n",
        "random.seed(42)\n",
        "shuffled_patients = list(train_patient_counts.index)\n",
        "shuffled_patients.remove(130)        # 130번 환자 제거\n",
        "random.shuffle(shuffled_patients)\n",
        "\n",
        "# 80%가 될 때까지 데이터 학습\n",
        "total = train_patient_counts.sum()\n",
        "pretrain_patients = [130]\n",
        "pretrain_count = train_patient_counts[130]  # 130번 환자 미리 할당\n",
        "\n",
        "for p in shuffled_patients:\n",
        "    if pretrain_count + train_patient_counts[p] <= total * 0.8:\n",
        "        pretrain_patients.append(p)\n",
        "        pretrain_count += train_patient_counts[p]\n",
        "    else:\n",
        "        break\n",
        "\n",
        "# 나머지 환자는 finetune 그룹\n",
        "finetune_patients = [p for p in shuffled_patients if p not in pretrain_patients]\n",
        "\n",
        "# 데이터프레임 분할\n",
        "pretrain_df = train_df[train_df['Patient number'].isin(pretrain_patients)]\n",
        "finetune_df = train_df[train_df['Patient number'].isin(finetune_patients)]\n",
        "\n",
        "# 결과 확인\n",
        "print(f\"사전훈련 데이터 수: {len(pretrain_df)}, 환자 수: {len(pretrain_patients)}\")\n",
        "print(f\"파인튜닝 데이터 수: {len(finetune_df)}, 환자 수: {len(finetune_patients)}\")\n",
        "print(f\"평가 데이터 수: {len(test_df)}, 환자 수: {len(test_patient_counts)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3v6Wf9oc-Nc"
      },
      "source": [
        "사전훈련 / 파인튜닝 info_dict와 annotations_dict 구축"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "executionInfo": {
          "elapsed": 16,
          "status": "ok",
          "timestamp": 1745181141111,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "BoOw3d_ss78A"
      },
      "outputs": [],
      "source": [
        "# 결과 딕셔너리 생성\n",
        "split_info_data = {\n",
        "    \"pretrain\": {key: train_rec_info_dict[key] for key in list(pretrain_df.index)},\n",
        "    \"finetune\": {key: train_rec_info_dict[key] for key in list(finetune_df.index)},\n",
        "}\n",
        "split_annotations_data = {\n",
        "    \"pretrain\": {key: train_rec_annotations_dict[key] for key in list(pretrain_df.index)},\n",
        "    \"finetune\": {key: train_rec_annotations_dict[key] for key in list(finetune_df.index)},\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 15,
          "status": "ok",
          "timestamp": 1745181141111,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "d57lUSiDxsb4",
        "outputId": "15b1abe5-1071-4c1e-9d34-45217c1bf3ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "사전훈련용 info 데이터       : 428, 파인튜닝용 info 데이터        : 111\n",
            "사전훈련용 annotation 데이터 : 428, 파인튜닝용 annotation 데이터  : 111\n"
          ]
        }
      ],
      "source": [
        "# 사전훈련 / 파인튜닝 데이터의 info 데이터 구축 ('Patient number', 'Recording index', 'Chest location','Acquisition mode','Recording equipment')\n",
        "pretrain_rec_info_dict = split_info_data['pretrain']\n",
        "finetune_rec_info_dict = split_info_data['finetune']\n",
        "\n",
        "# 사전훈련 / 파인튜닝 데이터의 annotation 데이터 구축 ('Start', 'End', 'Crackles', 'Wheezes')\n",
        "pretrain_rec_annotations_dict = split_annotations_data['pretrain']\n",
        "finetune_rec_annotations_dict = split_annotations_data['finetune']\n",
        "\n",
        "# 데이터 개수 확인\n",
        "print(f\"사전훈련용 info 데이터       : {len(pretrain_rec_info_dict)}, 파인튜닝용 info 데이터        : {len(finetune_rec_info_dict)}\")\n",
        "print(f\"사전훈련용 annotation 데이터 : {len(pretrain_rec_annotations_dict)}, 파인튜닝용 annotation 데이터  : {len(finetune_rec_annotations_dict)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOmDXUX90Bkv"
      },
      "source": [
        "#### 0-3. 파일경로 데이터 구축"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 424,
          "status": "ok",
          "timestamp": 1745181237144,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "yfV8E5gpyatx",
        "outputId": "a2bd8f2d-b5c5-4af3-9806-bf6dacfae2dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "사전훈련용 데이터: 427, 파인튜닝용 데이터: 111, 평가용 데이터: 381\n"
          ]
        }
      ],
      "source": [
        "files = sorted(Path(GCS_DATA_PATH).glob('*.wav'))\n",
        "\n",
        "# split_annotations_data['pretrain']에 해당하는 파일명만 필터링\n",
        "pretrain_filenames = set(split_annotations_data[\"pretrain\"].keys())  # pretrain 데이터의 파일명 집합 생성\n",
        "pretrain_files = [f for f in files if f.stem in pretrain_filenames]  # 파일명(stem) 기준 필터링\n",
        "\n",
        "# split_annotations_data['finetune']에 해당하는 파일명만 필터링\n",
        "finetune_filenames = set(split_annotations_data[\"finetune\"].keys())  # finetune 데이터의 파일명 집합 생성\n",
        "finetune_files = [f for f in files if f.stem in finetune_filenames]  # 파일명(stem) 기준 필터링\n",
        "\n",
        "# test_rec_annotations_dict에 해당하는 파일명만 필터링\n",
        "test_filenames = set(test_rec_annotations_dict.keys())               # test 데이터의 파일명 집합 생성\n",
        "test_files = [f for f in files if f.stem in test_filenames]          # 파일명(stem) 기준 필터링\n",
        "\n",
        "# 데이터 개수 확인\n",
        "print(f\"사전훈련용 데이터: {len(pretrain_files)}, 파인튜닝용 데이터: {len(finetune_files)}, 평가용 데이터: {len(test_files)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfVeQFU7z9go"
      },
      "source": [
        "## 1. 데이터 EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qM6jvRXD0ZnJ"
      },
      "source": [
        "#### 1-1. 호흡 주기 분포 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "executionInfo": {
          "elapsed": 195,
          "status": "ok",
          "timestamp": 1745181237337,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "NR_G_urDgjRh"
      },
      "outputs": [],
      "source": [
        "def get_duration_list(rec_annotations_dict):\n",
        "    duration_list = []\n",
        "    rec_annotations = list(rec_annotations_dict.values())\n",
        "\n",
        "    for i in range(len(rec_annotations)):\n",
        "        current = rec_annotations[i]\n",
        "        duration = current['End'] - current['Start']\n",
        "        duration_list.extend(duration)\n",
        "\n",
        "    return np.array(duration_list)\n",
        "\n",
        "pretrain_duration_list = get_duration_list(pretrain_rec_annotations_dict)\n",
        "finetune_duration_list = get_duration_list(finetune_rec_annotations_dict)\n",
        "test_duration_list = get_duration_list(test_rec_annotations_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMVKXAU2fQ98"
      },
      "source": [
        "(1) 사전훈련 데이터"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "executionInfo": {
          "elapsed": 422,
          "status": "ok",
          "timestamp": 1745181237755,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "7vtaTZYB0CVp",
        "outputId": "6ad0a542-5e59-4349-d92d-19c7e6b140c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the number of cycles: 3268\n",
            "longest cycle: 16.163\n",
            "shortest cycle: 0.1999999999999993\n",
            "Fraction of samples less than 4 seconds:0.8996328029375765\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGdCAYAAAA44ojeAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAJYZJREFUeJzt3X9w1PWdx/FXfi4xsBsTzS45E0hbLET5JYGwwrVXzRFpauWIVZyIuZbRKZOgIS0HmeNH/XEE8VooFkhxHKFzcrbOHLaEAYxRY1tCwHBcETRiiyY23cQezS7gkITke3/02LsVBDZs8v1keT5mdsZ8v9/dvD+DbJ5897ubGMuyLAEAABgk1u4BAAAAPotAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGCceLsH6I++vj61tbVpxIgRiomJsXscAABwBSzL0qlTp5SRkaHY2EufIxmSgdLW1qbMzEy7xwAAAP3Q2tqqm2666ZLHDMlAGTFihKS/LtDpdNo8DQAAuBKBQECZmZnBn+OXMiQD5fzLOk6nk0ABAGCIuZLLM8K+SPaPf/yjHnzwQaWlpSkpKUnjx4/X22+/HdxvWZZWrlypkSNHKikpSfn5+Tp+/HjIY5w8eVLFxcVyOp1KSUnRggULdPr06XBHAQAAUSqsQPnLX/6iGTNmKCEhQbt379axY8f0wx/+UNdff33wmLVr12rDhg2qrq5WY2OjkpOTVVBQoLNnzwaPKS4u1tGjR1VbW6uamhq99dZbeuSRRyK3KgAAMKTFWJZlXenBy5Yt029/+1v9+te/vuh+y7KUkZGh733ve/r+978vSfL7/XK73dq6davmzZund999Vzk5OTp48KByc3MlSXv27NHXv/51ffzxx8rIyLjsHIFAQC6XS36/n5d4AAAYIsL5+R3WGZRf/epXys3N1be+9S2lp6dr8uTJeu6554L7T5w4IZ/Pp/z8/OA2l8ulvLw8NTQ0SJIaGhqUkpISjBNJys/PV2xsrBobGy/6fbu6uhQIBEJuAAAgeoUVKH/4wx+0efNmjRkzRnv37tXChQv16KOPatu2bZIkn88nSXK73SH3c7vdwX0+n0/p6ekh++Pj45Wamho85rOqqqrkcrmCN95iDABAdAsrUPr6+nTbbbdp9erVmjx5sh555BE9/PDDqq6uHqj5JEmVlZXy+/3BW2tr64B+PwAAYK+wAmXkyJHKyckJ2TZu3Di1tLRIkjwejySpvb095Jj29vbgPo/Ho46OjpD9586d08mTJ4PHfJbD4Qi+pZi3FgMAEP3CCpQZM2aoubk5ZNv777+vUaNGSZKys7Pl8XhUV1cX3B8IBNTY2Civ1ytJ8nq96uzsVFNTU/CY119/XX19fcrLy+v3QgAAQPQI64PaFi9erNtvv12rV6/WfffdpwMHDmjLli3asmWLpL9+8Ep5ebmeeuopjRkzRtnZ2VqxYoUyMjI0Z84cSX8943LXXXcFXxrq6elRWVmZ5s2bd0Xv4AEAANEvrLcZS1JNTY0qKyt1/PhxZWdnq6KiQg8//HBwv2VZWrVqlbZs2aLOzk7NnDlTmzZt0s033xw85uTJkyorK9POnTsVGxuroqIibdiwQcOHD7+iGXibMQAAQ084P7/DDhQTECgAAAw9A/Y5KAAAAIOBQAEAAMYhUAAAgHHCehcPhrbRy3Zd9pgP1xQOwiQAAFwaZ1AAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMaJt3sAmGX0sl2XPebDNYWDMAkA4FrGGRQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHHCCpQf/OAHiomJCbmNHTs2uP/s2bMqLS1VWlqahg8frqKiIrW3t4c8RktLiwoLC3XdddcpPT1dS5Ys0blz5yKzGgAAEBXiw73DLbfcotdee+3/HiD+/x5i8eLF2rVrl15++WW5XC6VlZVp7ty5+u1vfytJ6u3tVWFhoTwej/bt26c//elPeuihh5SQkKDVq1dHYDkAACAahB0o8fHx8ng8F2z3+/16/vnntX37dt1xxx2SpBdeeEHjxo3T/v37NX36dL366qs6duyYXnvtNbndbk2aNElPPvmkli5dqh/84AdKTEy8+hUBAIAhL+xrUI4fP66MjAx94QtfUHFxsVpaWiRJTU1N6unpUX5+fvDYsWPHKisrSw0NDZKkhoYGjR8/Xm63O3hMQUGBAoGAjh49+rnfs6urS4FAIOQGAACiV1iBkpeXp61bt2rPnj3avHmzTpw4ob/927/VqVOn5PP5lJiYqJSUlJD7uN1u+Xw+SZLP5wuJk/P7z+/7PFVVVXK5XMFbZmZmOGMDAIAhJqyXeGbPnh387wkTJigvL0+jRo3SL37xCyUlJUV8uPMqKytVUVER/DoQCBApAABEsat6m3FKSopuvvlmffDBB/J4POru7lZnZ2fIMe3t7cFrVjwezwXv6jn/9cWuaznP4XDI6XSG3AAAQPS6qkA5ffq0fv/732vkyJGaMmWKEhISVFdXF9zf3NyslpYWeb1eSZLX69WRI0fU0dERPKa2tlZOp1M5OTlXMwoAAIgiYb3E8/3vf1933323Ro0apba2Nq1atUpxcXF64IEH5HK5tGDBAlVUVCg1NVVOp1OLFi2S1+vV9OnTJUmzZs1STk6O5s+fr7Vr18rn82n58uUqLS2Vw+EYkAUCAIChJ6xA+fjjj/XAAw/ov//7v3XjjTdq5syZ2r9/v2688UZJ0rp16xQbG6uioiJ1dXWpoKBAmzZtCt4/Li5ONTU1Wrhwobxer5KTk1VSUqInnngisqsCAABDWoxlWZbdQ4QrEAjI5XLJ7/dzPUoYRi/bFZHH+XBNYUQeBwBwbQnn5ze/iwcAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMa5qkBZs2aNYmJiVF5eHtx29uxZlZaWKi0tTcOHD1dRUZHa29tD7tfS0qLCwkJdd911Sk9P15IlS3Tu3LmrGQUAAESRfgfKwYMH9dOf/lQTJkwI2b548WLt3LlTL7/8surr69XW1qa5c+cG9/f29qqwsFDd3d3at2+ftm3bpq1bt2rlypX9XwUAAIgq/QqU06dPq7i4WM8995yuv/764Ha/36/nn39eP/rRj3THHXdoypQpeuGFF7Rv3z7t379fkvTqq6/q2LFj+rd/+zdNmjRJs2fP1pNPPqmNGzequ7s7MqsCAABDWr8CpbS0VIWFhcrPzw/Z3tTUpJ6enpDtY8eOVVZWlhoaGiRJDQ0NGj9+vNxud/CYgoICBQIBHT169KLfr6urS4FAIOQGAACiV3y4d3jppZd06NAhHTx48IJ9Pp9PiYmJSklJCdnudrvl8/mCx/z/ODm///y+i6mqqtLjjz8e7qgAAGCICusMSmtrqx577DG9+OKLGjZs2EDNdIHKykr5/f7grbW1ddC+NwAAGHxhBUpTU5M6Ojp02223KT4+XvHx8aqvr9eGDRsUHx8vt9ut7u5udXZ2htyvvb1dHo9HkuTxeC54V8/5r88f81kOh0NOpzPkBgAAoldYgXLnnXfqyJEjOnz4cPCWm5ur4uLi4H8nJCSorq4ueJ/m5ma1tLTI6/VKkrxer44cOaKOjo7gMbW1tXI6ncrJyYnQsgAAwFAW1jUoI0aM0K233hqyLTk5WWlpacHtCxYsUEVFhVJTU+V0OrVo0SJ5vV5Nnz5dkjRr1izl5ORo/vz5Wrt2rXw+n5YvX67S0lI5HI4ILQsAAAxlYV8keznr1q1TbGysioqK1NXVpYKCAm3atCm4Py4uTjU1NVq4cKG8Xq+Sk5NVUlKiJ554ItKjAACAISrGsizL7iHCFQgE5HK55Pf7uR4lDKOX7YrI43y4pjAijwMAuLaE8/Ob38UDAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDjxdg+AyBi9bJfdIwAAEDGcQQEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHHi7R4AQ8/oZbsue8yHawoHYRIAQLTiDAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIwTVqBs3rxZEyZMkNPplNPplNfr1e7du4P7z549q9LSUqWlpWn48OEqKipSe3t7yGO0tLSosLBQ1113ndLT07VkyRKdO3cuMqsBAABRIaxAuemmm7RmzRo1NTXp7bff1h133KF77rlHR48elSQtXrxYO3fu1Msvv6z6+nq1tbVp7ty5wfv39vaqsLBQ3d3d2rdvn7Zt26atW7dq5cqVkV0VAAAY0mIsy7Ku5gFSU1P1zDPP6N5779WNN96o7du3695775Ukvffeexo3bpwaGho0ffp07d69W9/4xjfU1tYmt9stSaqurtbSpUv1ySefKDEx8Yq+ZyAQkMvlkt/vl9PpvJrxo8boZbvsHiHEh2sK7R4BAGCYcH5+9/salN7eXr300ks6c+aMvF6vmpqa1NPTo/z8/OAxY8eOVVZWlhoaGiRJDQ0NGj9+fDBOJKmgoECBQCB4FuZiurq6FAgEQm4AACB6hR0oR44c0fDhw+VwOPTd735XO3bsUE5Ojnw+nxITE5WSkhJyvNvtls/nkyT5fL6QODm///y+z1NVVSWXyxW8ZWZmhjs2AAAYQsIOlC9/+cs6fPiwGhsbtXDhQpWUlOjYsWMDMVtQZWWl/H5/8Nba2jqg3w8AANgrPtw7JCYm6ktf+pIkacqUKTp48KB+/OMf6/7771d3d7c6OztDzqK0t7fL4/FIkjwejw4cOBDyeOff5XP+mItxOBxyOBzhjgoAAIaoq/4clL6+PnV1dWnKlClKSEhQXV1dcF9zc7NaWlrk9XolSV6vV0eOHFFHR0fwmNraWjmdTuXk5FztKAAAIEqEdQalsrJSs2fPVlZWlk6dOqXt27frzTff1N69e+VyubRgwQJVVFQoNTVVTqdTixYtktfr1fTp0yVJs2bNUk5OjubPn6+1a9fK5/Np+fLlKi0t5QwJAAAICitQOjo69NBDD+lPf/qTXC6XJkyYoL179+rv//7vJUnr1q1TbGysioqK1NXVpYKCAm3atCl4/7i4ONXU1GjhwoXyer1KTk5WSUmJnnjiiciuCgAADGlX/TkoduBzUC7E56AAAEw3KJ+DAgAAMFAIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAceLtHgCXN3rZLrtHCNuVzPzhmsJBmAQAMBRxBgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAccIKlKqqKk2dOlUjRoxQenq65syZo+bm5pBjzp49q9LSUqWlpWn48OEqKipSe3t7yDEtLS0qLCzUddddp/T0dC1ZskTnzp27+tUAAICoEFag1NfXq7S0VPv371dtba16eno0a9YsnTlzJnjM4sWLtXPnTr388suqr69XW1ub5s6dG9zf29urwsJCdXd3a9++fdq2bZu2bt2qlStXRm5VAABgSIuxLMvq750/+eQTpaenq76+Xl/5ylfk9/t14403avv27br33nslSe+9957GjRunhoYGTZ8+Xbt379Y3vvENtbW1ye12S5Kqq6u1dOlSffLJJ0pMTLzs9w0EAnK5XPL7/XI6nf0df8gYvWyX3SMMiA/XFNo9AgBgEIXz8/uqrkHx+/2SpNTUVElSU1OTenp6lJ+fHzxm7NixysrKUkNDgySpoaFB48ePD8aJJBUUFCgQCOjo0aNXMw4AAIgS8f29Y19fn8rLyzVjxgzdeuutkiSfz6fExESlpKSEHOt2u+Xz+YLH/P84Ob///L6L6erqUldXV/DrQCDQ37EBAMAQ0O8zKKWlpXrnnXf00ksvRXKei6qqqpLL5QreMjMzB/x7AgAA+/QrUMrKylRTU6M33nhDN910U3C7x+NRd3e3Ojs7Q45vb2+Xx+MJHvPZd/Wc//r8MZ9VWVkpv98fvLW2tvZnbAAAMESEFSiWZamsrEw7duzQ66+/ruzs7JD9U6ZMUUJCgurq6oLbmpub1dLSIq/XK0nyer06cuSIOjo6gsfU1tbK6XQqJyfnot/X4XDI6XSG3AAAQPQK6xqU0tJSbd++Xb/85S81YsSI4DUjLpdLSUlJcrlcWrBggSoqKpSamiqn06lFixbJ6/Vq+vTpkqRZs2YpJydH8+fP19q1a+Xz+bR8+XKVlpbK4XBEfoUAAGDICStQNm/eLEn6u7/7u5DtL7zwgv7xH/9RkrRu3TrFxsaqqKhIXV1dKigo0KZNm4LHxsXFqaamRgsXLpTX61VycrJKSkr0xBNPXN1KAABA1Liqz0GxC5+DEh34HBQAuLYM2uegAAAADAQCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGibd7AOBSRi/bddljPlxTOAiTAAAGE2dQAACAcQgUAABgHAIFAAAYh2tQYJsrub4EAHBt4gwKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIzD24xtxlttAQC4EGdQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxeBfPAOIdOgAA9A9nUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHHCDpS33npLd999tzIyMhQTE6NXXnklZL9lWVq5cqVGjhyppKQk5efn6/jx4yHHnDx5UsXFxXI6nUpJSdGCBQt0+vTpq1oIAACIHmEHypkzZzRx4kRt3LjxovvXrl2rDRs2qLq6Wo2NjUpOTlZBQYHOnj0bPKa4uFhHjx5VbW2tampq9NZbb+mRRx7p/yoAAEBUiQ/3DrNnz9bs2bMvus+yLK1fv17Lly/XPffcI0n62c9+JrfbrVdeeUXz5s3Tu+++qz179ujgwYPKzc2VJD377LP6+te/rn/9139VRkbGVSwHAABEg4heg3LixAn5fD7l5+cHt7lcLuXl5amhoUGS1NDQoJSUlGCcSFJ+fr5iY2PV2Nh40cft6upSIBAIuQEAgOgV0UDx+XySJLfbHbLd7XYH9/l8PqWnp4fsj4+PV2pqavCYz6qqqpLL5QreMjMzIzk2AAAwzJB4F09lZaX8fn/w1traavdIAABgAIV9DcqleDweSVJ7e7tGjhwZ3N7e3q5JkyYFj+no6Ai537lz53Ty5Mng/T/L4XDI4XBEclREkdHLdl32mA/XFA7CJACASInoGZTs7Gx5PB7V1dUFtwUCATU2Nsrr9UqSvF6vOjs71dTUFDzm9ddfV19fn/Ly8iI5DgAAGKLCPoNy+vRpffDBB8GvT5w4ocOHDys1NVVZWVkqLy/XU089pTFjxig7O1srVqxQRkaG5syZI0kaN26c7rrrLj388MOqrq5WT0+PysrKNG/ePN7BAwAAJPUjUN5++2197WtfC35dUVEhSSopKdHWrVv1T//0Tzpz5oweeeQRdXZ2aubMmdqzZ4+GDRsWvM+LL76osrIy3XnnnYqNjVVRUZE2bNgQgeUAAIBoEGNZlmX3EOEKBAJyuVzy+/1yOp12j/O5ruTaCAwOrkEBAPuF8/N7SLyLBwAAXFsIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYJ97uAYDBMHrZrsse8+GawkGYBABwJTiDAgAAjEOgAAAA4/ASTz9dyUsGAACgfziDAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDj8Lh7AUFfy+54+XFM4CJMAwODjDAoAADAOgQIAAIzDSzzA/+IlFQAwB2dQAACAcTiDchFX8i9pXJs4ywIAg4MzKAAAwDgECgAAMA4v8QA24GVEALg0zqAAAADjcAYFiHJc2AtgKCJQgAjj5RsAuHq8xAMAAIzDGRRgCBvMszW8VARgMBEoACKGiAEQKbYGysaNG/XMM8/I5/Np4sSJevbZZzVt2jQ7RwKuSVw3A8A0tl2D8vOf/1wVFRVatWqVDh06pIkTJ6qgoEAdHR12jQQAAAwRY1mWZcc3zsvL09SpU/WTn/xEktTX16fMzEwtWrRIy5Ytu+R9A4GAXC6X/H6/nE5nxGfjX5OAvXgZCIhO4fz8tuUlnu7ubjU1NamysjK4LTY2Vvn5+WpoaLjg+K6uLnV1dQW/9vv9kv660IHQ1/XpgDwugCtzJX+3b12197LHvPN4QSTGARAh5/9uX8m5EVsC5c9//rN6e3vldrtDtrvdbr333nsXHF9VVaXHH3/8gu2ZmZkDNiMA+7jWm/U4ACLr1KlTcrlclzxmSLyLp7KyUhUVFcGv+/r6dPLkSaWlpSkmJqZfjxkIBJSZmanW1tYBeZnIbtG+Pin61xjt65Oif43Rvj4p+tcY7euTBneNlmXp1KlTysjIuOyxtgTKDTfcoLi4OLW3t4dsb29vl8fjueB4h8Mhh8MRsi0lJSUiszidzqj9n06K/vVJ0b/GaF+fFP1rjPb1SdG/xmhfnzR4a7zcmZPzbHkXT2JioqZMmaK6urrgtr6+PtXV1cnr9doxEgAAMIhtL/FUVFSopKREubm5mjZtmtavX68zZ87o29/+tl0jAQAAQ9gWKPfff78++eQTrVy5Uj6fT5MmTdKePXsuuHB2oDgcDq1ateqCl46iRbSvT4r+NUb7+qToX2O0r0+K/jVG+/okc9do2+egAAAAfB5+mzEAADAOgQIAAIxDoAAAAOMQKAAAwDjXZKBs3LhRo0eP1rBhw5SXl6cDBw7YPVLEVFVVaerUqRoxYoTS09M1Z84cNTc32z3WgFmzZo1iYmJUXl5u9ygR9cc//lEPPvig0tLSlJSUpPHjx+vtt9+2e6yI6e3t1YoVK5Sdna2kpCR98Ytf1JNPPnlFv5/DRG+99ZbuvvtuZWRkKCYmRq+88krIfsuytHLlSo0cOVJJSUnKz8/X8ePH7Rm2ny61xp6eHi1dulTjx49XcnKyMjIy9NBDD6mtrc2+gcN0uT/D/++73/2uYmJitH79+kGbLxKuZI3vvvuuvvnNb8rlcik5OVlTp05VS0vL4A+razBQfv7zn6uiokKrVq3SoUOHNHHiRBUUFKijo8Pu0SKivr5epaWl2r9/v2pra9XT06NZs2bpzJkzdo8WcQcPHtRPf/pTTZgwwe5RIuovf/mLZsyYoYSEBO3evVvHjh3TD3/4Q11//fV2jxYxTz/9tDZv3qyf/OQnevfdd/X0009r7dq1evbZZ+0erV/OnDmjiRMnauPGjRfdv3btWm3YsEHV1dVqbGxUcnKyCgoKdPbs2UGetP8utcZPP/1Uhw4d0ooVK3To0CH9x3/8h5qbm/XNb37Thkn753J/huft2LFD+/fvv6KPajfN5db4+9//XjNnztTYsWP15ptv6ne/+51WrFihYcOGDfKk/8u6xkybNs0qLS0Nft3b22tlZGRYVVVVNk41cDo6OixJVn19vd2jRNSpU6esMWPGWLW1tdZXv/pV67HHHrN7pIhZunSpNXPmTLvHGFCFhYXWd77znZBtc+fOtYqLi22aKHIkWTt27Ah+3dfXZ3k8HuuZZ54Jbuvs7LQcDof17//+7zZMePU+u8aLOXDggCXJ+uijjwZnqAj6vPV9/PHH1t/8zd9Y77zzjjVq1Chr3bp1gz5bpFxsjffff7/14IMP2jPQRVxTZ1C6u7vV1NSk/Pz84LbY2Fjl5+eroaHBxskGjt/vlySlpqbaPElklZaWqrCwMOTPMlr86le/Um5urr71rW8pPT1dkydP1nPPPWf3WBF1++23q66uTu+//74k6b/+67/0m9/8RrNnz7Z5ssg7ceKEfD5fyP+rLpdLeXl5Ufu8I/31uScmJiZivzfNbn19fZo/f76WLFmiW265xe5xIq6vr0+7du3SzTffrIKCAqWnpysvL++SL3UNtGsqUP785z+rt7f3gk+rdbvd8vl8Nk01cPr6+lReXq4ZM2bo1ltvtXuciHnppZd06NAhVVVV2T3KgPjDH/6gzZs3a8yYMdq7d68WLlyoRx99VNu2bbN7tIhZtmyZ5s2bp7FjxyohIUGTJ09WeXm5iouL7R4t4s4/t1wrzzuSdPbsWS1dulQPPPBA1PyCvaefflrx8fF69NFH7R5lQHR0dOj06dNas2aN7rrrLr366qv6h3/4B82dO1f19fW2zGTbR91j4JWWluqdd97Rb37zG7tHiZjW1lY99thjqq2tte910QHW19en3NxcrV69WpI0efJkvfPOO6qurlZJSYnN00XGL37xC7344ovavn27brnlFh0+fFjl5eXKyMiImjVeq3p6enTffffJsixt3rzZ7nEioqmpST/+8Y916NAhxcTE2D3OgOjr65Mk3XPPPVq8eLEkadKkSdq3b5+qq6v11a9+ddBnuqbOoNxwww2Ki4tTe3t7yPb29nZ5PB6bphoYZWVlqqmp0RtvvKGbbrrJ7nEipqmpSR0dHbrtttsUHx+v+Ph41dfXa8OGDYqPj1dvb6/dI161kSNHKicnJ2TbuHHjbLuSfiAsWbIkeBZl/Pjxmj9/vhYvXhyVZ8XOP7dcC8875+Pko48+Um1tbdScPfn1r3+tjo4OZWVlBZ93PvroI33ve9/T6NGj7R4vIm644QbFx8cb9dxzTQVKYmKipkyZorq6uuC2vr4+1dXVyev12jhZ5FiWpbKyMu3YsUOvv/66srOz7R4pou68804dOXJEhw8fDt5yc3NVXFysw4cPKy4uzu4Rr9qMGTMueGv4+++/r1GjRtk0UeR9+umnio0NffqJi4sL/isummRnZ8vj8YQ87wQCATU2NkbN8470f3Fy/Phxvfbaa0pLS7N7pIiZP3++fve734U872RkZGjJkiXau3ev3eNFRGJioqZOnWrUc8819xJPRUWFSkpKlJubq2nTpmn9+vU6c+aMvv3tb9s9WkSUlpZq+/bt+uUvf6kRI0YEX+N2uVxKSkqyebqrN2LEiAuup0lOTlZaWlrUXGezePFi3X777Vq9erXuu+8+HThwQFu2bNGWLVvsHi1i7r77bv3Lv/yLsrKydMstt+g///M/9aMf/Ujf+c537B6tX06fPq0PPvgg+PWJEyd0+PBhpaamKisrS+Xl5Xrqqac0ZswYZWdna8WKFcrIyNCcOXPsGzpMl1rjyJEjde+99+rQoUOqqalRb29v8LknNTVViYmJdo19xS73Z/jZ4EpISJDH49GXv/zlwR613y63xiVLluj+++/XV77yFX3ta1/Tnj17tHPnTr355pv2DGz324js8Oyzz1pZWVlWYmKiNW3aNGv//v12jxQxki56e+GFF+webcBE29uMLcuydu7cad16662Ww+Gwxo4da23ZssXukSIqEAhYjz32mJWVlWUNGzbM+sIXvmD98z//s9XV1WX3aP3yxhtvXPTvXUlJiWVZf32r8YoVKyy32205HA7rzjvvtJqbm+0dOkyXWuOJEyc+97nnjTfesHv0K3K5P8PPGopvM76SNT7//PPWl770JWvYsGHWxIkTrVdeecW2eWMsa4h+dCMAAIha19Q1KAAAYGggUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABjnfwB+vZHlX4UmkwAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.hist(pretrain_duration_list, bins = 50)\n",
        "\n",
        "print('the number of cycles: {}'.format(len(pretrain_duration_list)))\n",
        "print('longest cycle: {}'.format(max(pretrain_duration_list)))\n",
        "print('shortest cycle: {}'.format(min(pretrain_duration_list)))\n",
        "\n",
        "threshold = 4\n",
        "print('Fraction of samples less than {} seconds:{}'.format(threshold, np.sum(pretrain_duration_list < threshold)/len(pretrain_duration_list)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96EZ9ii1gG3x"
      },
      "source": [
        "(2) Fine-tuning 데이터"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "executionInfo": {
          "elapsed": 229,
          "status": "ok",
          "timestamp": 1745181237976,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "vsvD7_51gOmb",
        "outputId": "cf4f68d9-6c7a-478b-c979-d05a0fd511b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the number of cycles: 874\n",
            "longest cycle: 11.204999999999998\n",
            "shortest cycle: 0.2569999999999979\n",
            "Fraction of samples less than 4 seconds:0.7723112128146453\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAGdCAYAAACyzRGfAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAGdVJREFUeJzt3X1slfXd+PFPodB22FbB0NII0hkSVFARBAGzLdqMGDQSic4EF4ZmLltRHhIVNsH4WGC3SEAEMY5pIj79gY+RhVSHMfIkiJHowEUdjaRly6RVDIXQc/+x35pfJ/ekePgeLny9kiuh17nOdT45AfrO91znnKJcLpcLAIBEehR6AADg+0V8AABJiQ8AICnxAQAkJT4AgKTEBwCQlPgAAJISHwBAUsWFHuA/dXR0xN69e6O8vDyKiooKPQ4AcAxyuVx8+eWXUVNTEz16/Pe1jZMuPvbu3RsDBw4s9BgAwHFoamqKs846678ec9LFR3l5eUT8a/iKiooCTwMAHIu2trYYOHBg5+/x/+aki49/v9RSUVEhPgAgY47lkgkXnAIASYkPACAp8QEAJCU+AICkxAcAkJT4AACSEh8AQFLiAwBISnwAAEmJDwAgKfEBACQlPgCApMQHAJCU+AAAkiou9ABkz+A5r33rMZ8tmJhgEgCyyMoHAJCU+AAAkhIfAEBS4gMASEp8AABJiQ8AICnxAQAkJT4AgKTEBwCQlPgAAJISHwBAUuIDAEhKfAAASYkPACAp8QEAJCU+AICkxAcAkJT4AACSEh8AQFLiAwBISnwAAEmJDwAgKfEBACQlPgCApMQHAJCU+AAAkhIfAEBS4gMASEp8AABJFRd6AE4ug+e8VugRADjFWfkAAJISHwBAUuIDAEhKfAAASYkPACAp8QEAJCU+AICkxAcAkJT4AACSEh8AQFLiAwBISnwAAEmJDwAgKfEBACQlPgCApMQHAJCU+AAAkhIfAEBS3YqPI0eOxLx586K2tjbKysrinHPOifvuuy9yuVznMblcLubPnx8DBgyIsrKyqKuri48//jjvgwMA2dSt+Fi4cGGsWLEiHnnkkfjoo49i4cKFsWjRoli2bFnnMYsWLYqlS5fGypUrY/PmzdGnT5+YMGFCHDx4MO/DAwDZU9ydg99555245pprYuLEiRERMXjw4HjmmWdiy5YtEfGvVY8lS5bEXXfdFddcc01ERDz11FNRVVUVL774Ytxwww15Hh8AyJpurXyMGzcuGhsbY/fu3RER8f7778fbb78dV155ZUREfPrpp9Hc3Bx1dXWd96msrIwxY8bExo0bj3rO9vb2aGtr67IBAKeubq18zJkzJ9ra2mLo0KHRs2fPOHLkSDzwwAMxZcqUiIhobm6OiIiqqqou96uqquq87T81NDTEPffcczyzAwAZ1K2Vj+effz6efvrpWLNmTWzfvj2efPLJ+J//+Z948sknj3uAuXPnRmtra+fW1NR03OcCAE5+3Vr5uP3222POnDmd124MHz48/va3v0VDQ0NMnTo1qqurIyKipaUlBgwY0Hm/lpaWuOiii456zpKSkigpKTnO8QGArOnWysfXX38dPXp0vUvPnj2jo6MjIiJqa2ujuro6GhsbO29va2uLzZs3x9ixY/MwLgCQdd1a+bj66qvjgQceiEGDBsX5558f7733XixevDhuuummiIgoKiqKmTNnxv333x9DhgyJ2tramDdvXtTU1MSkSZNOxPwAQMZ0Kz6WLVsW8+bNi9/85jexb9++qKmpiV/96lcxf/78zmPuuOOOOHDgQNxyyy2xf//+uOyyy2LdunVRWlqa9+EBgOwpyv3/H096Emhra4vKyspobW2NioqKQo/zvTN4zmt5Oc9nCybm5TwAZEN3fn/7bhcAICnxAQAkJT4AgKTEBwCQlPgAAJISHwBAUuIDAEhKfAAASYkPACAp8QEAJCU+AICkxAcAkJT4AACSEh8AQFLiAwBISnwAAEmJDwAgKfEBACQlPgCApMQHAJCU+AAAkhIfAEBS4gMASEp8AABJiQ8AICnxAQAkJT4AgKTEBwCQlPgAAJISHwBAUuIDAEhKfAAASYkPACAp8QEAJCU+AICkxAcAkJT4AACSEh8AQFLiAwBISnwAAEmJDwAgKfEBACQlPgCApMQHAJCU+AAAkiou9ACcmgbPee1bj/lswcQEkwBwsrHyAQAkJT4AgKTEBwCQlGs+IM9c7wLw31n5AACSEh8AQFLiAwBISnwAAEmJDwAgKfEBACQlPgCApMQHAJCU+AAAkhIfAEBS4gMASEp8AABJiQ8AIKlux8fnn38eN954Y/Tr1y/Kyspi+PDh8e6773bensvlYv78+TFgwIAoKyuLurq6+Pjjj/M6NACQXd2Kjy+++CLGjx8fvXr1itdffz0+/PDDeOihh+KMM87oPGbRokWxdOnSWLlyZWzevDn69OkTEyZMiIMHD+Z9eAAge4q7c/DChQtj4MCBsXr16s59tbW1nX/O5XKxZMmSuOuuu+Kaa66JiIinnnoqqqqq4sUXX4wbbrghT2MDAFnVrZWPl19+OUaNGhXXXXdd9O/fP0aMGBGPP/545+2ffvppNDc3R11dXee+ysrKGDNmTGzcuDF/UwMAmdWt+Pjkk09ixYoVMWTIkPjTn/4Uv/71r+O2226LJ598MiIimpubIyKiqqqqy/2qqqo6b/tP7e3t0dbW1mUDAE5d3XrZpaOjI0aNGhUPPvhgRESMGDEidu7cGStXroypU6ce1wANDQ1xzz33HNd9AYDs6dbKx4ABA+K8887rsu/cc8+NPXv2REREdXV1RES0tLR0OaalpaXztv80d+7caG1t7dyampq6MxIAkDHdio/x48fHrl27uuzbvXt3nH322RHxr4tPq6uro7GxsfP2tra22Lx5c4wdO/ao5ywpKYmKioouGwBw6urWyy6zZs2KcePGxYMPPhjXX399bNmyJVatWhWrVq2KiIiioqKYOXNm3H///TFkyJCora2NefPmRU1NTUyaNOlEzA8AZEy34uOSSy6JtWvXxty5c+Pee++N2traWLJkSUyZMqXzmDvuuCMOHDgQt9xyS+zfvz8uu+yyWLduXZSWluZ9eAAge7oVHxERV111VVx11VX/5+1FRUVx7733xr333vudBgMATk2+2wUASEp8AABJiQ8AICnxAQAkJT4AgKTEBwCQlPgAAJISHwBAUuIDAEhKfAAASYkPACAp8QEAJCU+AICkxAcAkJT4AACSEh8AQFLFhR4AUhg857VvPeazBRMTTAKAlQ8AICnxAQAkJT4AgKTEBwCQlPgAAJISHwBAUuIDAEjK53ycInyOBQBZYeUDAEhKfAAASYkPACAp8QEAJCU+AICkxAcAkJS32sL/4+3KAGlY+QAAkhIfAEBS4gMASEp8AABJiQ8AICnxAQAkJT4AgKTEBwCQlPgAAJISHwBAUuIDAEhKfAAASYkPACAp8QEAJCU+AICkxAcAkJT4AACSEh8AQFLFhR6A76/Bc1771mM+WzAxwSQApGTlAwBISnwAAEmJDwAgKfEBACQlPgCApMQHAJCU+AAAkhIfAEBS4gMASEp8AABJiQ8AICnxAQAkJT4AgKS+U3wsWLAgioqKYubMmZ37Dh48GPX19dGvX7847bTTYvLkydHS0vJd5wQAThHHHR9bt26Nxx57LC644IIu+2fNmhWvvPJKvPDCC7Fhw4bYu3dvXHvttd95UADg1HBc8fHVV1/FlClT4vHHH48zzjijc39ra2s88cQTsXjx4rj88stj5MiRsXr16njnnXdi06ZNeRsaAMiu44qP+vr6mDhxYtTV1XXZv23btjh8+HCX/UOHDo1BgwbFxo0bj3qu9vb2aGtr67IBAKeu4u7e4dlnn43t27fH1q1bv3Fbc3Nz9O7dO04//fQu+6uqqqK5ufmo52toaIh77rmnu2NAQQye81qhRwDIvG6tfDQ1NcWMGTPi6aefjtLS0rwMMHfu3Ghtbe3cmpqa8nJeAODk1K342LZtW+zbty8uvvjiKC4ujuLi4tiwYUMsXbo0iouLo6qqKg4dOhT79+/vcr+Wlpaorq4+6jlLSkqioqKiywYAnLq69bLLFVdcER988EGXfdOmTYuhQ4fGnXfeGQMHDoxevXpFY2NjTJ48OSIidu3aFXv27ImxY8fmb2oAILO6FR/l5eUxbNiwLvv69OkT/fr169x/8803x+zZs6Nv375RUVERt956a4wdOzYuvfTS/E0NAGRWty84/TYPP/xw9OjRIyZPnhzt7e0xYcKEePTRR/P9MABARn3n+Pjzn//c5efS0tJYvnx5LF++/LueGgA4BfluFwAgKfEBACQlPgCApMQHAJCU+AAAksr7W205efleEgBOBlY+AICkxAcAkJT4AACSEh8AQFLiAwBISnwAAEl5qy0UwLG87fmzBRMTTAKQnpUPACAp8QEAJCU+AICkxAcAkJT4AACSEh8AQFLiAwBISnwAAEmJDwAgKfEBACQlPgCApMQHAJCU+AAAkhIfAEBS4gMASEp8AABJiQ8AICnxAQAkVVzoAYDjN3jOa996zGcLJiaYBODYWfkAAJISHwBAUuIDAEjKNR/HyWvtAHB8rHwAAEmJDwAgKS+7wEnqWF7aA8giKx8AQFLiAwBISnwAAEmJDwAgKfEBACQlPgCApMQHAJCU+AAAkhIfAEBS4gMASEp8AABJ+W4XTmq+3wTg1GPlAwBISnwAAEmJDwAgKfEBACQlPgCApMQHAJCU+AAAkhIfAEBS4gMASEp8AABJ+Xj1DPAR4wCcSqx8AABJiQ8AIKluxUdDQ0NccsklUV5eHv37949JkybFrl27uhxz8ODBqK+vj379+sVpp50WkydPjpaWlrwODQBkV7fiY8OGDVFfXx+bNm2K9evXx+HDh+OnP/1pHDhwoPOYWbNmxSuvvBIvvPBCbNiwIfbu3RvXXntt3gcHALKpWxecrlu3rsvPf/zjH6N///6xbdu2+NGPfhStra3xxBNPxJo1a+Lyyy+PiIjVq1fHueeeG5s2bYpLL700f5MDAJn0na75aG1tjYiIvn37RkTEtm3b4vDhw1FXV9d5zNChQ2PQoEGxcePGo56jvb092traumwAwKnruOOjo6MjZs6cGePHj49hw4ZFRERzc3P07t07Tj/99C7HVlVVRXNz81HP09DQEJWVlZ3bwIEDj3ckACADjjs+6uvrY+fOnfHss89+pwHmzp0bra2tnVtTU9N3Oh8AcHI7rg8Zmz59erz66qvx1ltvxVlnndW5v7q6Og4dOhT79+/vsvrR0tIS1dXVRz1XSUlJlJSUHM8YAEAGdWvlI5fLxfTp02Pt2rXxxhtvRG1tbZfbR44cGb169YrGxsbOfbt27Yo9e/bE2LFj8zMxAJBp3Vr5qK+vjzVr1sRLL70U5eXlnddxVFZWRllZWVRWVsbNN98cs2fPjr59+0ZFRUXceuutMXbsWO90AQAiopvxsWLFioiI+MlPftJl/+rVq+MXv/hFREQ8/PDD0aNHj5g8eXK0t7fHhAkT4tFHH83LsABA9nUrPnK53LceU1paGsuXL4/ly5cf91AAwKnLd7sAAEmJDwAgKfEBACQlPgCApMQHAJCU+AAAkhIfAEBS4gMASEp8AABJiQ8AICnxAQAkJT4AgKS69cVydM/gOa8VegQAOOlY+QAAkhIfAEBS4gMASEp8AABJiQ8AICnxAQAkJT4AgKR8zgdwTJ9J89mCiQkmAb4PrHwAAEmJDwAgKS+7HIWPRQeAE8fKBwCQlPgAAJISHwBAUuIDAEhKfAAASYkPACAp8QEAJCU+AICkxAcAkJT4AACSEh8AQFK+2wVOcb6rCDjZWPkAAJISHwBAUuIDAEhKfAAASYkPACAp8QEAJOWttsAxOZa37H62YGKCSYCss/IBACQlPgCApMQHAJDU9+6aDx81DQCFZeUDAEhKfAAASYkPACAp8QEAJCU+AICkxAcAkJT4AACSEh8AQFLiAwBISnwAAEl97z5eHThx8vX1BZ8tmJiX8wAnJysfAEBS4gMASEp8AABJueYDyKRjub7EtSNwcrLyAQAkJT4AgKROWHwsX748Bg8eHKWlpTFmzJjYsmXLiXooACBDTsg1H88991zMnj07Vq5cGWPGjIklS5bEhAkTYteuXdG/f/8T8ZAA35Cv60JOxs8vcc1LGll8nrMw8wlZ+Vi8eHH88pe/jGnTpsV5550XK1eujB/84Afxhz/84UQ8HACQIXlf+Th06FBs27Yt5s6d27mvR48eUVdXFxs3bvzG8e3t7dHe3t75c2tra0REtLW15Xu0iIjoaP/6hJwXyJ9j+fefr3/LJ9tjHatjmelE/T/6fZLF57lQM//7nLlc7tsPzuXZ559/nouI3DvvvNNl/+23354bPXr0N46/++67cxFhs9lsNpvtFNiampq+tRUK/jkfc+fOjdmzZ3f+3NHREf/85z+jX79+UVRU9K33b2tri4EDB0ZTU1NUVFScyFFPeZ7L/PA85ofnMX88l/nhefzvcrlcfPnll1FTU/Otx+Y9Ps4888zo2bNntLS0dNnf0tIS1dXV3zi+pKQkSkpKuuw7/fTTu/24FRUV/jLkiecyPzyP+eF5zB/PZX54Hv9vlZWVx3Rc3i847d27d4wcOTIaGxs793V0dERjY2OMHTs23w8HAGTMCXnZZfbs2TF16tQYNWpUjB49OpYsWRIHDhyIadOmnYiHAwAy5ITEx89+9rP4+9//HvPnz4/m5ua46KKLYt26dVFVVZX3xyopKYm77777Gy/d0H2ey/zwPOaH5zF/PJf54XnMn6Jc7ljeEwMAkB++2wUASEp8AABJiQ8AICnxAQAklfn4WL58eQwePDhKS0tjzJgxsWXLlkKPlCkNDQ1xySWXRHl5efTv3z8mTZoUu3btKvRYmbdgwYIoKiqKmTNnFnqUTPr888/jxhtvjH79+kVZWVkMHz483n333UKPlSlHjhyJefPmRW1tbZSVlcU555wT991337F978b33FtvvRVXX3111NTURFFRUbz44otdbs/lcjF//vwYMGBAlJWVRV1dXXz88ceFGTajMh0fzz33XMyePTvuvvvu2L59e1x44YUxYcKE2LdvX6FHy4wNGzZEfX19bNq0KdavXx+HDx+On/70p3HgwIFCj5ZZW7dujcceeywuuOCCQo+SSV988UWMHz8+evXqFa+//np8+OGH8dBDD8UZZ5xR6NEyZeHChbFixYp45JFH4qOPPoqFCxfGokWLYtmyZYUe7aR34MCBuPDCC2P58uVHvX3RokWxdOnSWLlyZWzevDn69OkTEyZMiIMHDyaeNMPy8WVyhTJ69OhcfX19589HjhzJ1dTU5BoaGgo4Vbbt27cvFxG5DRs2FHqUTPryyy9zQ4YMya1fvz734x//ODdjxoxCj5Q5d955Z+6yyy4r9BiZN3HixNxNN93UZd+1116bmzJlSoEmyqaIyK1du7bz546Ojlx1dXXu97//fee+/fv350pKSnLPPPNMASbMpsyufBw6dCi2bdsWdXV1nft69OgRdXV1sXHjxgJOlm2tra0REdG3b98CT5JN9fX1MXHixC5/L+mel19+OUaNGhXXXXdd9O/fP0aMGBGPP/54ocfKnHHjxkVjY2Ps3r07IiLef//9ePvtt+PKK68s8GTZ9umnn0Zzc3OXf+OVlZUxZswYv3u6oeDfanu8/vGPf8SRI0e+8ampVVVV8Ze//KVAU2VbR0dHzJw5M8aPHx/Dhg0r9DiZ8+yzz8b27dtj69athR4l0z755JNYsWJFzJ49O37729/G1q1b47bbbovevXvH1KlTCz1eZsyZMyfa2tpi6NCh0bNnzzhy5Eg88MADMWXKlEKPlmnNzc0REUf93fPv2/h2mY0P8q++vj527twZb7/9dqFHyZympqaYMWNGrF+/PkpLSws9TqZ1dHTEqFGj4sEHH4yIiBEjRsTOnTtj5cqV4qMbnn/++Xj66adjzZo1cf7558eOHTti5syZUVNT43mk4DL7ssuZZ54ZPXv2jJaWli77W1paorq6ukBTZdf06dPj1VdfjTfffDPOOuusQo+TOdu2bYt9+/bFxRdfHMXFxVFcXBwbNmyIpUuXRnFxcRw5cqTQI2bGgAED4rzzzuuy79xzz409e/YUaKJsuv3222POnDlxww03xPDhw+PnP/95zJo1KxoaGgo9Wqb9+/eL3z3fTWbjo3fv3jFy5MhobGzs3NfR0RGNjY0xduzYAk6WLblcLqZPnx5r166NN954I2praws9UiZdccUV8cEHH8SOHTs6t1GjRsWUKVNix44d0bNnz0KPmBnjx4//xtu9d+/eHWeffXaBJsqmr7/+Onr06PpffM+ePaOjo6NAE50aamtro7q6usvvnra2tti8ebPfPd2Q6ZddZs+eHVOnTo1Ro0bF6NGjY8mSJXHgwIGYNm1aoUfLjPr6+lizZk289NJLUV5e3vmaZWVlZZSVlRV4uuwoLy//xnUyffr0iX79+rl+pptmzZoV48aNiwcffDCuv/762LJlS6xatSpWrVpV6NEy5eqrr44HHnggBg0aFOeff3689957sXjx4rjpppsKPdpJ76uvvoq//vWvnT9/+umnsWPHjujbt28MGjQoZs6cGffff38MGTIkamtrY968eVFTUxOTJk0q3NBZU+i323xXy5Ytyw0aNCjXu3fv3OjRo3ObNm0q9EiZEhFH3VavXl3o0TLPW22P3yuvvJIbNmxYrqSkJDd06NDcqlWrCj1S5rS1teVmzJiRGzRoUK60tDT3wx/+MPe73/0u197eXujRTnpvvvnmUf9fnDp1ai6X+9fbbefNm5erqqrKlZSU5K644orcrl27Cjt0xhTlcj7uDgBIJ7PXfAAA2SQ+AICkxAcAkJT4AACSEh8AQFLiAwBISnwAAEmJDwAgKfEBACQlPgCApMQHAJCU+AAAkvpfqW2zSLh4uzcAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.hist(finetune_duration_list, bins = 50)\n",
        "\n",
        "print('the number of cycles: {}'.format(len(finetune_duration_list)))\n",
        "print('longest cycle: {}'.format(max(finetune_duration_list)))\n",
        "print('shortest cycle: {}'.format(min(finetune_duration_list)))\n",
        "\n",
        "threshold = 4\n",
        "print('Fraction of samples less than {} seconds:{}'.format(threshold, np.sum(finetune_duration_list < threshold)/len(finetune_duration_list)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNfZ9kELhXrh"
      },
      "source": [
        "(3) Test 데이터"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "executionInfo": {
          "elapsed": 248,
          "status": "ok",
          "timestamp": 1745181238213,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "lUEPMZ-_hOAt",
        "outputId": "245d9c6e-064f-45bb-a8bc-022997125fde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the number of cycles: 2756\n",
            "longest cycle: 9.217\n",
            "shortest cycle: 0.228\n",
            "Fraction of samples less than 4 seconds:0.9034833091436865\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.hist(test_duration_list, bins = 50)\n",
        "\n",
        "print('the number of cycles: {}'.format(len(test_duration_list)))\n",
        "print('longest cycle: {}'.format(max(test_duration_list)))\n",
        "print('shortest cycle: {}'.format(min(test_duration_list)))\n",
        "\n",
        "threshold = 4\n",
        "print('Fraction of samples less than {} seconds:{}'.format(threshold, np.sum(test_duration_list < threshold)/len(test_duration_list)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLEk_jop0cCV"
      },
      "source": [
        "#### 1-2. Label 분포 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "executionInfo": {
          "elapsed": 11,
          "status": "ok",
          "timestamp": 1745181238213,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "w8Y3CJAkiyw0"
      },
      "outputs": [],
      "source": [
        "def get_label_list(name, rec_annotations_dict):\n",
        "\n",
        "    no_label_list = []    # Normal\n",
        "    crack_list = []       # Crackle Only\n",
        "    wheeze_list = []      # Wheeze Only\n",
        "    both_sym_list = []    # Both\n",
        "\n",
        "    for f,d in rec_annotations_dict.items():\n",
        "        no_labels = len(d[(d['Crackles'] == 0) & (d['Wheezes'] == 0)].index)\n",
        "        n_crackles = len(d[(d['Crackles'] == 1) & (d['Wheezes'] == 0)].index)\n",
        "        n_wheezes = len(d[(d['Crackles'] == 0) & (d['Wheezes'] == 1)].index)\n",
        "        both_sym = len(d[(d['Crackles'] == 1) & (d['Wheezes'] == 1)].index)\n",
        "        no_label_list.append(no_labels)\n",
        "        crack_list.append(n_crackles)\n",
        "        wheeze_list.append(n_wheezes)\n",
        "        both_sym_list.append(both_sym)\n",
        "\n",
        "    df = pd.DataFrame(data = {'normal':no_label_list, 'crackles only':crack_list, 'wheezes only':wheeze_list, 'both':both_sym_list})\n",
        "    label_df = df.sum(axis=0).to_frame()\n",
        "    label_df.columns = [name]\n",
        "\n",
        "    return label_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "executionInfo": {
          "elapsed": 2495,
          "status": "ok",
          "timestamp": 1745181240699,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "kUQs1Qvo0gGf",
        "outputId": "9a99d208-1f42-4461-f7f6-00905183f409"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"Pretrain\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 640,\n        \"min\": 255,\n        \"max\": 1621,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1043,\n          255,\n          1621\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Finetune\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 151,\n        \"min\": 108,\n        \"max\": 442,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          172,\n          108,\n          442\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Train\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 780,\n        \"min\": 363,\n        \"max\": 2063,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1215,\n          363,\n          2063\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Test\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 628,\n        \"min\": 143,\n        \"max\": 1579,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          649,\n          143,\n          1579\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-987a089e-294b-4b3e-8246-48037cf6e8e2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pretrain</th>\n",
              "      <th>Finetune</th>\n",
              "      <th>Train</th>\n",
              "      <th>Test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>normal</th>\n",
              "      <td>1621</td>\n",
              "      <td>442</td>\n",
              "      <td>2063</td>\n",
              "      <td>1579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>crackles only</th>\n",
              "      <td>1043</td>\n",
              "      <td>172</td>\n",
              "      <td>1215</td>\n",
              "      <td>649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>wheezes only</th>\n",
              "      <td>349</td>\n",
              "      <td>152</td>\n",
              "      <td>501</td>\n",
              "      <td>385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>both</th>\n",
              "      <td>255</td>\n",
              "      <td>108</td>\n",
              "      <td>363</td>\n",
              "      <td>143</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-987a089e-294b-4b3e-8246-48037cf6e8e2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-987a089e-294b-4b3e-8246-48037cf6e8e2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-987a089e-294b-4b3e-8246-48037cf6e8e2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-dacc22d9-c2e3-452d-8e37-87a0ab8883f8\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dacc22d9-c2e3-452d-8e37-87a0ab8883f8')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-dacc22d9-c2e3-452d-8e37-87a0ab8883f8 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "               Pretrain  Finetune  Train  Test\n",
              "normal             1621       442   2063  1579\n",
              "crackles only      1043       172   1215   649\n",
              "wheezes only        349       152    501   385\n",
              "both                255       108    363   143"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pretrain_label_df = get_label_list('Pretrain', pretrain_rec_annotations_dict)\n",
        "finetune_label_df = get_label_list('Finetune', finetune_rec_annotations_dict)\n",
        "train_label_df = get_label_list('Train', train_rec_annotations_dict)\n",
        "test_label_df = get_label_list('Test', test_rec_annotations_dict)\n",
        "\n",
        "pd.concat([pretrain_label_df, finetune_label_df, train_label_df, test_label_df], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 23,
          "status": "ok",
          "timestamp": 1745181240700,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "qJFhD-kClZJ_",
        "outputId": "745e64d6-50c0-4a67-ecf1-bf5601787677"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "사전훈련 데이터 수: 428, 환자 수: 65\n",
            "파인튜닝 데이터 수: 111, 환자 수: 14\n",
            "\n",
            "Train 데이터 수   : 539, 환자 수: 79\n",
            "Test 데이터 수    : 381, 환자 수: 49\n"
          ]
        }
      ],
      "source": [
        "print(f\"사전훈련 데이터 수: {len(pretrain_df)}, 환자 수: {len(pretrain_patients)}\")\n",
        "print(f\"파인튜닝 데이터 수: {len(finetune_df)}, 환자 수: {len(finetune_patients)}\")\n",
        "print(f\"\\nTrain 데이터 수   : {len(train_df)}, 환자 수: {len(train_patient_counts)}\")\n",
        "print(f\"Test 데이터 수    : {len(test_df)}, 환자 수: {len(test_patient_counts)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbTj_j4SJeon"
      },
      "source": [
        "## 2. 데이터 전처리  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGTGA4qvwKB2"
      },
      "source": [
        "#### 2-1. 호흡 주기 분할"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "executionInfo": {
          "elapsed": 17,
          "status": "ok",
          "timestamp": 1745181240700,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "OUKiqUQ7I3gd"
      },
      "outputs": [],
      "source": [
        "def _slice_data_torchaudio(start, end, data, sample_rate):\n",
        "    \"\"\"\n",
        "    SCL paper..\n",
        "    sample_rate denotes how many sample points for one second\n",
        "    \"\"\"\n",
        "    max_ind = len(data)\n",
        "    start_ind = min(int(start * sample_rate), max_ind)\n",
        "    end_ind = min(int(end * sample_rate), max_ind)\n",
        "\n",
        "    return data[start_ind: end_ind]\n",
        "\n",
        "\n",
        "def _get_lungsound_label(crackle, wheeze):\n",
        "    if crackle == 0 and wheeze == 0:\n",
        "        return 0\n",
        "    elif crackle == 1 and wheeze == 0:\n",
        "        return 1\n",
        "    elif crackle == 0 and wheeze == 1:\n",
        "        return 2\n",
        "    elif crackle == 1 and wheeze == 1:\n",
        "        return 3\n",
        "\n",
        "\n",
        "def get_individual_cycles_torchaudio(recording_annotations, wav, sample_rate):\n",
        "    \"\"\"\n",
        "    SCL paper..\n",
        "    used to split each individual sound file into separate sound clips containing one respiratory cycle each\n",
        "    output: [(audio_chunk:np.array, label:int), (...)]\n",
        "    \"\"\"\n",
        "    sample_data = []\n",
        "    # fpath = os.path.join(data_folder, filename)\n",
        "\n",
        "    # sr = librosa.get_samplerate(fpath)\n",
        "    # data, _ = torchaudio.load(fpath)\n",
        "\n",
        "    for idx in recording_annotations.index:\n",
        "        row = recording_annotations.loc[idx]\n",
        "\n",
        "        start = row['Start'] # time (second)\n",
        "        end = row['End'] # time (second)\n",
        "        audio_chunk = _slice_data_torchaudio(start, end, wav, sample_rate)\n",
        "\n",
        "        crackles = row['Crackles']\n",
        "        wheezes = row['Wheezes']\n",
        "        sample_data.append((audio_chunk, _get_lungsound_label(crackles, wheezes)))\n",
        "\n",
        "    return sample_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XavJi3_eDWqW"
      },
      "source": [
        "#### 2-2. Log mel spectrogram 변환  \n",
        "- WaveInLMSOutDataset 클래스 정의  \n",
        "- zero-padding, random crop을 통한 전처리 방법 정의\n",
        "- n_frames = floor{(unit_sec*sample_rate - n_fft) / hop_length} + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "executionInfo": {
          "elapsed": 16,
          "status": "ok",
          "timestamp": 1745181240700,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "aS2KEL97DNsF"
      },
      "outputs": [],
      "source": [
        "class MelSpectrogramLibrosa:\n",
        "    \"\"\"Mel spectrogram using librosa.\"\"\"\n",
        "    def __init__(self, fs=16000, n_fft=1024, shift=160, n_mels=64, fmin=60, fmax=7800):\n",
        "        self.fs, self.n_fft, self.shift, self.n_mels, self.fmin, self.fmax = fs, n_fft, shift, n_mels, fmin, fmax\n",
        "        self.mfb = librosa.filters.mel(sr=fs, n_fft=n_fft, n_mels=n_mels, fmin=fmin, fmax=fmax)\n",
        "\n",
        "    def __call__(self, audio):\n",
        "        X = librosa.stft(np.array(audio), n_fft=self.n_fft, hop_length=self.shift)\n",
        "        return torch.tensor(np.matmul(self.mfb, np.abs(X)**2))      # [n_mels, n_frames] 반환\n",
        "\n",
        "\n",
        "class WaveInLMSOutDataset(Dataset):\n",
        "    \"\"\"Wave in, log-mel spectrogram out, dataset class.\n",
        "\n",
        "    Choosing librosa or torchaudio:\n",
        "        librosa: Stable but slower.\n",
        "        torchaudio: Faster but cannot reproduce the exact performance of pretrained weight,\n",
        "            which might be caused by the difference with librosa. Librosa was used in the pretraining.\n",
        "\n",
        "    Args:\n",
        "        cfg: Configuration settings.\n",
        "        mode: Purpose of constructing this dataset. ('train', 'finetune' and 'test')\n",
        "        audio_files: List of audio file pathnames.\n",
        "        tfms: Transforms (augmentations), callable.\n",
        "        use_labels: True if using labels (fine-tuning or evaluation).\n",
        "        use_librosa: True if using librosa for converting audio to log-mel spectrogram (LMS).\n",
        "\n",
        "    Returns:\n",
        "        LMS if use_labels=False.\n",
        "        (LMS, label) if use_labels=True.\n",
        "        shape of LMS: (1, n_mels, time)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cfg, mode, audio_files, tfms=None, use_labels=False, use_librosa=False):\n",
        "        # argment check\n",
        "        assert mode in ['train', 'finetune', 'test'], 'Argument \"mode\" must be one of: \"train\", \"finetune\" or \"test\".'\n",
        "        super().__init__()\n",
        "\n",
        "        self.cfg = cfg\n",
        "        self.mode = mode\n",
        "        self.files = audio_files\n",
        "        self.tfms = tfms\n",
        "        self.use_labels = use_labels\n",
        "        self.audio_cycle_wavs = []          # 입력받은 audio를 호흡 cycle 단위로 분해하여 저장\n",
        "        self.audio_cycle_labels = []        # label을 입력받은 경우, 각 cycle에 부여된 label을 저장\n",
        "        self.unit_length = int(cfg.unit_sec * cfg.sample_rate)\n",
        "        self.to_melspecgram = MelSpectrogramLibrosa(\n",
        "            fs=cfg.sample_rate,\n",
        "            n_fft=cfg.n_fft,\n",
        "            shift=cfg.hop_length,\n",
        "            n_mels=cfg.n_mels,\n",
        "            fmin=cfg.f_min,\n",
        "            fmax=cfg.f_max,\n",
        "        ) if use_librosa else T.MelSpectrogram(\n",
        "            sample_rate=cfg.sample_rate,\n",
        "            n_fft=cfg.n_fft,\n",
        "            win_length=cfg.win_length,\n",
        "            hop_length=cfg.hop_length,\n",
        "            n_mels=cfg.n_mels,\n",
        "            f_min=cfg.f_min,\n",
        "            f_max=cfg.f_max,\n",
        "            power=2,\n",
        "        )\n",
        "\n",
        "        # 데이터 로드\n",
        "        if self.mode == 'train':\n",
        "            rec_annotations_dict = pretrain_rec_annotations_dict\n",
        "        elif self.mode == 'finetune':\n",
        "            rec_annotations_dict = finetune_rec_annotations_dict\n",
        "        elif self.mode == 'test':\n",
        "            rec_annotations_dict = test_rec_annotations_dict\n",
        "\n",
        "        # 각 wav 파일을 처리하여 호흡 주기 단위로 저장\n",
        "        for file in self.files:\n",
        "            wav, sr = torchaudio.load(file)\n",
        "\n",
        "            # Resample if needed\n",
        "            if sr != self.cfg.sample_rate:\n",
        "                resampler = T.Resample(orig_freq=sr, new_freq=self.cfg.sample_rate)\n",
        "                wav = resampler(wav)\n",
        "\n",
        "            # Ensure single channel\n",
        "            assert wav.shape[0] == 1, f'Convert .wav files to single channel audio, {file} has {wav.shape[0]} channels.'\n",
        "            wav = wav[0]  # (1, length) -> (length,)\n",
        "\n",
        "            # 호흡 cycle 분할\n",
        "            annotation = rec_annotations_dict[file.stem]\n",
        "            cycles = get_individual_cycles_torchaudio(annotation, wav, self.cfg.sample_rate)\n",
        "\n",
        "            for cycle, label in cycles:\n",
        "                self.audio_cycle_wavs.append(cycle)\n",
        "                # label을 포함한 데이터셋을 구축하는 경우\n",
        "                if self.use_labels:\n",
        "                    # 각 cycle에 부여된 label을 저장\n",
        "                    self.audio_cycle_labels.append(label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_cycle_wavs)  # 전체 호흡 cycle 개수\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        cycle = self.audio_cycle_wavs[idx]\n",
        "\n",
        "        # zero padding: unit_length보다 짧은 경우, 양쪽에 0-padding 추가\n",
        "        length_adj = self.unit_length - len(cycle)\n",
        "        if length_adj > 0:\n",
        "            half_unit = self.unit_length // 2\n",
        "\n",
        "            if length_adj < half_unit:\n",
        "                # 길이 차이가 unit_length의 절반보다 작다면 zero padding 적용\n",
        "                half_adj = length_adj // 2\n",
        "                cycle = F.pad(cycle, (half_adj, length_adj - half_adj))\n",
        "            else:\n",
        "                # 길이 차이가 unit_length의 절반보다 크다면 충분히 cycle 반복하여 채우고 zero padding 적용\n",
        "                repeat_factor = (self.unit_length // len(cycle))        # 반복 횟수 결정\n",
        "                cycle = cycle.repeat(repeat_factor)[:self.unit_length]  # 필요한 길이만큼 자름\n",
        "                remaining_length = self.unit_length - len(cycle)        # 남은 길이 계산\n",
        "\n",
        "                # 남은 길이를 반으로 나눠서 앞뒤에 zero padding 적용\n",
        "                half_pad = remaining_length // 2\n",
        "                cycle = F.pad(cycle, (half_pad, remaining_length - half_pad))\n",
        "\n",
        "        # random crop: unit_length보다 긴 경우, 랜덤한 위치에서 unit_length만큼 자르기\n",
        "        length_adj = len(cycle) - self.unit_length\n",
        "        if length_adj > 0:\n",
        "            start = random.randint(0, length_adj // 4)  # 시작점을 cycle 길이의 앞쪽에서 랜덤 선택\n",
        "            end = start + self.unit_length  # 시작점으로부터 unit_length 길이만큼 크롭\n",
        "            cycle = cycle[start:end]\n",
        "\n",
        "        # Log mel spectrogram 변환 -> (1, n_mels, time)\n",
        "        lms = (self.to_melspecgram(cycle) + torch.finfo().eps).log().unsqueeze(0)  # (1, n_mels, time)\n",
        "\n",
        "        # AugmentationModule or PrecomputedNorm\n",
        "        if self.tfms is not None:\n",
        "            lms = self.tfms(lms)\n",
        "\n",
        "        # label이 있다면 반환\n",
        "        if self.use_labels:\n",
        "            return lms, self.audio_cycle_labels[idx]\n",
        "        return lms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GKbLHbIznaR"
      },
      "source": [
        "## 3. 모델 설계"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-9scNDMDdRx"
      },
      "source": [
        "#### 3-1. 모델 구조 정의  \n",
        "- 변경점: 레이어 구조, 활성화 함수  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "executionInfo": {
          "elapsed": 268,
          "status": "ok",
          "timestamp": 1745181240954,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "nlAwhi3zDeE-"
      },
      "outputs": [],
      "source": [
        "class NetworkCommonMixIn():\n",
        "    \"\"\"Common mixin for network definition.\"\"\"\n",
        "\n",
        "    def load_weight(self, weight_file, device, state_dict=None, key_check=True):\n",
        "        \"\"\"Utility to load a weight file to a device.\"\"\"\n",
        "\n",
        "        state_dict = state_dict or torch.load(weight_file, map_location=device)\n",
        "        if 'state_dict' in state_dict:\n",
        "            state_dict = state_dict['state_dict']\n",
        "        # Remove unneeded prefixes from the keys of parameters.\n",
        "        if key_check:\n",
        "            weights = {}\n",
        "            for k in state_dict:\n",
        "                m = re.search(r'(^fc\\.|\\.fc\\.|^features\\.|\\.features\\.)', k)\n",
        "                if m is None: continue\n",
        "                new_k = k[m.start():]\n",
        "                new_k = new_k[1:] if new_k[0] == '.' else new_k\n",
        "                weights[new_k] = state_dict[k]\n",
        "        else:\n",
        "            weights = state_dict\n",
        "        # Load weights and set model to eval().\n",
        "        self.load_state_dict(weights)\n",
        "        self.eval()\n",
        "        logging.info(f'Using audio embbeding network pretrained weight: {Path(weight_file).name}')\n",
        "        return self\n",
        "\n",
        "    def set_trainable(self, trainable=False):\n",
        "        for p in self.parameters():\n",
        "            p.requires_grad = trainable\n",
        "\n",
        "\n",
        "class AudioNTT2020Task6(nn.Module, NetworkCommonMixIn):\n",
        "    \"\"\"DCASE2020 Task6 NTT Solution Audio Embedding Network.\"\"\"\n",
        "\n",
        "    def __init__(self, n_mels, d):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, 3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(64, 64, 3, stride=2, padding=1),    # stride=2로 다운샘플링\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.GELU(),\n",
        "\n",
        "            nn.Conv2d(64, 128, 3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(128, 256, 3, stride=2, padding=1),  # stride=2로 다운샘플링\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.GELU(),\n",
        "\n",
        "            nn.Conv2d(256, 512, 3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(512, 512, 3, stride=2, padding=1),  # stride=2로 다운샘플링\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.GELU(),\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(512 * (n_mels // (2**3)), d),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(d, d),\n",
        "            nn.GELU(),\n",
        "        )\n",
        "        self.d = d\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)       # (batch, ch, mel, time)\n",
        "        x = x.permute(0, 3, 2, 1)  # (batch, time, mel, ch)\n",
        "        B, T, D, C = x.shape\n",
        "        x = x.reshape((B, T, C*D)) # (batch, time, mel*ch)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class AudioNTT2020(AudioNTT2020Task6):\n",
        "    \"\"\"BYOL-A General Purpose Representation Network.\n",
        "    This is an extension of the DCASE 2020 Task 6 NTT Solution Audio Embedding Network.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_mels=64, d=512):\n",
        "        super().__init__(n_mels=n_mels, d=d)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = super().forward(x)\n",
        "        (x1, _) = torch.max(x, dim=1)\n",
        "        x2 = torch.mean(x, dim=1)\n",
        "        x = x1 + x2\n",
        "        assert x.shape[1] == self.d and x.ndim == 2\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oluurwU9EyLf"
      },
      "source": [
        "#### 3-2. BYOL 훈련 알고리즘 정의"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2N7D8MTi72fs"
      },
      "source": [
        "Online Network, Target Network, EMA 등 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "executionInfo": {
          "elapsed": 31,
          "status": "ok",
          "timestamp": 1745181240955,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "ZIzapl36Ex49"
      },
      "outputs": [],
      "source": [
        "\"\"\"BYOL for Audio\n",
        "\n",
        "Kudos to Phil Wang, this implementation is based on https://github.com/lucidrains/byol-pytorch/\n",
        "\n",
        "This code is customized to enable:\n",
        "- Decoupling augmentations.\n",
        "- Feeding two augmented input batches independently.\n",
        "\"\"\"\n",
        "\n",
        "import copy\n",
        "from functools import wraps\n",
        "\n",
        "# helper functions\n",
        "\n",
        "def default(val, def_val):\n",
        "    return def_val if val is None else val\n",
        "\n",
        "def flatten(t):\n",
        "    return t.reshape(t.shape[0], -1)\n",
        "\n",
        "def singleton(cache_key):\n",
        "    def inner_fn(fn):\n",
        "        @wraps(fn)\n",
        "        def wrapper(self, *args, **kwargs):\n",
        "            instance = getattr(self, cache_key)\n",
        "            if instance is not None:\n",
        "                return instance\n",
        "\n",
        "            instance = fn(self, *args, **kwargs)\n",
        "            setattr(self, cache_key, instance)\n",
        "            return instance\n",
        "        return wrapper\n",
        "    return inner_fn\n",
        "\n",
        "def get_module_device(module):\n",
        "    return next(module.parameters()).device\n",
        "\n",
        "def set_requires_grad(model, val):\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad = val\n",
        "\n",
        "# loss fn\n",
        "\n",
        "def loss_fn(x, y):\n",
        "    x = F.normalize(x, dim=-1, p=2)\n",
        "    y = F.normalize(y, dim=-1, p=2)\n",
        "    return 2 - 2 * (x * y).sum(dim=-1)\n",
        "\n",
        "# exponential moving average\n",
        "\n",
        "class EMA():\n",
        "    def __init__(self, beta):\n",
        "        super().__init__()\n",
        "        self.beta = beta\n",
        "\n",
        "    def update_average(self, old, new):\n",
        "        if old is None:\n",
        "            return new\n",
        "        return old * self.beta + (1 - self.beta) * new\n",
        "\n",
        "def update_moving_average(ema_updater, ma_model, current_model):\n",
        "    for current_params, ma_params in zip(current_model.parameters(), ma_model.parameters()):\n",
        "        old_weight, up_weight = ma_params.data, current_params.data\n",
        "        ma_params.data = ema_updater.update_average(old_weight, up_weight)\n",
        "\n",
        "# MLP class for projector and predictor\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, dim, projection_size, hidden_size = 4096):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_size),\n",
        "            nn.BatchNorm1d(hidden_size),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(hidden_size, projection_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# a wrapper class for the base neural network\n",
        "# will manage the interception of the hidden layer output\n",
        "# and pipe it into the projecter and predictor nets\n",
        "\n",
        "class NetWrapper(nn.Module):\n",
        "    def __init__(self, net, projection_size, projection_hidden_size, layer = -2):\n",
        "        super().__init__()\n",
        "        self.net = net\n",
        "        self.layer = layer\n",
        "\n",
        "        self.projector = None\n",
        "        self.projection_size = projection_size\n",
        "        self.projection_hidden_size = projection_hidden_size\n",
        "\n",
        "        self.hidden = {}\n",
        "        self.hook_registered = False\n",
        "\n",
        "    def _find_layer(self):\n",
        "        if type(self.layer) == str:\n",
        "            modules = dict([*self.net.named_modules()])\n",
        "            return modules.get(self.layer, None)\n",
        "        elif type(self.layer) == int:\n",
        "            children = [*self.net.children()]\n",
        "            return children[self.layer]\n",
        "        return None\n",
        "\n",
        "    def _hook(self, _, input, output):\n",
        "        device = input[0].device\n",
        "        self.hidden[device] = flatten(output)\n",
        "\n",
        "    def _register_hook(self):\n",
        "        layer = self._find_layer()\n",
        "        assert layer is not None, f'hidden layer ({self.layer}) not found'\n",
        "        handle = layer.register_forward_hook(self._hook)\n",
        "        self.hook_registered = True\n",
        "\n",
        "    @singleton('projector')\n",
        "    def _get_projector(self, hidden):\n",
        "        _, dim = hidden.shape\n",
        "        projector = MLP(dim, self.projection_size, self.projection_hidden_size)\n",
        "        return projector.to(hidden)\n",
        "\n",
        "    def get_representation(self, x):\n",
        "        if self.layer == -1:\n",
        "            return self.net(x)\n",
        "\n",
        "        if not self.hook_registered:\n",
        "            self._register_hook()\n",
        "\n",
        "        self.hidden.clear()\n",
        "        _ = self.net(x)\n",
        "        hidden = self.hidden[x.device]\n",
        "        self.hidden.clear()\n",
        "\n",
        "        assert hidden is not None, f'hidden layer {self.layer} never emitted an output'\n",
        "        return hidden\n",
        "\n",
        "    def forward(self, x, return_projection = True):\n",
        "        representation = self.get_representation(x)\n",
        "\n",
        "        if not return_projection:\n",
        "            return representation\n",
        "\n",
        "        projector = self._get_projector(representation)\n",
        "        projection = projector(representation)\n",
        "        return projection, representation\n",
        "\n",
        "\n",
        "# main class\n",
        "\n",
        "class BYOL(nn.Module):\n",
        "    \"\"\"BYOL training module that is:\n",
        "    - Decoupled augmentations.\n",
        "    - Accepts two augmented inputs independently.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        net,\n",
        "        image_size,\n",
        "        hidden_layer=-1,\n",
        "        projection_size=256,\n",
        "        projection_hidden_size=4096,\n",
        "        moving_average_decay=0.99,\n",
        "        use_momentum=True,\n",
        "        channels=1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.net = net\n",
        "\n",
        "        self.online_encoder = NetWrapper(net, projection_size, projection_hidden_size, layer=hidden_layer)\n",
        "\n",
        "        self.use_momentum = use_momentum\n",
        "        self.target_encoder = None\n",
        "        self.target_ema_updater = EMA(moving_average_decay)\n",
        "\n",
        "        self.online_predictor = MLP(projection_size, projection_size, projection_hidden_size)\n",
        "\n",
        "        # get device of network and make wrapper same device\n",
        "        device = get_module_device(net)\n",
        "        self.to(device)\n",
        "\n",
        "        # send a mock image tensor to instantiate singleton parameters\n",
        "        with torch.no_grad():\n",
        "            self.forward(torch.randn(2, channels, image_size[0], image_size[1]),\n",
        "                         torch.randn(2, channels, image_size[0], image_size[1]))\n",
        "\n",
        "    @singleton('target_encoder')\n",
        "    def _get_target_encoder(self):\n",
        "        target_encoder = copy.deepcopy(self.online_encoder)\n",
        "        set_requires_grad(target_encoder, False)\n",
        "        return target_encoder\n",
        "\n",
        "    def reset_moving_average(self):\n",
        "        del self.target_encoder\n",
        "        self.target_encoder = None\n",
        "\n",
        "    def update_moving_average(self):\n",
        "        assert self.use_momentum, 'you do not need to update the moving average, since you have turned off momentum for the target encoder'\n",
        "        assert self.target_encoder is not None, 'target encoder has not been created yet'\n",
        "        update_moving_average(self.target_ema_updater, self.target_encoder, self.online_encoder)\n",
        "\n",
        "    def forward(self, image_one, image_two,\n",
        "        return_embedding = False,\n",
        "        return_projection = True\n",
        "    ):\n",
        "        if return_embedding:\n",
        "            return self.online_encoder(x, return_projection=return_projection)\n",
        "\n",
        "        online_proj_one, _ = self.online_encoder(image_one)\n",
        "        online_proj_two, _ = self.online_encoder(image_two)\n",
        "\n",
        "        online_pred_one = self.online_predictor(online_proj_one)\n",
        "        online_pred_two = self.online_predictor(online_proj_two)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            target_encoder = self._get_target_encoder() if self.use_momentum else self.online_encoder\n",
        "            target_proj_one, _ = target_encoder(image_one)\n",
        "            target_proj_two, _ = target_encoder(image_two)\n",
        "            target_proj_one.detach_()\n",
        "            target_proj_two.detach_()\n",
        "\n",
        "        loss_one = loss_fn(online_pred_one, target_proj_two.detach())\n",
        "        loss_two = loss_fn(online_pred_two, target_proj_one.detach())\n",
        "\n",
        "        loss = loss_one + loss_two\n",
        "        return loss.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeb3I6vKKBhf"
      },
      "source": [
        "#### 3-3. Augmentations 방법 정의  \n",
        "BYOL-A 논문 저자들이 제안하는 방법을 적용  \n",
        "1. RandomResizeCrop  \n",
        "2. Mixup  \n",
        "3. MixGaussianNoise (사용x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "executionInfo": {
          "elapsed": 30,
          "status": "ok",
          "timestamp": 1745181240955,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "FecGX_SWKD0l"
      },
      "outputs": [],
      "source": [
        "class RandomResizeCrop(nn.Module):\n",
        "    \"\"\"Random Resize Crop block.\n",
        "\n",
        "    Args:\n",
        "        virtual_crop_scale: Virtual crop area `(F ratio, T ratio)` in ratio to input size.\n",
        "        freq_scale: Random frequency range `(min, max)`.\n",
        "        time_scale: Random time frame range `(min, max)`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, virtual_crop_scale=(1.0, 1.5), freq_scale=(0.6, 1.5), time_scale=(0.6, 1.5)):\n",
        "        super().__init__()\n",
        "        self.virtual_crop_scale = virtual_crop_scale\n",
        "        self.freq_scale = freq_scale\n",
        "        self.time_scale = time_scale\n",
        "        self.interpolation = 'bicubic'\n",
        "        assert time_scale[1] >= 1.0 and freq_scale[1] >= 1.0\n",
        "\n",
        "    @staticmethod\n",
        "    def get_params(virtual_crop_size, in_size, time_scale, freq_scale):\n",
        "        canvas_h, canvas_w = virtual_crop_size\n",
        "        src_h, src_w = in_size\n",
        "        h = np.clip(int(np.random.uniform(*freq_scale) * src_h), 1, canvas_h)\n",
        "        w = np.clip(int(np.random.uniform(*time_scale) * src_w), 1, canvas_w)\n",
        "        i = random.randint(0, canvas_h - h) if canvas_h > h else 0\n",
        "        j = random.randint(0, canvas_w - w) if canvas_w > w else 0\n",
        "        return i, j, h, w\n",
        "\n",
        "    def forward(self, lms):\n",
        "        # make virtual_crop_arear empty space (virtual crop area) and copy the input log mel spectrogram to th the center\n",
        "        virtual_crop_size = [int(s * c) for s, c in zip(lms.shape[-2:], self.virtual_crop_scale)]\n",
        "        virtual_crop_area = (torch.zeros((lms.shape[0], virtual_crop_size[0], virtual_crop_size[1]))\n",
        "                            .to(torch.float).to(lms.device))\n",
        "        _, lh, lw = virtual_crop_area.shape\n",
        "        c, h, w = lms.shape\n",
        "        x, y = (lw - w) // 2, (lh - h) // 2\n",
        "        virtual_crop_area[:, y:y+h, x:x+w] = lms\n",
        "        # get random area\n",
        "        i, j, h, w = self.get_params(virtual_crop_area.shape[-2:], lms.shape[-2:], self.time_scale, self.freq_scale)\n",
        "        crop = virtual_crop_area[:, i:i+h, j:j+w]\n",
        "        # print(f'shapes {virtual_crop_area.shape} {crop.shape} -> {lms.shape}')\n",
        "        lms = F.interpolate(crop.unsqueeze(0), size=lms.shape[-2:],\n",
        "            mode=self.interpolation, align_corners=True).squeeze(0)\n",
        "        return lms.to(torch.float)\n",
        "\n",
        "    def __repr__(self):\n",
        "        format_string = self.__class__.__name__ + f'(virtual_crop_size={self.virtual_crop_scale}'\n",
        "        format_string += ', time_scale={0}'.format(tuple(round(s, 4) for s in self.time_scale))\n",
        "        format_string += ', freq_scale={0})'.format(tuple(round(r, 4) for r in self.freq_scale))\n",
        "        return format_string\n",
        "\n",
        "\n",
        "def log_mixup_exp(xa, xb, alpha):\n",
        "    xa = xa.exp()\n",
        "    xb = xb.exp()\n",
        "    x = alpha * xa + (1. - alpha) * xb\n",
        "    return torch.log(x + torch.finfo(x.dtype).eps)\n",
        "\n",
        "\n",
        "class MixupBYOLA(nn.Module):\n",
        "    \"\"\"Mixup for BYOL-A.\n",
        "\n",
        "    Args:\n",
        "        ratio: Alpha in the paper.\n",
        "        n_memory: Size of memory bank FIFO.\n",
        "        log_mixup_exp: Use log-mixup-exp to mix if this is True, or mix without notion of log-scale.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, ratio=0.4, n_memory=2048, log_mixup_exp=True):\n",
        "        super().__init__()\n",
        "        self.ratio = ratio\n",
        "        self.n = n_memory\n",
        "        self.log_mixup_exp = log_mixup_exp\n",
        "        self.memory_bank = []\n",
        "\n",
        "    def forward(self, x):\n",
        "        # mix random\n",
        "        alpha = self.ratio * np.random.random()\n",
        "        if self.memory_bank:\n",
        "            # get z as a mixing background sound\n",
        "            z = self.memory_bank[np.random.randint(len(self.memory_bank))]\n",
        "            # mix them\n",
        "            mixed = log_mixup_exp(x, z, 1. - alpha) if self.log_mixup_exp \\\n",
        "                    else alpha * z + (1. - alpha) * x\n",
        "        else:\n",
        "            mixed = x\n",
        "        # update memory bank\n",
        "        self.memory_bank = (self.memory_bank + [x])[-self.n:]\n",
        "\n",
        "        return mixed.to(torch.float)\n",
        "\n",
        "    def __repr__(self):\n",
        "        format_string = self.__class__.__name__ + f'(ratio={self.ratio},n={self.n}'\n",
        "        format_string += f',log_mixup_exp={self.log_mixup_exp})'\n",
        "        return format_string\n",
        "\n",
        "\n",
        "class MixGaussianNoise():\n",
        "    \"\"\"Gaussian Noise Mixer.\n",
        "    This interpolates with random sample, unlike Mixup.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, ratio=0.3):\n",
        "        self.ratio = ratio\n",
        "\n",
        "    def forward(self, lms):\n",
        "        x = lms.exp()\n",
        "\n",
        "        lambd = self.ratio * np.random.rand()\n",
        "        z = torch.normal(0, lambd, x.shape).exp()\n",
        "        mixed = (1 - lambd) * x + z + torch.finfo(x.dtype).eps\n",
        "\n",
        "        return mixed.log()\n",
        "\n",
        "    def __repr__(self):\n",
        "        format_string = self.__class__.__name__ + f'(ratio={self.ratio})'\n",
        "        return format_string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDPtUlTV2dRX"
      },
      "source": [
        "#### 3.4 평균, 분산 연산"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "executionInfo": {
          "elapsed": 29,
          "status": "ok",
          "timestamp": 1745181240955,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "EZaJWBie2dRX"
      },
      "outputs": [],
      "source": [
        "class RunningMean:\n",
        "    \"\"\"Running mean calculator for arbitrary axis configuration.\"\"\"\n",
        "\n",
        "    def __init__(self, axis):\n",
        "        self.n = 0\n",
        "        self.axis = axis\n",
        "\n",
        "    def put(self, x):\n",
        "        # https://math.stackexchange.com/questions/106700/incremental-averageing\n",
        "        self.n += 1\n",
        "        if self.n == 1:\n",
        "            self.mu = x.mean(self.axis, keepdims=True)\n",
        "        else:\n",
        "            self.mu += (x.mean(self.axis, keepdims=True) - self.mu) / self.n\n",
        "\n",
        "    def __call__(self):\n",
        "        return self.mu\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n\n",
        "\n",
        "\n",
        "class RunningVariance:\n",
        "    \"\"\"Calculate mean/variance of tensors online.\n",
        "    Thanks to https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, axis, mean):\n",
        "        self.update_mean(mean)\n",
        "        self.s2 = RunningMean(axis)\n",
        "\n",
        "    def update_mean(self, mean):\n",
        "        self.mean = mean\n",
        "\n",
        "    def put(self, x):\n",
        "        self.s2.put((x - self.mean) **2)\n",
        "\n",
        "    def __call__(self):\n",
        "        return self.s2()\n",
        "\n",
        "    def std(self):\n",
        "        # Calculate the std using PyTorch functions and convert to a PyTorch tensor and add small value for numerical stability\n",
        "        return torch.sqrt(self.s2() + torch.finfo(self.s2().dtype).eps)\n",
        "\n",
        "\n",
        "class RunningNorm(nn.Module):\n",
        "    \"\"\"Online Normalization using Running Mean/Std.\n",
        "\n",
        "    This module will only update the statistics up to the specified number of epochs.\n",
        "    After the `max_update_epochs`, this will normalize with the last updated statistics.\n",
        "\n",
        "    Args:\n",
        "        epoch_samples: Number of samples in one epoch\n",
        "        max_update_epochs: Number of epochs to allow update of running mean/variance.\n",
        "        axis: Axis setting used to calculate mean/variance.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, epoch_samples, max_update_epochs=20, axis=[1, 2]):\n",
        "        super().__init__()\n",
        "        self.max_update = epoch_samples * max_update_epochs\n",
        "        self.ema_mean = RunningMean(axis)\n",
        "        self.ema_var = RunningVariance(axis, 0)\n",
        "\n",
        "    def forward(self, image):\n",
        "        if len(self.ema_mean) < self.max_update:\n",
        "            self.ema_mean.put(image)\n",
        "            self.ema_var.update_mean(self.ema_mean())\n",
        "            self.ema_var.put(image)\n",
        "            self.mean = self.ema_mean()\n",
        "            self.std = torch.clamp(self.ema_var.std(), torch.finfo().eps, torch.finfo().max)\n",
        "        return ((image - self.mean) / self.std)\n",
        "\n",
        "    def __repr__(self):\n",
        "        format_string = self.__class__.__name__ + f'(max_update={self.max_update},axis={self.ema_mean.axis})'\n",
        "        return format_string\n",
        "\n",
        "\n",
        "class NormalizeBatch(nn.Module):\n",
        "    \"\"\"Normalization of Input Batch.\n",
        "\n",
        "    Note:\n",
        "        Unlike other blocks, use this with *batch inputs*.\n",
        "\n",
        "    Args:\n",
        "        axis: Axis setting used to calculate mean/variance.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, axis=[0, 2, 3]):\n",
        "        super().__init__()\n",
        "        self.axis = axis\n",
        "\n",
        "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
        "        _mean = X.mean(dim=self.axis, keepdims=True)\n",
        "        _std = torch.clamp(X.std(dim=self.axis, keepdims=True), torch.finfo().eps, torch.finfo().max)\n",
        "        return ((X - _mean) / _std)\n",
        "\n",
        "    def __repr__(self):\n",
        "        format_string = self.__class__.__name__ + f'(axis={self.axis})'\n",
        "        return format_string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Oen-Ycs2dRZ"
      },
      "source": [
        "#### 3.5 Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "executionInfo": {
          "elapsed": 28,
          "status": "ok",
          "timestamp": 1745181240955,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "_YLwahPs2dRZ"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def get_timestamp():\n",
        "    \"\"\"Outputs current time in KST like 2404070830\"\"\"\n",
        "    kst_time = datetime.now(ZoneInfo(\"Asia/Seoul\"))\n",
        "    return kst_time.strftime('%y%m%d%H%M')\n",
        "\n",
        "def load_yaml_config(path_to_config):\n",
        "    \"\"\"Loads yaml configuration settings as an EasyDict object.\"\"\"\n",
        "    path_to_config = Path(path_to_config)\n",
        "    assert path_to_config.is_file()\n",
        "    with open(path_to_config) as f:\n",
        "        yaml_contents = yaml.safe_load(f)\n",
        "    cfg = EasyDict(yaml_contents)\n",
        "    return cfg\n",
        "\n",
        "def get_logger(name):\n",
        "    logging.basicConfig(format='%(asctime)s %(levelname)s %(message)s',\n",
        "                        datefmt='%Y-%m-%d %H:%M', level=logging.DEBUG)\n",
        "    logger = logging.getLogger(name)\n",
        "    return logger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QZvrenoz4GU"
      },
      "source": [
        "## 4. Pretraining"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4HzJqqbFI9b"
      },
      "source": [
        "#### 4-1. 훈련 관련 파라미터 설정    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I60X4TmqeNmQ"
      },
      "source": [
        "1. 오디오 관련 설정\n",
        "- unit sec: 한 개의 오디오 클립(segment)의 길이 (초 단위).  \n",
        "- sample_rate: 샘플링 레이트\n",
        "- n_fft:\tFFT(Fast Fourier Transform) 윈도우 크기. 스펙트로그램 변환 시 한 번에 처리할 샘플 수.  \n",
        "- win_length: STFT(SHORT-TIME Fourier Transform)에서 한 프레임의 길이. n_fft와 같음.\n",
        "- hop_length: STFT에서 프레임 간 오버랩 없이 이동하는 샘플 수. (16kHz 기준 10ms 간격)\n",
        "- n_mels:\tMel-spectrogram의 Mel filterbank 개수.\n",
        "- f_min: Mel-spectrogram에서 최소 주파수 (Hz).\n",
        "- f_max: Mel-spectrogram에서 최대 주파수 (Hz).  \n",
        "\n",
        "2. 모델 관련 설정\n",
        "- feature_d\t오디오 특징(feature) 벡터의 차원 수.\n",
        "- proj_size\tProjection 레이어의 출력 차원.\n",
        "- proj_dim BYOL에서 사용되는 MLP projection head의 중간 차원 크기.\n",
        "- ema_decay\tEMA (Exponential Moving Average) 업데이트 계수. 타겟 네트워크 업데이트에 사용됨.  \n",
        "\n",
        "3. 훈련 관련 설정  \n",
        "- seed\t42\t랜덤 시드 값. 실험 재현성을 보장하기 위해 사용됨.\n",
        "- bs 배치 크기 (batch size). 한 번의 학습 스텝에서 처리할 샘플 개수.\n",
        "- lr 학습률 (learning rate). 모델을 학습할 때 가중치를 업데이트하는 크기.\n",
        "- epochs 전체 데이터셋을 학습하는 횟수 (에포크 수).\n",
        "- num_workers\t데이터 로딩 시 병렬 처리를 위한 워커(worker) 수.  \n",
        "\n",
        "4. 체크포인트 관련 설정  \n",
        "- shape Mel-spectrogram의 최종 출력 형태 (n_mels x time frames).\n",
        "- checkpoint_folder\tcheckpoints\t모델 가중치를 저장할 폴더 이름."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 27,
          "status": "ok",
          "timestamp": 1745181240955,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "_9crNbsl9a2q",
        "outputId": "86d6ba62-8b00-4218-8f42-61a7b52228e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "498\n"
          ]
        }
      ],
      "source": [
        "# num_frames를 계산하는 함수, shapes: [n_mels, num_frames]\n",
        "def compute_num_frames(unit_sec, sample_rate, win_length, hop_length):\n",
        "    signal_length = int(unit_sec * sample_rate)\n",
        "    num_frames = (signal_length - win_length) // hop_length + 1\n",
        "    return int(num_frames)\n",
        "\n",
        "print(compute_num_frames(5,16000,400,160))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "executionInfo": {
          "elapsed": 22,
          "status": "ok",
          "timestamp": 1745181240956,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "-1d1iKIQFKUI"
      },
      "outputs": [],
      "source": [
        "config_data = {\n",
        "    # 1. 오디오 관련 설정\n",
        "    \"unit_sec\": 5,\n",
        "    \"sample_rate\": 16000,    # FFT parameters.\n",
        "    \"n_fft\": 400,\n",
        "    \"win_length\": 400,\n",
        "    \"hop_length\": 160,\n",
        "    \"n_mels\": 64,\n",
        "    \"f_min\": 60,\n",
        "    \"f_max\": 4000,\n",
        "    \"shape\": [64, 498],    # Shape of log-mel spectrogram [F, T].\n",
        "\n",
        "    # 2. 모델 관련 설정\n",
        "    \"feature_d\": 2048,       # Dimensions of feature representations.\n",
        "    \"proj_size\": 256,        # BYOL parameters.\n",
        "    \"proj_dim\": 4096,\n",
        "    \"ema_decay\": 0.99,\n",
        "\n",
        "    # 3. 훈련 관련 설정\n",
        "    \"seed\": 42,\n",
        "    \"bs\": 128,\n",
        "    \"lr\": 0.0003,\n",
        "    \"epochs\": 200,\n",
        "    \"num_workers\": 8,\n",
        "    \"checkpoint_folder\": PRETRAINED_MODEL_PATH\n",
        "}\n",
        "\n",
        "# YAML 파일 저장\n",
        "yaml_filename = \"icbhi_config.yaml\"\n",
        "with open(yaml_filename, \"w\") as yaml_file:\n",
        "    yaml.dump(config_data, yaml_file, default_flow_style=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5m9KnO-xEMll"
      },
      "source": [
        "#### 4-2. 모델 훈련 코드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "executionInfo": {
          "elapsed": 22,
          "status": "ok",
          "timestamp": 1745181240956,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "saC4MtAKENJd"
      },
      "outputs": [],
      "source": [
        "# 데이터 전처리 및 증강 모듈 정의\n",
        "class AugmentationModule:\n",
        "    \"\"\"BYOL-A augmentation module example, the same parameter with the paper.\"\"\"\n",
        "    def __init__(self, size, epoch_samples, log_mixup_exp=True, mixup_ratio=0.4):\n",
        "        self.train_transform = torch.nn.Sequential(\n",
        "            # 입력 데이터를 혼합하여 더 강력한 표현 학습을 가능하게 함\n",
        "            MixupBYOLA(ratio=mixup_ratio, log_mixup_exp=log_mixup_exp),\n",
        "            # 랜덤한 크기로 시간 및 주파수 영역을 크롭\n",
        "            RandomResizeCrop(virtual_crop_scale=(1.0, 1.5), freq_scale=(0.6, 1.5), time_scale=(0.6, 1.5)),\n",
        "        )\n",
        "        # 데이터 정규화\n",
        "        self.pre_norm = RunningNorm(epoch_samples=epoch_samples)\n",
        "        print('Augmentations:', self.train_transform)\n",
        "\n",
        "    # 이 모듈은 WaveInLMSOutDataset 클래스 안에 들어가고, lms를 입력받음\n",
        "    def __call__(self, x):\n",
        "        # 데이터 정규화\n",
        "        x = self.pre_norm(x)\n",
        "        # 같은 데이터를 두 번 변형하여 두 개의 증강 버전을 생성 → BYOL 모델에 필요한 두 개의 입력 생성\n",
        "        return self.train_transform(x), self.train_transform(x)\n",
        "\n",
        "\n",
        "class BYOLALearner(pl.LightningModule):\n",
        "    \"\"\"BYOL-A learner. Shows batch statistics for each epochs.\"\"\"\n",
        "    def __init__(self, model, lr, shape, max_epochs=200, **kwargs):\n",
        "        super().__init__()                  # 부모 클래스의 속성 및 메소드 가져오기\n",
        "        self.learner = BYOL(model, image_size=shape, **kwargs)  # BYOL 모델 학습\n",
        "        self.lr = lr                        # 학습률\n",
        "        self.max_epochs = max_epochs        # 최대 epoch\n",
        "        self.post_norm = NormalizeBatch()   # 배치 정규화\n",
        "\n",
        "    def forward(self, images1, images2):\n",
        "        return self.learner(images1, images2)\n",
        "\n",
        "    def training_step(self, paired_inputs, batch_idx):\n",
        "        def to_np(A): return [a.cpu().numpy() for a in A]\n",
        "        bs = paired_inputs[0].shape[0]      # 배치 크기\n",
        "\n",
        "        # 배치 크기 B, 채널 1, 주파수 축 F, 시간 축 T\n",
        "        # [(B,1,F,T), (B,1,F,T)] -> (2*B,1,F,T)\n",
        "\n",
        "        paired_inputs = torch.cat(paired_inputs)  # 배치 차원을 기준으로 두 개의 입력 텐서를 하나로 합침 (2 * batch_size)\n",
        "        mb, sb = to_np((paired_inputs.mean(), paired_inputs.std()))    # 입력 데이터의 평균 및 표준편차 계산 (정규화 전)\n",
        "\n",
        "        paired_inputs = self.post_norm(paired_inputs)                  # 배치 정규화 한번 더 수행\n",
        "        ma, sa = to_np((paired_inputs.mean(), paired_inputs.std()))    # 정규화 후 평균 및 표준편차 계산\n",
        "\n",
        "        loss = self.forward(paired_inputs[:bs], paired_inputs[bs:])    # 1번째 view와 2번째 view를 입력하여 BYOL 손실 계산\n",
        "\n",
        "        # 현재 epoch 번호\n",
        "        current_epoch = self.current_epoch\n",
        "\n",
        "        # 현재 weight_decay\n",
        "        current_weight_decay = self.optimizers().param_groups[0]['weight_decay']\n",
        "\n",
        "        # 로그 저장\n",
        "        for k, v in {'train_loss': loss, 'weight_decay': current_weight_decay, 'ma': ma, 'sa': sa, 'mb': mb, 'sb': sb}.items():\n",
        "            self.log(k, float(v), prog_bar=True, on_step=False, on_epoch=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr, weight_decay=0.4)  # 초기 weight decay=0.4\n",
        "        lr_scheduler = torch.optim.lr_scheduler\n",
        "\n",
        "        # Linear Warmup 적용 (20 epochs 동안 선형 증가)\n",
        "        warmup_epochs = 20\n",
        "        warmup_scheduler = lr_scheduler.LinearLR(\n",
        "            optimizer,\n",
        "            start_factor=3e-5 / self.lr,  # 시작 학습률 비율\n",
        "            end_factor=1.0,               # 최종 학습률 비율\n",
        "            total_iters=warmup_epochs     # warmup 지속 기간 (epoch 수)\n",
        "        )\n",
        "\n",
        "        # Cosine Annealing 적용 (20 epochs 이후)\n",
        "        cosine_scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=self.max_epochs - warmup_epochs, eta_min=1e-6)\n",
        "\n",
        "        # 학습률 Scheduler 정리\n",
        "        scheduler = lr_scheduler.SequentialLR(optimizer, schedulers=[warmup_scheduler, cosine_scheduler], milestones=[warmup_epochs])\n",
        "\n",
        "        return {\n",
        "            \"optimizer\": optimizer,\n",
        "            \"lr_scheduler\": scheduler\n",
        "        }\n",
        "\n",
        "    def on_before_zero_grad(self, _):\n",
        "        self.learner.update_moving_average()  # Moving Average로 가중치를 천천히 업데이트\n",
        "\n",
        "    def on_train_epoch_start(self):\n",
        "        # Weight Decay Cosine Scheduler 적용 (0.4 -> 0.04)\n",
        "        current_epoch = self.current_epoch\n",
        "        weight_decay = 0.04 + 0.36 * (1 + math.cos(current_epoch / self.max_epochs * math.pi)) / 2\n",
        "        self.optimizers().param_groups[0]['weight_decay'] = weight_decay\n",
        "\n",
        "\n",
        "# 사전훈련 수행\n",
        "def BYOL_TRAIN(audio_files=pretrain_files, config_path='icbhi_config.yaml'):\n",
        "\n",
        "    ########################################  1. 초기 설정  ########################################\n",
        "\n",
        "    # config 불러오기\n",
        "    cfg = load_yaml_config(config_path)  # BYOL-A 모델 훈련을 위한 설정값이 저장된 config.yaml 로드\n",
        "\n",
        "    logger = get_logger(__name__)        # 로깅 설정 (학습 진행 상황을 출력)\n",
        "    logger.info(cfg)                     # 설정값(cfg)을 로그로 출력\n",
        "    seed_everything(cfg.seed)            # cfg.seed = 42을 사용하여 랜덤 시드 고정\n",
        "\n",
        "    wandb.init(\n",
        "        project=\"ICBHI_BYOL\",    # 프로젝트명\n",
        "        name=f\"BYOL_{cfg.epochs}epoch_{get_timestamp()}\",\n",
        "        config={                 # WandB에 기록할 설정값\n",
        "            \"epochs\": cfg.epochs,\n",
        "            \"batch_size\": cfg.bs,\n",
        "            \"lr\": cfg.lr,\n",
        "            \"ema_decay\": cfg.ema_decay,\n",
        "            \"spectrogram size\": cfg.shape\n",
        "        }\n",
        "    )\n",
        "\n",
        "    wandb_logger = WandbLogger(log_model=True)  # checkpoints are logged at the end of training\n",
        "\n",
        "    ########################################  2. 데이터셋 로드  ########################################\n",
        "\n",
        "    # 데이터 증강 적용\n",
        "    spectrogram_size = tuple(cfg.shape)\n",
        "    tfms = AugmentationModule(spectrogram_size, 2*len(pretrain_duration_list))  # 스펙트로그램 크기, 데이터 정규화를 위한 epoch 샘플 개수 지정 (2배로 설정)\n",
        "\n",
        "    # WaveInLMSOutDataset 클래스의 객체를 생성하여 데이터셋을 구성\n",
        "    ds = WaveInLMSOutDataset(cfg, 'train', audio_files=audio_files, tfms=tfms, use_labels=False, use_librosa=False)\n",
        "\n",
        "    # ds 데이터셋을 DataLoader로 변환하여 배치 단위로 불러올 수 있도록 함\n",
        "    dl = DataLoader(ds, batch_size=cfg.bs,                    # batch_size=cfg.bs: 한 번에 bs개 샘플을 학습\n",
        "                    num_workers=multiprocessing.cpu_count(),  # num_workers=multiprocessing.cpu_count(): CPU 코어 개수만큼 데이터 로딩 속도 증가\n",
        "                    pin_memory=True, shuffle=True)            # pin_memory=True: GPU 사용 시 성능 최적화, shuffle=True: 데이터셋을 매 epoch마다 랜덤으로 섞음\n",
        "\n",
        "    ########################################  3. 모델 정의  ########################################\n",
        "\n",
        "    # 사전훈련 모델 이름 설정\n",
        "    name = (f'BYOLA-Pretrain-u{cfg.unit_sec}-sr{cfg.sample_rate}-d{cfg.feature_d}-s{cfg.shape[0]}x{cfg.shape[1]}-ps{cfg.proj_size}-ph{cfg.proj_dim}'\n",
        "            f'-e{cfg.epochs}-bs{cfg.bs}-lr{str(cfg.lr)[2:]}-{get_timestamp()}')\n",
        "\n",
        "    # 학습 시작 로그 출력\n",
        "    logger.info(f'Training {name}...')\n",
        "\n",
        "    # n_mels=cfg.n_mels : Mel Spectrogram의 Mel 필터 개수 설정\n",
        "    # d=cfg.feature_d : 모델의 feature embedding 차원 설정\n",
        "    model = AudioNTT2020(n_mels=cfg.n_mels, d=cfg.feature_d)\n",
        "\n",
        "    ########################################  4. 학습 수행  ########################################\n",
        "\n",
        "    # 학습률 스케줄러를 모니터링하는 콜백 추가\n",
        "    lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
        "\n",
        "    # 체크포인트 저장 콜백 추가\n",
        "    start_time = get_timestamp()\n",
        "    checkpoint_callback = ModelCheckpoint(\n",
        "        dirpath=PRETRAINED_MODEL_PATH,\n",
        "        filename=(f\"BYOLA-saved-{start_time}\"),\n",
        "        save_top_k=1,                # 최고 성능 1개만 저장\n",
        "        monitor=\"train_loss\",        # 모니터할 지표\n",
        "        mode=\"min\",                  # loss는 작을수록 좋음\n",
        "        save_last=True               # 마지막 체크포인트 따로 저장\n",
        "    )\n",
        "\n",
        "    learner = BYOLALearner(model, cfg.lr, cfg.shape,              # cfg.shape : Mel Spectrogram 크기 설정\n",
        "                            hidden_layer=-1,                      # BYOL에서 feature extraction을 수행할 레이어 설정, -1이면 모델의 마지막 레이어를 사용\n",
        "                            projection_size=cfg.proj_size,        # BYOL의 projection head의 출력 차원 설정\n",
        "                            projection_hidden_size=cfg.proj_dim,  # projection head 내부 hidden layer 크기\n",
        "                            moving_average_decay=cfg.ema_decay)   # BYOL의 Moving Average 업데이트 비율\n",
        "\n",
        "    # PyTorch Lightning을 사용하여 학습을 수행할 Trainer 객체 생성\n",
        "    # max_depth=2: 더 깊은 레이어까지 출력 (세부적인 모델 정보 확인)\n",
        "    trainer = pl.Trainer(\n",
        "        accelerator='gpu', devices=1, logger=wandb_logger,\n",
        "        max_epochs=cfg.epochs,\n",
        "        callbacks=[ModelSummary(max_depth=2), lr_monitor, checkpoint_callback]\n",
        "    )\n",
        "\n",
        "    # dl(DataLoader)에서 제공하는 데이터를 사용하여 BYOL 모델 학습\n",
        "    try:\n",
        "        trainer.fit(learner, dl)\n",
        "    except:\n",
        "        current_epoch = trainer.current_epoch\n",
        "        logger.info(f\"Stopped at epoch {current_epoch+1}\")\n",
        "\n",
        "    ########################################  5. 학습된 모델 가중치 저장  ########################################\n",
        "\n",
        "    to_file = Path(cfg.checkpoint_folder)/(name+'.pth')  # 저장 경로 설정\n",
        "    to_file.parent.mkdir(exist_ok=True, parents=True)    # checkpoints 폴더가 존재하지 않으면 생성\n",
        "    torch.save(model.state_dict(), to_file)              # model.state_dict()를 저장하여 가중치만 저장\n",
        "    logger.info(f'Saved weight as {to_file}')            # \"Saved weight as checkpoints/... .pth\" 로그 출력\n",
        "\n",
        "    wandb.finish()    # WandB 로깅 종료\n",
        "\n",
        "    return to_file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvLOoXnB-RbK"
      },
      "source": [
        "#### 4-3. 모델 훈련"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "executionInfo": {
          "elapsed": 21,
          "status": "ok",
          "timestamp": 1745181240956,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "2RIxlRZt1i45"
      },
      "outputs": [],
      "source": [
        "# wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "dc90a99d98244bddb3afbb60c09fa127",
            "462a864db91f485eb81c9c90ee181f80",
            "38edc585ef1d4aad9bb1702e7ce453a6",
            "48619dae342d4430b7ebb545e1b7d7eb",
            "89598695c11f4eed804408464ceef0da",
            "39201b2a2e134063ae6fde03128a5ff7",
            "129bd74066294ae6a7e403f90285be18",
            "ed7de0e6c03440dd971f22cc747ee3d7",
            "c3c871dee6e34d519b76082ca52fae52",
            "2308b0b56a5d4fdfbfd21bce0b5ce773",
            "05407c93f36a4d43839bb05615a18972"
          ]
        },
        "executionInfo": {
          "elapsed": 11037745,
          "status": "ok",
          "timestamp": 1745192278682,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "rljlGE5z3_kt",
        "outputId": "d98f80ae-f7ae-4e80-cc81-5265cc04fb87"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvanillahub12\u001b[0m (\u001b[33mboaz_woony-boaz\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250420_203402-fr8qp3m6</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/boaz_woony-boaz/ICBHI_BYOL/runs/fr8qp3m6' target=\"_blank\">BYOL_200epoch_2504210534</a></strong> to <a href='https://wandb.ai/boaz_woony-boaz/ICBHI_BYOL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/boaz_woony-boaz/ICBHI_BYOL' target=\"_blank\">https://wandb.ai/boaz_woony-boaz/ICBHI_BYOL</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/boaz_woony-boaz/ICBHI_BYOL/runs/fr8qp3m6' target=\"_blank\">https://wandb.ai/boaz_woony-boaz/ICBHI_BYOL/runs/fr8qp3m6</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Augmentations: Sequential(\n",
            "  (0): MixupBYOLA(ratio=0.4,n=2048,log_mixup_exp=True)\n",
            "  (1): RandomResizeCrop(virtual_crop_size=(1.0, 1.5), time_scale=(0.6, 1.5), freq_scale=(0.6, 1.5))\n",
            ")\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA L4') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /mnt/gcs/CHECKPOINT exists and is not empty.\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name                     | Type           | Params | Mode \n",
            "--------------------------------------------------------------------\n",
            "0 | learner                  | BYOL           | 54.1 M | train\n",
            "1 | learner.net              | AudioNTT2020   | 16.5 M | train\n",
            "2 | learner.online_encoder   | NetWrapper     | 26.0 M | train\n",
            "3 | learner.online_predictor | MLP            | 2.1 M  | train\n",
            "4 | learner.target_encoder   | NetWrapper     | 26.0 M | train\n",
            "5 | post_norm                | NormalizeBatch | 0      | train\n",
            "--------------------------------------------------------------------\n",
            "28.1 M    Trainable params\n",
            "26.0 M    Non-trainable params\n",
            "54.1 M    Total params\n",
            "216.330   Total estimated model params size (MB)\n",
            "74        Modules in train mode\n",
            "0         Modules in eval mode\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dc90a99d98244bddb3afbb60c09fa127",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:\n",
            "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>lr-AdamW</td><td>▁▇████████▇▇▇▇▇▇▇▆▆▆▆▆▅▅▄▄▄▄▄▄▄▄▃▃▃▃▃▂▂▂</td></tr><tr><td>ma</td><td>▅▃▅▄▄▄▄▅▆▃▃▄▄▄▅▅▄▄▄▅▆▂▄█▂▄▆▅▃▅▅▆▇▁▅▄▃▅▇▅</td></tr><tr><td>mb</td><td>▆▅▄▃▅▄▇▅▄▆▃▁▅▆▄▅▄▅▅▆▅▆▅▆▇▃▄█▃▆▅▅▅▅▆▂▄▇▄▇</td></tr><tr><td>sa</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sb</td><td>▄▄█▄▅▄▅▃▃▅▄▄▆▄▃▄▂▂▂▂▆▆▄▅▂▅▄▂▄▄▄▅▃▂▁▂▅▅▃▅</td></tr><tr><td>train_loss</td><td>█▅▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▂▂▂▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▆▆▇▇▇▇████</td></tr><tr><td>weight_decay</td><td>██████████▇▇▇▇▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>146</td></tr><tr><td>lr-AdamW</td><td>6e-05</td></tr><tr><td>ma</td><td>0.0</td></tr><tr><td>mb</td><td>0.11067</td></tr><tr><td>sa</td><td>1</td></tr><tr><td>sb</td><td>0.78461</td></tr><tr><td>train_loss</td><td>0.23039</td></tr><tr><td>trainer/global_step</td><td>3822</td></tr><tr><td>weight_decay</td><td>0.10096</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">BYOL_200epoch_2504210534</strong> at: <a href='https://wandb.ai/boaz_woony-boaz/ICBHI_BYOL/runs/fr8qp3m6' target=\"_blank\">https://wandb.ai/boaz_woony-boaz/ICBHI_BYOL/runs/fr8qp3m6</a><br> View project at: <a href='https://wandb.ai/boaz_woony-boaz/ICBHI_BYOL' target=\"_blank\">https://wandb.ai/boaz_woony-boaz/ICBHI_BYOL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250420_203402-fr8qp3m6/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "PRETRAIN_CHECKPOINT_PATH = BYOL_TRAIN(config_path='icbhi_config.yaml')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ep-OT8Ge08lQ"
      },
      "source": [
        "## 5. Classifier 학습  \n",
        "parameter들을 freezing 했기 때문에 엄밀히는 fine-tuning이라 볼 수 없고, MLPClassifier층 학습임."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "510saN3lDOfe"
      },
      "source": [
        "#### 5-1. Fine-tuning 코드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "executionInfo": {
          "elapsed": 43,
          "status": "ok",
          "timestamp": 1745192278684,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "BVVsLbND2dRh"
      },
      "outputs": [],
      "source": [
        "class PrecomputedNorm(nn.Module):\n",
        "    \"\"\"Normalization using Pre-computed Mean/Std.\n",
        "\n",
        "    Args:\n",
        "        stats: Precomputed (mean, std).\n",
        "        axis: Axis setting used to calculate mean/variance.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, stats, axis=[1, 2]):\n",
        "        super().__init__()\n",
        "        self.axis = axis\n",
        "        self.mean, self.std = stats\n",
        "\n",
        "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
        "        return ((X - self.mean) / self.std)\n",
        "\n",
        "    def __repr__(self):\n",
        "        format_string = self.__class__.__name__ + f'(mean={self.mean}, std={self.std}, axis={self.axis})'\n",
        "        return format_string\n",
        "\n",
        "def calc_norm_stats(cfg, data, n_stats=10000):\n",
        "    \"\"\"Calculates statistics of log-mel spectrogram features in a data source for normalization.\n",
        "\n",
        "    Args:\n",
        "        cfg: Configuration settings.\n",
        "        stats_data: Pretraining dataset.\n",
        "        n_stats: Maximum number of files to calculate statistics.\n",
        "    \"\"\"\n",
        "\n",
        "    n_stats = min(n_stats, len(data))\n",
        "    sample_idxes = np.random.choice(range(len(data)), size=n_stats, replace=False)\n",
        "    ds = WaveInLMSOutDataset(cfg, 'train', pretrain_files, tfms=None, use_labels=False, use_librosa=True)\n",
        "\n",
        "    X = [ds[i] for i in tqdm(sample_idxes)]\n",
        "    X = np.hstack(X)\n",
        "    norm_stats = np.array([X.mean(), X.std()])\n",
        "\n",
        "    return norm_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "executionInfo": {
          "elapsed": 41,
          "status": "ok",
          "timestamp": 1745192278684,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "nvRyRedpI1KW"
      },
      "outputs": [],
      "source": [
        "class FineTunedModel(nn.Module):\n",
        "    def __init__(self, base_model, num_classes, feature_dim, mean=None, std=None):\n",
        "        super().__init__()\n",
        "        self.base_model = base_model\n",
        "        self.mean = nn.Parameter(torch.tensor(mean), requires_grad=False)\n",
        "        self.std = nn.Parameter(torch.tensor(std), requires_grad=False)\n",
        "        self.mlp_classifier = nn.Sequential(\n",
        "            nn.Linear(feature_dim, 64),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(32, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.base_model(x)\n",
        "        x = (x - self.mean) / self.std  # StandardScaler 방식\n",
        "        x = self.mlp_classifier(x)\n",
        "        return x\n",
        "\n",
        "    def get_mean_std(self):\n",
        "        return self.mean, self.std\n",
        "\n",
        "\n",
        "def BYOL_FINETUNING(CHECKPOINT_PATH, audio_files=finetune_files, norm_files=pretrain_files, config_path='icbhi_config.yaml'):\n",
        "\n",
        "    ########################################  1. 초기 설정  ########################################\n",
        "\n",
        "    # config.yaml 파일에서 설정값 불러오기\n",
        "    cfg = load_yaml_config(config_path)\n",
        "\n",
        "    logger = get_logger(__name__)\n",
        "    logger.info(cfg)\n",
        "    seed_everything(cfg.seed)\n",
        "\n",
        "    # wandb 초기화\n",
        "    wandb.init(\n",
        "        project=\"ICBHI_BYOL\",\n",
        "        name=f\"BYOL_Finetuning_{cfg.epochs}epoch_{get_timestamp()}\",\n",
        "        config={\"batch_size\": cfg.bs, \"sample_rate\": cfg.sample_rate, \"spectrogram size\": cfg.shape}\n",
        "    )\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    ########################################  2. 데이터셋 로드  ########################################\n",
        "\n",
        "    # 정규화에 사용할 평균, 분산 계산\n",
        "    norm_stats = calc_norm_stats(cfg, norm_files, n_stats=10000)\n",
        "\n",
        "    # 데이터 로드\n",
        "    train_ds = WaveInLMSOutDataset(cfg, 'finetune', audio_files=audio_files, tfms=PrecomputedNorm(norm_stats), use_labels=True)\n",
        "    train_dl = DataLoader(train_ds, batch_size=cfg.bs, num_workers=multiprocessing.cpu_count(), pin_memory=False, shuffle=True)\n",
        "\n",
        "    ########################################  3. 모델 정의  ########################################\n",
        "\n",
        "    # Fine-tuning 모델 이름 설정\n",
        "    name = (f'BYOLA-Finetune-u{cfg.unit_sec}-sr{cfg.sample_rate}-d{cfg.feature_d}-s{cfg.shape[0]}x{cfg.shape[1]}-ps{cfg.proj_size}-ph{cfg.proj_dim}'\n",
        "            f'-e{cfg.epochs}-bs{cfg.bs}-lr{str(cfg.lr)[2:]}-{get_timestamp()}')\n",
        "\n",
        "    # 사전훈련된 가중치 불러오기\n",
        "    pretrained_weights = torch.load(CHECKPOINT_PATH,\n",
        "        # map_location=torch.device('cpu'),\n",
        "        map_location='cuda'\n",
        "    )\n",
        "    model = AudioNTT2020(n_mels=cfg.n_mels, d=cfg.feature_d)\n",
        "    model.load_state_dict(pretrained_weights)\n",
        "\n",
        "    # Fine-tuning을 위해 학습 가능한 상태로 변경\n",
        "    model.train()\n",
        "\n",
        "    # 가중치 Freezing\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # 분류기만 학습하도록 설정\n",
        "    for param in model.fc.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    # Fine-tuning 모델 정의\n",
        "    model = FineTunedModel(model, num_classes=4, feature_dim=cfg.feature_d, mean=None, std=None).to(device)\n",
        "\n",
        "    ########################################  4. 학습 수행  ########################################\n",
        "\n",
        "    # Loss, 옵티마이저 정의\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=0.4)\n",
        "    lr_scheduler = torch.optim.lr_scheduler\n",
        "\n",
        "    # Linear Warmup 설정 (20 epoch 동안 선형 증가)\n",
        "    max_epochs = cfg.epochs\n",
        "    warmup_epochs = 20\n",
        "\n",
        "    warmup_scheduler = lr_scheduler.LinearLR(optimizer, start_factor=3e-5 / cfg.lr, end_factor=1.0, total_iters=warmup_epochs)\n",
        "    cosine_scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs - warmup_epochs, eta_min=1e-6)\n",
        "\n",
        "    scheduler = lr_scheduler.SequentialLR(optimizer, schedulers=[warmup_scheduler, cosine_scheduler], milestones=[warmup_epochs])\n",
        "\n",
        "    # ============================================ 학습 루프 ============================================\n",
        "    for epoch in range(max_epochs):\n",
        "\n",
        "        # Training Phase\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        # 현재 epoch의 학습률\n",
        "        lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        # 현재 epoch의 Weight Decay\n",
        "        weight_decay = 0.04 + 0.36 * (1 + math.cos(epoch / max_epochs * math.pi)) / 2\n",
        "        optimizer.param_groups[0]['weight_decay'] = weight_decay    # Weight Decay 업데이트\n",
        "\n",
        "        all_labels = []\n",
        "        all_preds = []\n",
        "\n",
        "        with tqdm(total=len(train_dl), desc=f\"Training Step - Epoch {epoch+1}\", unit='batch') as train_pbar:\n",
        "\n",
        "            # Data 전체를 한 번에 로딩\n",
        "            inputs_list = []\n",
        "            labels_list = []\n",
        "\n",
        "            for inputs, labels in train_dl:\n",
        "                inputs_list.append(inputs)\n",
        "                labels_list.append(labels)\n",
        "\n",
        "            # 전체 텐서로 병합\n",
        "            full_inputs = torch.cat(inputs_list).to(device)   # shape: (N, 1, F, T)\n",
        "            full_labels = torch.cat(labels_list).to(device)   # shape: (N,)\n",
        "\n",
        "            # 학습\n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(full_inputs)                      # shape: (N, num_classes)\n",
        "            loss = criterion(outputs, full_labels)            # 전체 데이터에 대해 CrossEntropyLoss 계산\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # 정확도 계산\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct = (predicted == full_labels).sum().item()\n",
        "            total = full_labels.size(0)\n",
        "            accuracy = correct / total\n",
        "\n",
        "            # 전체 라벨과 예측 결과 저장\n",
        "            all_labels = torch.cat(labels_list).cpu().tolist()  # 정답 라벨 저장\n",
        "            all_preds.extend(predicted.cpu().tolist())          # 예측값 저장\n",
        "\n",
        "            train_pbar.update(1)\n",
        "\n",
        "            ####################  실제로 Weight Decay가 업데이트 되고 있는지 확인 필요  ####################\n",
        "            print(f\"Epoch: {epoch+1} | Weight Decay: {optimizer.param_groups[0]['weight_decay']:.4f} | Loss: {loss.item():.4f} | Acc: {accuracy:.4f}\")\n",
        "\n",
        "        # 4x4 confusion matrix\n",
        "        conf_matrix = confusion_matrix(all_labels, all_preds, labels=[0, 1, 2, 3])\n",
        "\n",
        "        # Positive: 1,2,3 / Negative: 0\n",
        "        TP = conf_matrix[1:, 1:].sum()    # 양성 중에 양성으로 예측\n",
        "        FN = conf_matrix[1:, 0].sum()     # 양성인데 음성으로 예측\n",
        "        FP = conf_matrix[0, 1:].sum()     # 음성인데 양성으로 예측\n",
        "        TN = conf_matrix[0, 0]            # 음성인데 양성으로 예측\n",
        "\n",
        "        train_sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0    # 민감도\n",
        "        train_specificity = TN / (TN + FP) if (TN + FP) > 0 else 0    # 특이도\n",
        "\n",
        "        train_loss = running_loss / len(train_dl)\n",
        "        train_f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "\n",
        "        # 에폭 결과 출력\n",
        "        tqdm.write(f\"Epoch {epoch+1}\")\n",
        "        tqdm.write(f\"├─ Learning Rate : {lr:.2e}\")\n",
        "        tqdm.write(f\"├─ Loss          : {train_loss:.4f}\")\n",
        "        tqdm.write(f\"├─ F1-Score      : {train_f1:.4f}\")\n",
        "        tqdm.write(f\"├─ Sensitivity   : {train_sensitivity:.4f}\")\n",
        "        tqdm.write(f\"├─ Specificity   : {train_specificity:.4f}\")\n",
        "\n",
        "        # wandb 로깅\n",
        "        wandb.log({\n",
        "            \"Learning_rate\": lr,\n",
        "            \"Loss\": train_loss,\n",
        "            \"F1-Score\": train_f1,\n",
        "            \"Sensitivity\": train_sensitivity,\n",
        "            \"Specificity\": train_specificity,\n",
        "        })\n",
        "\n",
        "        # 학습률 업데이트 (로깅 이후에 하기)\n",
        "        scheduler.step()\n",
        "\n",
        "    ########################################  5. 가중치 저장  ########################################\n",
        "\n",
        "    # 가중치 저장\n",
        "    to_file = Path(cfg.checkpoint_folder)/(name+'.pth')\n",
        "    torch.save(model.state_dict(), to_file)\n",
        "    print(\"Fine-tuned model saved successfully.\")\n",
        "\n",
        "    # wandb 종료\n",
        "    wandb.finish()\n",
        "\n",
        "    return to_file, model.get_mean_std()[0], model.get_mean_std()[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8BI1AvHBDhw"
      },
      "source": [
        "#### 5-2. Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "executionInfo": {
          "elapsed": 41,
          "status": "ok",
          "timestamp": 1745192278685,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "AwNSqdayl_z9"
      },
      "outputs": [],
      "source": [
        "# FILE_NAME = \"BYOLA-Pretrain-u3-sr16000-d2048-s64x298-ps256-ph4096-e200-bs64-lr0003-2504110541.pth\"\n",
        "# temp_path = os.path.join(PRETRAINED_MODEL_PATH, FILE_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "executionInfo": {
          "elapsed": 4327,
          "status": "error",
          "timestamp": 1745192282972,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "-x5Itj4YytAE",
        "outputId": "8b913b4f-7f7f-4dbf-879c-ec020f48082f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250420_233758-us6v6unw</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/boaz_woony-boaz/ICBHI_BYOL/runs/us6v6unw' target=\"_blank\">BYOL_Finetuning_200epoch_2504210837</a></strong> to <a href='https://wandb.ai/boaz_woony-boaz/ICBHI_BYOL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/boaz_woony-boaz/ICBHI_BYOL' target=\"_blank\">https://wandb.ai/boaz_woony-boaz/ICBHI_BYOL</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/boaz_woony-boaz/ICBHI_BYOL/runs/us6v6unw' target=\"_blank\">https://wandb.ai/boaz_woony-boaz/ICBHI_BYOL/runs/us6v6unw</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-f9f3537dd043>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mFINETUNE_CHECKPOINT_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrain_data_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrain_data_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBYOL_FINETUNING\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPRETRAIN_CHECKPOINT_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'icbhi_config.yaml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-41-7a2a7a79b196>\u001b[0m in \u001b[0;36mBYOL_FINETUNING\u001b[0;34m(CHECKPOINT_PATH, audio_files, norm_files, config_path)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# 정규화에 사용할 평균, 분산 계산\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mnorm_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_norm_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_stats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;31m# 데이터 로드\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-bce5a0fbbc3d>\u001b[0m in \u001b[0;36mcalc_norm_stats\u001b[0;34m(cfg, data, n_stats)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mn_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_stats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0msample_idxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_stats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWaveInLMSOutDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrain_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_librosa\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_idxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-5533064f4b05>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cfg, mode, audio_files, tfms, use_labels, use_librosa)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m# 각 wav 파일을 처리하여 호흡 주기 단위로 저장\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mwav\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;31m# Resample if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchaudio/_backend/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \"\"\"\n\u001b[1;32m    204\u001b[0m         \u001b[0mbackend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdispatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels_first\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchaudio/_backend/ffmpeg.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size)\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mbuffer_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4096\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     ) -> Tuple[torch.Tensor, int]:\n\u001b[0;32m--> 297\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels_first\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchaudio/_backend/ffmpeg.py\u001b[0m in \u001b[0;36mload_audio\u001b[0;34m(src, frame_offset, num_frames, convert, channels_first, format, buffer_size)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"vorbis\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mformat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ogg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStreamReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0msample_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_src_stream_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_audio_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mfilter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_load_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torio/io/_streaming_media_decoder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, format, option, buffer_size)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_be\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mffmpeg_ext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStreamingMediaDecoderFileObj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moption\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_be\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mffmpeg_ext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStreamingMediaDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moption\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_be\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_best_audio_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "FINETUNE_CHECKPOINT_PATH, pretrain_data_mean, pretrain_data_std = BYOL_FINETUNING(PRETRAIN_CHECKPOINT_PATH, config_path='icbhi_config.yaml')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRgce8gWzJ_Y"
      },
      "source": [
        "## 6. 성능 평가"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UwO1izLruph"
      },
      "source": [
        "#### 6-1. 성능 평가 코드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 27,
          "status": "aborted",
          "timestamp": 1745192282974,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "j6EzN_8LzJg_"
      },
      "outputs": [],
      "source": [
        "def BYOL_EVALUATION(CHECKPOINT_PATH, audio_files=test_files, norm_files=pretrain_files, mean=pretrain_data_mean, std=pretrain_data_std, config_path='icbhi_config.yaml'):\n",
        "\n",
        "    ########################################  1. 초기 설정  ########################################\n",
        "    # config.yaml 파일에서 설정값 불러오기\n",
        "    cfg = load_yaml_config(config_path)\n",
        "\n",
        "    logger = get_logger(__name__)        # 로깅 설정 (학습 진행 상황을 출력)\n",
        "    logger.info(cfg)                     # 설정값(cfg)을 로그로 출력\n",
        "    seed_everything(cfg.seed)            # cfg.seed = 42을 사용하여 랜덤 시드 고정\n",
        "\n",
        "    # wandb 초기화\n",
        "    wandb.init(\n",
        "        project=\"ICBHI_BYOL\",\n",
        "        name=f\"BYOL_Evaluation_{get_timestamp()}\",\n",
        "        config={\"batch_size\": cfg.bs, \"sample_rate\": cfg.sample_rate, \"spectrogram size\": cfg.shape}\n",
        "    )\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    ########################################  2. 데이터셋 로드  ########################################\n",
        "\n",
        "    # 정규화에 사용할 평균, 분산 계산\n",
        "    norm_stats = calc_norm_stats(cfg, norm_files, n_stats=10000)\n",
        "\n",
        "    # Test 데이터 로드\n",
        "    test_ds = WaveInLMSOutDataset(cfg, 'test', audio_files=audio_files, tfms=PrecomputedNorm(norm_stats), use_labels=True)\n",
        "    test_dl = DataLoader(test_ds, batch_size=cfg.bs, num_workers=multiprocessing.cpu_count(), pin_memory=False, shuffle=False)\n",
        "\n",
        "    ########################################  3. 모델 정의  ########################################\n",
        "\n",
        "    # Model 정의\n",
        "    base_model = AudioNTT2020(n_mels=cfg.n_mels, d=cfg.feature_d)\n",
        "    model = FineTunedModel(base_model, num_classes=4, feature_dim=cfg.feature_d, mean=mean, std=std).to(device)\n",
        "\n",
        "    # Fine-tuning된 가중치 불러오기\n",
        "    finetuned_weights = torch.load(CHECKPOINT_PATH,\n",
        "        # map_location=torch.device('cpu'),\n",
        "        map_location='cuda'\n",
        "    )\n",
        "\n",
        "    # 가중치 로드\n",
        "    model.load_state_dict(finetuned_weights)\n",
        "\n",
        "    ########################################  4. 모델 평가 ########################################\n",
        "\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # Data 전체를 한 번에 로딩\n",
        "        inputs_list = []\n",
        "        labels_list = []\n",
        "\n",
        "        for inputs, labels in test_dl:\n",
        "            inputs_list.append(inputs)\n",
        "            labels_list.append(labels)\n",
        "\n",
        "        # 전체 텐서로 병합\n",
        "        full_inputs = torch.cat(inputs_list).to(device)   # shape: (N, 1, F, T)\n",
        "\n",
        "        # 정확도 계산\n",
        "        outputs = model(full_inputs)                      # shape: (N, num_classes)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        # 전체 라벨과 예측 결과 저장\n",
        "        all_labels = torch.cat(labels_list).cpu().tolist()  # 정답 라벨 저장\n",
        "        all_preds.extend(predicted.cpu().tolist())          # 예측값 저장\n",
        "\n",
        "    # 4x4 confusion matrix\n",
        "    conf_matrix = confusion_matrix(all_labels, all_preds, labels=[0, 1, 2, 3])\n",
        "    print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
        "\n",
        "    # Positive: 1,2,3 / Negative: 0\n",
        "    TP = conf_matrix[1:, 1:].sum()    # 양성 중에 양성으로 예측\n",
        "    FN = conf_matrix[1:, 0].sum()     # 양성인데 음성으로 예측\n",
        "    FP = conf_matrix[0, 1:].sum()     # 음성인데 양성으로 예측\n",
        "    TN = conf_matrix[0, 0]            # 음성인데 양성으로 예측\n",
        "\n",
        "    accuracy = (TP + TN) / (TP + FN + FP + TN)              # 정분류율\n",
        "    error_rate = 1- accuracy                                # 오분류율\n",
        "    sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0    # 민감도\n",
        "    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0    # 특이도\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')   # F1 Score\n",
        "\n",
        "    print(f\"\\nFinal Results:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Error Rate: {error_rate:.4f}\")\n",
        "    print(f\"Sensitivity: {sensitivity:.4f}\")\n",
        "    print(f\"Specificity: {specificity:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"ICBHI Score: {(sensitivity+specificity)/2:.4f}\")\n",
        "\n",
        "    ########################################  5. Plotting  ########################################\n",
        "\n",
        "    def log_confusion_matrix_wandb(y_true, y_pred, class_names, sensitivity, specificity):\n",
        "        cm = confusion_matrix(y_true, y_pred, labels=[0, 1, 2, 3])\n",
        "        fig, ax = plt.subplots(figsize=(6, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                    xticklabels=class_names, yticklabels=class_names, ax=ax)\n",
        "        ax.set_xlabel('Predicted')\n",
        "        ax.set_ylabel('Actual')\n",
        "        ax.set_title('Confusion Matrix')\n",
        "\n",
        "        plt.text(\n",
        "            0.99, 0.16,  # 우하단 (x=99%, y=16%) 위치\n",
        "            f\"Sensitivity: {sensitivity*100:.2f}\\nSpecificity: {specificity*100:.2f}\\nICBHI Score: {100*(sensitivity+specificity)/2:.2f}\",\n",
        "            ha='right', va='bottom',\n",
        "            transform=plt.gca().transAxes,  # 축 기준 좌표로 해석\n",
        "            fontsize=10, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8)\n",
        "        )\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # wandb에 이미지로 로그\n",
        "        wandb.log({\"confusion_matrix\": wandb.Image(fig)})\n",
        "        plt.close(fig)\n",
        "\n",
        "    # 혼동행렬은 따로 이미지로 출력\n",
        "    log_confusion_matrix_wandb(\n",
        "        y_true=np.array(all_labels),\n",
        "        y_pred=np.array(all_preds),\n",
        "        class_names=[\"Normal\", \"Crackle\", \"Wheeze\", \"Both\"],\n",
        "        sensitivity=sensitivity,\n",
        "        specificity=specificity\n",
        "    )\n",
        "\n",
        "    def log_bar_chart_wandb(y_true, y_pred, class_names, sensitivity, specificity):\n",
        "        num_classes = len(class_names)\n",
        "        cm = confusion_matrix(y_true, y_pred, labels=[0, 1, 2, 3])\n",
        "        cm_normalized = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
        "\n",
        "        fig, axs = plt.subplots(len(class_names), len(class_names), figsize=(6, 6))\n",
        "\n",
        "        for i in range(len(class_names)):\n",
        "            for j in range(len(class_names)):\n",
        "                ax = axs[i, j]\n",
        "\n",
        "                value = cm_normalized[i, j]\n",
        "                ax.barh([0], [value], height=0.5, color='steelblue')  # 가로 막대\n",
        "                ax.set_xlim(0, 1)\n",
        "\n",
        "                # 숫자 텍스트 추가 (막대 오른쪽)\n",
        "                ax.text(value + 0.02, 0, f'{int(cm[i,j])}', va='center', fontsize=8)\n",
        "\n",
        "                # 눈금 제거\n",
        "                ax.set_xticks(np.arange(0, 1.1, 0.2))\n",
        "                ax.set_xticklabels([])\n",
        "                ax.set_yticks([])\n",
        "                ax.set_yticklabels([])\n",
        "\n",
        "                # grid 추가 (0.2 단위)\n",
        "                ax.grid(axis='x', linestyle='--', alpha=0.5)\n",
        "\n",
        "                # 좌하단 텍스트: Actual / Predicted\n",
        "                if i == len(class_names) - 1:\n",
        "                    ax.set_xlabel(class_names[j], fontsize=10)\n",
        "                if j == 0:\n",
        "                    ax.set_ylabel(class_names[i], fontsize=10, rotation=0, labelpad=20)\n",
        "\n",
        "        # 전체 축 제목\n",
        "        fig.suptitle(\"Confusion Matrix (as horizontal bar charts)\", fontsize=14)\n",
        "        fig.supxlabel(\"Predicted\")\n",
        "        fig.supylabel(\"Actual\")\n",
        "\n",
        "        plt.text(\n",
        "            0.99, 0.16,  # 우하단 (x=99%, y=16%) 위치\n",
        "            f\"Sensitivity: {sensitivity*100:.2f}\\nSpecificity: {specificity*100:.2f}\\nICBHI Score: {100*(sensitivity+specificity)/2:.2f}\",\n",
        "            ha='right', va='bottom',\n",
        "            transform=plt.gca().transAxes,  # 축 기준 좌표로 해석\n",
        "            fontsize=10, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8)\n",
        "        )\n",
        "\n",
        "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "\n",
        "        # wandb에 이미지로 로그\n",
        "        wandb.log({\"bar_chart\": wandb.Image(fig)})\n",
        "        plt.close(fig)\n",
        "\n",
        "    # 막대그래프로 바꿔서 따로 이미지로 출력\n",
        "    log_bar_chart_wandb(\n",
        "        y_true=np.array(all_labels),\n",
        "        y_pred=np.array(all_preds),\n",
        "        class_names=[\"Normal\", \"Crackle\", \"Wheeze\", \"Both\"],\n",
        "        sensitivity=sensitivity,\n",
        "        specificity=specificity\n",
        "    )\n",
        "\n",
        "    # t-SNE를 위한 feature 추출 함수\n",
        "    @torch.no_grad()\n",
        "    def extract_features(encoder, dataloader, device):\n",
        "        features = []\n",
        "        labels = []\n",
        "\n",
        "        for x, label in tqdm(dataloader, desc=\"Extracting features\"):\n",
        "            x = x.to(device)\n",
        "            out = encoder(x)\n",
        "            out = torch.nn.functional.normalize(out, dim=1)  # L2 정규화\n",
        "            features.append(out.cpu())\n",
        "            labels.append(label.cpu())\n",
        "\n",
        "        features = torch.cat(features, dim=0).numpy()\n",
        "        labels = torch.cat(labels, dim=0).numpy()\n",
        "        return features, labels\n",
        "\n",
        "    # t-SNE 시각화 함수\n",
        "    def plot_tsne(features, num_classes, sensitivity, specificity, title=\"t-SNE Visualization\"):\n",
        "        tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
        "        reduced = tsne.fit_transform(features)\n",
        "\n",
        "        plt.figure(figsize=(10, 8))\n",
        "\n",
        "        label_names = [\"Normal\", \"Crackle\", \"Wheeze\", \"Both\"]\n",
        "        for i in range(num_classes):\n",
        "            idx = labels == i\n",
        "            plt.scatter(reduced[idx, 0], reduced[idx, 1], label=label_names[i], alpha=0.6)\n",
        "\n",
        "        plt.text(\n",
        "            0.95, 0.1,  # 우하단 (x=90%, y=10%) 위치\n",
        "            f\"Sensitivity: {sensitivity*100:.2f}\\nSpecificity: {specificity*100:.2f}\\nICBHI Score: {100*(sensitivity+specificity)/2:.2f}\",\n",
        "            ha='right', va='bottom',\n",
        "            transform=plt.gca().transAxes,  # 축 기준 좌표로 해석\n",
        "            fontsize=10, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8)\n",
        "        )\n",
        "\n",
        "        plt.legend()\n",
        "        plt.title(title)\n",
        "        plt.xlabel(\"Dim 1\")\n",
        "        plt.ylabel(\"Dim 2\")\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "        return plt\n",
        "\n",
        "    # Feature 추출 후 t-SNE 시각화 수행\n",
        "    encoder = model.base_model.eval().to(device)\n",
        "    features, labels = extract_features(encoder, test_dl, device)\n",
        "    plot_tsne(features, num_classes=4, sensitivity=sensitivity, specificity=specificity, title=\"t-SNE Visualization of Test Data\")\n",
        "\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e183CQmtzle"
      },
      "source": [
        "#### 6-2. 성능 평가"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 26,
          "status": "aborted",
          "timestamp": 1745192282974,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "UNstIvGzt7E_"
      },
      "outputs": [],
      "source": [
        "# FILE_NAME_2 = \"BYOLA-Finetune-u5-sr16000-d2048-s64x498-ps256-ph4096-e200-bs64-lr0003-2504081417.pth\"\n",
        "# temp_path = os.path.join(FINETUNE_CHECKPOINT_PATH, FILE_NAME_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 26,
          "status": "aborted",
          "timestamp": 1745192282974,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "DyyV0oLIt_MK"
      },
      "outputs": [],
      "source": [
        "BYOL_EVALUATION(temp_path, config_path='icbhi_config.yaml')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "egM1OwiPbNvp",
        "SCviD6mvgfGi",
        "fda9T4cHjMx7",
        "OfVeQFU7z9go",
        "PbTj_j4SJeon",
        "SGTGA4qvwKB2",
        "XavJi3_eDWqW",
        "9GKbLHbIznaR",
        "y-9scNDMDdRx",
        "oluurwU9EyLf",
        "yeb3I6vKKBhf",
        "sDPtUlTV2dRX",
        "1Oen-Ycs2dRZ",
        "-4HzJqqbFI9b",
        "5m9KnO-xEMll",
        "510saN3lDOfe",
        "1UwO1izLruph"
      ],
      "gpuType": "L4",
      "name": "BYOL_A_250421_GCP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "jupyter",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.2"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "05407c93f36a4d43839bb05615a18972": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "129bd74066294ae6a7e403f90285be18": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2308b0b56a5d4fdfbfd21bce0b5ce773": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38edc585ef1d4aad9bb1702e7ce453a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed7de0e6c03440dd971f22cc747ee3d7",
            "max": 26,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c3c871dee6e34d519b76082ca52fae52",
            "value": 20
          }
        },
        "39201b2a2e134063ae6fde03128a5ff7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "462a864db91f485eb81c9c90ee181f80": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39201b2a2e134063ae6fde03128a5ff7",
            "placeholder": "​",
            "style": "IPY_MODEL_129bd74066294ae6a7e403f90285be18",
            "value": "Epoch 147:  77%"
          }
        },
        "48619dae342d4430b7ebb545e1b7d7eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2308b0b56a5d4fdfbfd21bce0b5ce773",
            "placeholder": "​",
            "style": "IPY_MODEL_05407c93f36a4d43839bb05615a18972",
            "value": " 20/26 [00:44&lt;00:13,  0.45it/s, v_num=p3m6, train_loss=0.230, weight_decay=0.101, ma=3.9e-10, sa=1.000, mb=0.111, sb=0.785]"
          }
        },
        "89598695c11f4eed804408464ceef0da": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "c3c871dee6e34d519b76082ca52fae52": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dc90a99d98244bddb3afbb60c09fa127": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_462a864db91f485eb81c9c90ee181f80",
              "IPY_MODEL_38edc585ef1d4aad9bb1702e7ce453a6",
              "IPY_MODEL_48619dae342d4430b7ebb545e1b7d7eb"
            ],
            "layout": "IPY_MODEL_89598695c11f4eed804408464ceef0da"
          }
        },
        "ed7de0e6c03440dd971f22cc747ee3d7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
